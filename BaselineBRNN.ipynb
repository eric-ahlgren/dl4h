{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data/\"\n",
    "GRAM_DATA_PATH = \"../Project/code/processed_data/gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targets = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targets) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b1709ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Project/code/processed_data/gram/gram_ccs.level1.pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ccs_level1 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGRAM_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgram_ccs.level1.pk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m ccs_level2 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GRAM_DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgram_ccs.level2.pk\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m ccs_level3 \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GRAM_DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgram_ccs.level3.pk\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Project/code/processed_data/gram/gram_ccs.level1.pk'"
     ]
    }
   ],
   "source": [
    "ccs_level1 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level1.pk'), 'rb'))\n",
    "ccs_level2 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level2.pk'), 'rb'))\n",
    "ccs_level3 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level3.pk'), 'rb'))\n",
    "ccs_level4 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level4.pk'), 'rb'))\n",
    "ccs_level5 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level5.pk'), 'rb'))\n",
    "ccs_seqs = ccs_level1 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.seqs'), 'rb'))\n",
    "ccs_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.types'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25767cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Project/code/processed_data/gram/processed_gram.3digitICD9.seqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gram_3digit_seqs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGRAM_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_gram.3digitICD9.seqs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m gram_3digit_types \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GRAM_DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_gram.3digitICD9.types\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m gram_dates \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(GRAM_DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_gram.dates\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Project/code/processed_data/gram/processed_gram.3digitICD9.seqs'"
     ]
    }
   ],
   "source": [
    "gram_3digit_seqs = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.3digitICD9.seqs'), 'rb'))\n",
    "gram_3digit_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.3digitICD9.types'), 'rb'))\n",
    "gram_dates = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.dates'), 'rb'), encoding='latin1')\n",
    "gram_pids = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.pids'), 'rb'))\n",
    "gram_seqs = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.seqs'), 'rb'))\n",
    "gram_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.types'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2cc1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gram_3digit_types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r_3digit_types \u001b[38;5;241m=\u001b[39m {v:k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgram_3digit_types\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gram_3digit_types' is not defined"
     ]
    }
   ],
   "source": [
    "r_3digit_types = {v:k for k,v in gram_3digit_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75dbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtypes_ccs = {v:k for k,v in ccs_types.items()}\n",
    "rtypes = {v:k for k,v in gram_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "#         x = []\n",
    "#         for i,patient in enumerate(seqs):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 x.append(visit)\n",
    "#         y = []\n",
    "#         for i,patient in enumerate(targets):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 y.append(patient[j+1])\n",
    "\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c9d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_YMask(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    #max_num_categories = max(num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_visits, num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y_masks = torch.zeros((num_patients, max_num_visits, num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             if j_visit == len(patient) - 2:\n",
    "#                 rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                 rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                 rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "    for i_patient, patient in enumerate(targets):   \n",
    "        for j_visit, visit in enumerate(patient[1:]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                y[i_patient, j_visit, k_code] = code\n",
    "                y_masks[i_patient, j_visit, k_code] = 1\n",
    "#             y[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             y_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d83bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    max_num_categories = max(batch_num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient[1:]):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "                \n",
    "    for i_patient, patient in enumerate(targets):   \n",
    "        for visit in patient[-1:]:\n",
    "            for k_code, code in enumerate(visit):\n",
    "                y[i_patient, k_code] = code\n",
    "                y_masks[i_patient, k_code] = 1\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 x[i_patient, j_visit, k_code] = code\n",
    "#                 x_masks[i_patient, j_visit, k_code] = 1\n",
    "#                 if j_visit == len(patient) - 1 and k_code == len(visit) - 1:\n",
    "#                     rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                     rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                     rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c323bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "        x = []\n",
    "        for i,patient in enumerate(seqs):\n",
    "            for j,visit in enumerate(patient):\n",
    "                if j == len(patient) - 1:\n",
    "                    break\n",
    "                x.append(visit)\n",
    "        y = []\n",
    "        for i,patient in enumerate(targets):\n",
    "            for j,visit in enumerate(patient):\n",
    "                if j == len(patient) - 1:\n",
    "                    break\n",
    "                y.append(patient[j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218c2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_old(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "    y = torch.tensor(targets, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "#     global categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     num_categories = len(categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     y = torch.zeros((num_patients, num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     y_masks = torch.zeros((num_patients, num_categories), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\" \n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 1 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for j_visit, visit in enumerate(targets):\n",
    "#         for k_cat, cat in enumerate(visit):\n",
    "#             y[j_visit, k_cat] = cat\n",
    "#             y_masks[j_visit, k_cat] = 1\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56cb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    x[~masks] = 0\n",
    "    return x.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b3d8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: Consider using `torch.gather()`.\n",
    "    \"\"\"\n",
    "    idx_vector = masks.any(dim=2).sum(1) - 1\n",
    "    p_idx = torch.arange(0,len(hidden_states), dtype=torch.int64)\n",
    "    last_hidden_state = hidden_states[p_idx,idx_vector]\n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(4903, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(256, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks, y_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order, and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        rev_output, _ = self.rev_rnn(rev_x)\n",
    "        true_h_n_rev = get_last_visit(rev_output, rev_masks)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        #import pdb; pdb.set_trace()\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        # probs = self.softmax(logits)\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "#         probs = probs.reshape(probs.shape[0]*probs.shape[1], probs.shape[2])\n",
    "#         y_masks = y_masks.reshape(y_masks.shape[0]*y_masks.shape[1], y_masks.shape[2])\n",
    "#         probs = probs[y_masks.any(dim=1)]\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.Adadelta(naive_rnn.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad8a5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, test_loader, threshold=0.5, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, rev_x, rev_masks, y, y_masks in test_loader:\n",
    "            #import pdb; pdb.set_trace()\n",
    "            nn = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks, rev_x, rev_masks, y_masks)\n",
    "#             num_categories = torch.count_nonzero(y, dim=2)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            num_predictions = 0\n",
    "            num_targets = 0\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "            precision = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "#             y_masks = y_masks.reshape(\n",
    "#                 y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "#             y = y[y_masks.any(dim=1)]\n",
    "#             y_masks = y_masks[y_masks.any(dim=1)]\n",
    "\n",
    "\n",
    "#             v_idx = masks.any(dim=2)\n",
    "#             v_idx = v_idx.sum(dim=1)\n",
    "#             v_idx = v_idx.unsqueeze(-1)\n",
    "#             v_idx = v_idx.repeat(1,y_hat.shape[2])\n",
    "#             v_idx = v_idx.unsqueeze(1)\n",
    "#             y_hat = torch.gather(y_hat,1,v_idx).squeeze()\n",
    "            for i in range(k):\n",
    "                \n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "\n",
    "                y_true = y[i, y_masks[i]].unique()\n",
    "                all_targets.extend(y_true.tolist())\n",
    "                _, y_pred = torch.topk(y_hat[i], len(y_true))\n",
    "                #y_pred = torch.nonzero(y_hat[0] > threshold).squeeze()\n",
    "                if y_pred.numel() > 0:\n",
    "                    try:\n",
    "                        all_predictions.extend(y_pred.tolist())\n",
    "                    except TypeError:\n",
    "                        y_pred = [y_pred.tolist()]\n",
    "                    all_predictions.extend(y_pred)\n",
    "#                     for v in y_pred:\n",
    "#                         if v in y_true:\n",
    "#                             visit_correct += 1\n",
    "                    for v in y_true:\n",
    "                        if v in y_pred:\n",
    "                            visit_correct += 1\n",
    "                    num_predictions += len(y_pred)\n",
    "\n",
    "                num_targets += len(y_true)\n",
    "                precision += visit_correct / min(k, len(y_true))\n",
    "                k_correct += visit_correct\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    " #           import pdb; pdb.set_trace()\n",
    "            precision_k = precision / k\n",
    "#             precision_k1 = k_correct / min(k, num_targets)\n",
    "            if num_predictions == 0:\n",
    "                accuracy_k = 0\n",
    "            else:\n",
    "                accuracy_k = k_correct / num_predictions\n",
    "            precision_k = total_precision / nn\n",
    "            accuracy_k = total_accuracy / nn\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "#     roc_auc = roc_auc_score(y_true, y_score)\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "682ebc31",
   "metadata": {},
   "outputs": [],
   "source": [
    " precision_k, accuracy_k = eval_model(naive_rnn, train_loader, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    #base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "#             import pdb; pdb.set_trace()\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks, y_masks)\n",
    "#             import pdb; pdb.set_trace()\n",
    "#             y[~y_masks] = criterion.ignore_index\n",
    "#             last_y = y_masks.any(dim=2).sum(dim=1) - 1\n",
    "#             indices = last_y.unsqueeze(-1)\n",
    "#             indices = indices.repeat(1, y.shape[2])\n",
    "#             indices = indices.unsqueeze(1)\n",
    "#             y_filt = torch.gather(y, 1, indices)\n",
    "\n",
    "#             n_visits = y_masks.any(dim=2).sum(dim=1)\n",
    "#             for i_patient, j_visit in enumerate(n_visits):\n",
    "#                 for visit in range(j_visit - 1):\n",
    "#                     mask = y_masks[i_patient, visit+1]\n",
    "\n",
    "#                     yh = y_hat[i_patient, visit]\n",
    "#                     y_tmp = indices_to_multihot(\n",
    "#                         y[i_patient, visit+1], mask, yh)\n",
    "            \n",
    "            # stack into visits\n",
    "#             yh = y_hat.reshape(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
    "#             y_masks = y_masks.reshape(y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "        \n",
    "                    \n",
    "            \n",
    "            y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "            loss = criterion(y_hat, y_mh)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.2f}, accuracy@k{k}: {accuracy_k:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 19.25\n",
      "RAM %: 61.7\n",
      "Epoch: 1 \t Training Loss: 0.136856\n",
      "Epoch: 1 \t Validation precision@k5: 0.05, accuracy@k5: 0.03\n",
      "Epoch: 1 \t Validation precision@k10: 0.06, accuracy@k10: 0.05\n",
      "Epoch: 1 \t Validation precision@k15: 0.08, accuracy@k15: 0.08\n",
      "Epoch: 1 \t Validation precision@k20: 0.10, accuracy@k20: 0.10\n",
      "Epoch: 1 \t Validation precision@k25: 0.13, accuracy@k25: 0.13\n",
      "Epoch: 1 \t Validation precision@k30: 0.15, accuracy@k30: 0.15\n",
      "CPU: 19.32\n",
      "RAM %: 61.7\n",
      "Epoch: 2 \t Training Loss: 0.131998\n",
      "Epoch: 2 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 2 \t Validation precision@k10: 0.07, accuracy@k10: 0.05\n",
      "Epoch: 2 \t Validation precision@k15: 0.08, accuracy@k15: 0.08\n",
      "Epoch: 2 \t Validation precision@k20: 0.11, accuracy@k20: 0.11\n",
      "Epoch: 2 \t Validation precision@k25: 0.13, accuracy@k25: 0.13\n",
      "Epoch: 2 \t Validation precision@k30: 0.16, accuracy@k30: 0.16\n",
      "CPU: 19.39\n",
      "RAM %: 62.3\n",
      "Epoch: 3 \t Training Loss: 0.128315\n",
      "Epoch: 3 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 3 \t Validation precision@k10: 0.07, accuracy@k10: 0.05\n",
      "Epoch: 3 \t Validation precision@k15: 0.08, accuracy@k15: 0.08\n",
      "Epoch: 3 \t Validation precision@k20: 0.11, accuracy@k20: 0.11\n",
      "Epoch: 3 \t Validation precision@k25: 0.13, accuracy@k25: 0.13\n",
      "Epoch: 3 \t Validation precision@k30: 0.16, accuracy@k30: 0.16\n",
      "CPU: 19.42\n",
      "RAM %: 62.8\n",
      "Epoch: 4 \t Training Loss: 0.125197\n",
      "Epoch: 4 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 4 \t Validation precision@k10: 0.07, accuracy@k10: 0.05\n",
      "Epoch: 4 \t Validation precision@k15: 0.09, accuracy@k15: 0.08\n",
      "Epoch: 4 \t Validation precision@k20: 0.11, accuracy@k20: 0.11\n",
      "Epoch: 4 \t Validation precision@k25: 0.14, accuracy@k25: 0.14\n",
      "Epoch: 4 \t Validation precision@k30: 0.17, accuracy@k30: 0.17\n",
      "CPU: 19.45\n",
      "RAM %: 63.0\n",
      "Epoch: 5 \t Training Loss: 0.122375\n",
      "Epoch: 5 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 5 \t Validation precision@k10: 0.07, accuracy@k10: 0.06\n",
      "Epoch: 5 \t Validation precision@k15: 0.09, accuracy@k15: 0.08\n",
      "Epoch: 5 \t Validation precision@k20: 0.11, accuracy@k20: 0.11\n",
      "Epoch: 5 \t Validation precision@k25: 0.14, accuracy@k25: 0.14\n",
      "Epoch: 5 \t Validation precision@k30: 0.17, accuracy@k30: 0.17\n",
      "CPU: 20.88\n",
      "RAM %: 63.3\n",
      "Epoch: 6 \t Training Loss: 0.119747\n",
      "Epoch: 6 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 6 \t Validation precision@k10: 0.07, accuracy@k10: 0.06\n",
      "Epoch: 6 \t Validation precision@k15: 0.09, accuracy@k15: 0.09\n",
      "Epoch: 6 \t Validation precision@k20: 0.12, accuracy@k20: 0.11\n",
      "Epoch: 6 \t Validation precision@k25: 0.14, accuracy@k25: 0.14\n",
      "Epoch: 6 \t Validation precision@k30: 0.17, accuracy@k30: 0.17\n",
      "CPU: 21.07\n",
      "RAM %: 63.7\n",
      "Epoch: 7 \t Training Loss: 0.117259\n",
      "Epoch: 7 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 7 \t Validation precision@k10: 0.07, accuracy@k10: 0.06\n",
      "Epoch: 7 \t Validation precision@k15: 0.09, accuracy@k15: 0.09\n",
      "Epoch: 7 \t Validation precision@k20: 0.12, accuracy@k20: 0.12\n",
      "Epoch: 7 \t Validation precision@k25: 0.15, accuracy@k25: 0.15\n",
      "Epoch: 7 \t Validation precision@k30: 0.18, accuracy@k30: 0.18\n",
      "CPU: 21.14\n",
      "RAM %: 63.8\n",
      "Epoch: 8 \t Training Loss: 0.114848\n",
      "Epoch: 8 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 8 \t Validation precision@k10: 0.07, accuracy@k10: 0.06\n",
      "Epoch: 8 \t Validation precision@k15: 0.09, accuracy@k15: 0.09\n",
      "Epoch: 8 \t Validation precision@k20: 0.12, accuracy@k20: 0.12\n",
      "Epoch: 8 \t Validation precision@k25: 0.15, accuracy@k25: 0.15\n",
      "Epoch: 8 \t Validation precision@k30: 0.18, accuracy@k30: 0.18\n",
      "CPU: 21.09\n",
      "RAM %: 64.1\n",
      "Epoch: 9 \t Training Loss: 0.112501\n",
      "Epoch: 9 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 9 \t Validation precision@k10: 0.08, accuracy@k10: 0.06\n",
      "Epoch: 9 \t Validation precision@k15: 0.09, accuracy@k15: 0.09\n",
      "Epoch: 9 \t Validation precision@k20: 0.12, accuracy@k20: 0.12\n",
      "Epoch: 9 \t Validation precision@k25: 0.15, accuracy@k25: 0.15\n",
      "Epoch: 9 \t Validation precision@k30: 0.18, accuracy@k30: 0.18\n",
      "CPU: 21.43\n",
      "RAM %: 64.9\n",
      "Epoch: 10 \t Training Loss: 0.110201\n",
      "Epoch: 10 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 10 \t Validation precision@k10: 0.08, accuracy@k10: 0.06\n",
      "Epoch: 10 \t Validation precision@k15: 0.10, accuracy@k15: 0.09\n",
      "Epoch: 10 \t Validation precision@k20: 0.12, accuracy@k20: 0.12\n",
      "Epoch: 10 \t Validation precision@k25: 0.15, accuracy@k25: 0.15\n",
      "Epoch: 10 \t Validation precision@k30: 0.18, accuracy@k30: 0.18\n",
      "CPU: 21.99\n",
      "RAM %: 65.0\n",
      "Epoch: 11 \t Training Loss: 0.107948\n",
      "Epoch: 11 \t Validation precision@k5: 0.06, accuracy@k5: 0.03\n",
      "Epoch: 11 \t Validation precision@k10: 0.08, accuracy@k10: 0.06\n",
      "Epoch: 11 \t Validation precision@k15: 0.10, accuracy@k15: 0.09\n",
      "Epoch: 11 \t Validation precision@k20: 0.12, accuracy@k20: 0.12\n",
      "Epoch: 11 \t Validation precision@k25: 0.16, accuracy@k25: 0.16\n",
      "Epoch: 11 \t Validation precision@k30: 0.19, accuracy@k30: 0.19\n",
      "CPU: 22.25\n",
      "RAM %: 65.1\n",
      "Epoch: 12 \t Training Loss: 0.105724\n",
      "Epoch: 12 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 12 \t Validation precision@k10: 0.08, accuracy@k10: 0.06\n",
      "Epoch: 12 \t Validation precision@k15: 0.10, accuracy@k15: 0.09\n",
      "Epoch: 12 \t Validation precision@k20: 0.13, accuracy@k20: 0.13\n",
      "Epoch: 12 \t Validation precision@k25: 0.16, accuracy@k25: 0.16\n",
      "Epoch: 12 \t Validation precision@k30: 0.19, accuracy@k30: 0.19\n",
      "CPU: 22.69\n",
      "RAM %: 65.0\n",
      "Epoch: 13 \t Training Loss: 0.103514\n",
      "Epoch: 13 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 13 \t Validation precision@k10: 0.08, accuracy@k10: 0.06\n",
      "Epoch: 13 \t Validation precision@k15: 0.10, accuracy@k15: 0.10\n",
      "Epoch: 13 \t Validation precision@k20: 0.13, accuracy@k20: 0.13\n",
      "Epoch: 13 \t Validation precision@k25: 0.16, accuracy@k25: 0.16\n",
      "Epoch: 13 \t Validation precision@k30: 0.19, accuracy@k30: 0.19\n",
      "CPU: 22.48\n",
      "RAM %: 65.1\n",
      "Epoch: 14 \t Training Loss: 0.101335\n",
      "Epoch: 14 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 14 \t Validation precision@k10: 0.08, accuracy@k10: 0.07\n",
      "Epoch: 14 \t Validation precision@k15: 0.10, accuracy@k15: 0.10\n",
      "Epoch: 14 \t Validation precision@k20: 0.13, accuracy@k20: 0.13\n",
      "Epoch: 14 \t Validation precision@k25: 0.16, accuracy@k25: 0.16\n",
      "Epoch: 14 \t Validation precision@k30: 0.20, accuracy@k30: 0.20\n",
      "CPU: 22.80\n",
      "RAM %: 65.2\n",
      "Epoch: 15 \t Training Loss: 0.099156\n",
      "Epoch: 15 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 15 \t Validation precision@k10: 0.08, accuracy@k10: 0.07\n",
      "Epoch: 15 \t Validation precision@k15: 0.10, accuracy@k15: 0.10\n",
      "Epoch: 15 \t Validation precision@k20: 0.13, accuracy@k20: 0.13\n",
      "Epoch: 15 \t Validation precision@k25: 0.17, accuracy@k25: 0.17\n",
      "Epoch: 15 \t Validation precision@k30: 0.20, accuracy@k30: 0.20\n",
      "CPU: 22.71\n",
      "RAM %: 65.3\n",
      "Epoch: 16 \t Training Loss: 0.097007\n",
      "Epoch: 16 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 16 \t Validation precision@k10: 0.08, accuracy@k10: 0.07\n",
      "Epoch: 16 \t Validation precision@k15: 0.10, accuracy@k15: 0.10\n",
      "Epoch: 16 \t Validation precision@k20: 0.13, accuracy@k20: 0.13\n",
      "Epoch: 16 \t Validation precision@k25: 0.17, accuracy@k25: 0.17\n",
      "Epoch: 16 \t Validation precision@k30: 0.20, accuracy@k30: 0.20\n",
      "CPU: 22.61\n",
      "RAM %: 65.4\n",
      "Epoch: 17 \t Training Loss: 0.094889\n",
      "Epoch: 17 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 17 \t Validation precision@k10: 0.08, accuracy@k10: 0.07\n",
      "Epoch: 17 \t Validation precision@k15: 0.11, accuracy@k15: 0.10\n",
      "Epoch: 17 \t Validation precision@k20: 0.14, accuracy@k20: 0.14\n",
      "Epoch: 17 \t Validation precision@k25: 0.17, accuracy@k25: 0.17\n",
      "Epoch: 17 \t Validation precision@k30: 0.20, accuracy@k30: 0.20\n",
      "CPU: 22.05\n",
      "RAM %: 65.3\n",
      "Epoch: 18 \t Training Loss: 0.092775\n",
      "Epoch: 18 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 18 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 18 \t Validation precision@k15: 0.11, accuracy@k15: 0.10\n",
      "Epoch: 18 \t Validation precision@k20: 0.14, accuracy@k20: 0.14\n",
      "Epoch: 18 \t Validation precision@k25: 0.17, accuracy@k25: 0.17\n",
      "Epoch: 18 \t Validation precision@k30: 0.21, accuracy@k30: 0.21\n",
      "CPU: 22.48\n",
      "RAM %: 65.4\n",
      "Epoch: 19 \t Training Loss: 0.090693\n",
      "Epoch: 19 \t Validation precision@k5: 0.07, accuracy@k5: 0.03\n",
      "Epoch: 19 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 19 \t Validation precision@k15: 0.11, accuracy@k15: 0.10\n",
      "Epoch: 19 \t Validation precision@k20: 0.14, accuracy@k20: 0.14\n",
      "Epoch: 19 \t Validation precision@k25: 0.17, accuracy@k25: 0.17\n",
      "Epoch: 19 \t Validation precision@k30: 0.21, accuracy@k30: 0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 22.29\n",
      "RAM %: 65.5\n",
      "Epoch: 20 \t Training Loss: 0.088615\n",
      "Epoch: 20 \t Validation precision@k5: 0.07, accuracy@k5: 0.04\n",
      "Epoch: 20 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 20 \t Validation precision@k15: 0.11, accuracy@k15: 0.11\n",
      "Epoch: 20 \t Validation precision@k20: 0.14, accuracy@k20: 0.14\n",
      "Epoch: 20 \t Validation precision@k25: 0.18, accuracy@k25: 0.18\n",
      "Epoch: 20 \t Validation precision@k30: 0.21, accuracy@k30: 0.21\n",
      "CPU: 22.03\n",
      "RAM %: 53.7\n",
      "Epoch: 21 \t Training Loss: 0.086547\n",
      "Epoch: 21 \t Validation precision@k5: 0.07, accuracy@k5: 0.04\n",
      "Epoch: 21 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 21 \t Validation precision@k15: 0.11, accuracy@k15: 0.11\n",
      "Epoch: 21 \t Validation precision@k20: 0.14, accuracy@k20: 0.14\n",
      "Epoch: 21 \t Validation precision@k25: 0.18, accuracy@k25: 0.18\n",
      "Epoch: 21 \t Validation precision@k30: 0.22, accuracy@k30: 0.22\n",
      "CPU: 22.03\n",
      "RAM %: 54.4\n",
      "Epoch: 22 \t Training Loss: 0.084498\n",
      "Epoch: 22 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 22 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 22 \t Validation precision@k15: 0.11, accuracy@k15: 0.11\n",
      "Epoch: 22 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 22 \t Validation precision@k25: 0.18, accuracy@k25: 0.18\n",
      "Epoch: 22 \t Validation precision@k30: 0.22, accuracy@k30: 0.22\n",
      "CPU: 22.38\n",
      "RAM %: 55.2\n",
      "Epoch: 23 \t Training Loss: 0.082480\n",
      "Epoch: 23 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 23 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 23 \t Validation precision@k15: 0.11, accuracy@k15: 0.11\n",
      "Epoch: 23 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 23 \t Validation precision@k25: 0.18, accuracy@k25: 0.18\n",
      "Epoch: 23 \t Validation precision@k30: 0.22, accuracy@k30: 0.22\n",
      "CPU: 22.51\n",
      "RAM %: 56.6\n",
      "Epoch: 24 \t Training Loss: 0.080474\n",
      "Epoch: 24 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 24 \t Validation precision@k10: 0.09, accuracy@k10: 0.07\n",
      "Epoch: 24 \t Validation precision@k15: 0.12, accuracy@k15: 0.11\n",
      "Epoch: 24 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 24 \t Validation precision@k25: 0.19, accuracy@k25: 0.19\n",
      "Epoch: 24 \t Validation precision@k30: 0.22, accuracy@k30: 0.22\n",
      "CPU: 22.08\n",
      "RAM %: 57.3\n",
      "Epoch: 25 \t Training Loss: 0.078505\n",
      "Epoch: 25 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 25 \t Validation precision@k10: 0.09, accuracy@k10: 0.08\n",
      "Epoch: 25 \t Validation precision@k15: 0.12, accuracy@k15: 0.11\n",
      "Epoch: 25 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 25 \t Validation precision@k25: 0.19, accuracy@k25: 0.19\n",
      "Epoch: 25 \t Validation precision@k30: 0.23, accuracy@k30: 0.23\n",
      "CPU: 22.51\n",
      "RAM %: 58.2\n",
      "Epoch: 26 \t Training Loss: 0.076546\n",
      "Epoch: 26 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 26 \t Validation precision@k10: 0.09, accuracy@k10: 0.08\n",
      "Epoch: 26 \t Validation precision@k15: 0.12, accuracy@k15: 0.11\n",
      "Epoch: 26 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 26 \t Validation precision@k25: 0.19, accuracy@k25: 0.19\n",
      "Epoch: 26 \t Validation precision@k30: 0.23, accuracy@k30: 0.23\n",
      "CPU: 22.45\n",
      "RAM %: 59.3\n",
      "Epoch: 27 \t Training Loss: 0.074597\n",
      "Epoch: 27 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 27 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 27 \t Validation precision@k15: 0.12, accuracy@k15: 0.12\n",
      "Epoch: 27 \t Validation precision@k20: 0.15, accuracy@k20: 0.15\n",
      "Epoch: 27 \t Validation precision@k25: 0.19, accuracy@k25: 0.19\n",
      "Epoch: 27 \t Validation precision@k30: 0.23, accuracy@k30: 0.23\n",
      "CPU: 22.72\n",
      "RAM %: 59.5\n",
      "Epoch: 28 \t Training Loss: 0.072694\n",
      "Epoch: 28 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 28 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 28 \t Validation precision@k15: 0.12, accuracy@k15: 0.12\n",
      "Epoch: 28 \t Validation precision@k20: 0.16, accuracy@k20: 0.16\n",
      "Epoch: 28 \t Validation precision@k25: 0.19, accuracy@k25: 0.19\n",
      "Epoch: 28 \t Validation precision@k30: 0.23, accuracy@k30: 0.23\n",
      "CPU: 22.80\n",
      "RAM %: 59.7\n",
      "Epoch: 29 \t Training Loss: 0.070794\n",
      "Epoch: 29 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 29 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 29 \t Validation precision@k15: 0.12, accuracy@k15: 0.12\n",
      "Epoch: 29 \t Validation precision@k20: 0.16, accuracy@k20: 0.16\n",
      "Epoch: 29 \t Validation precision@k25: 0.20, accuracy@k25: 0.20\n",
      "Epoch: 29 \t Validation precision@k30: 0.24, accuracy@k30: 0.24\n",
      "CPU: 23.54\n",
      "RAM %: 60.3\n",
      "Epoch: 30 \t Training Loss: 0.068930\n",
      "Epoch: 30 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 30 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 30 \t Validation precision@k15: 0.12, accuracy@k15: 0.12\n",
      "Epoch: 30 \t Validation precision@k20: 0.16, accuracy@k20: 0.16\n",
      "Epoch: 30 \t Validation precision@k25: 0.20, accuracy@k25: 0.20\n",
      "Epoch: 30 \t Validation precision@k30: 0.24, accuracy@k30: 0.24\n",
      "CPU: 23.55\n",
      "RAM %: 60.8\n",
      "Epoch: 31 \t Training Loss: 0.067124\n",
      "Epoch: 31 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 31 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 31 \t Validation precision@k15: 0.13, accuracy@k15: 0.12\n",
      "Epoch: 31 \t Validation precision@k20: 0.16, accuracy@k20: 0.16\n",
      "Epoch: 31 \t Validation precision@k25: 0.20, accuracy@k25: 0.20\n",
      "Epoch: 31 \t Validation precision@k30: 0.24, accuracy@k30: 0.24\n",
      "CPU: 23.75\n",
      "RAM %: 60.9\n",
      "Epoch: 32 \t Training Loss: 0.065337\n",
      "Epoch: 32 \t Validation precision@k5: 0.08, accuracy@k5: 0.04\n",
      "Epoch: 32 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 32 \t Validation precision@k15: 0.13, accuracy@k15: 0.12\n",
      "Epoch: 32 \t Validation precision@k20: 0.16, accuracy@k20: 0.16\n",
      "Epoch: 32 \t Validation precision@k25: 0.20, accuracy@k25: 0.20\n",
      "Epoch: 32 \t Validation precision@k30: 0.24, accuracy@k30: 0.24\n",
      "CPU: 23.69\n",
      "RAM %: 61.3\n",
      "Epoch: 33 \t Training Loss: 0.063580\n",
      "Epoch: 33 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 33 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 33 \t Validation precision@k15: 0.13, accuracy@k15: 0.12\n",
      "Epoch: 33 \t Validation precision@k20: 0.17, accuracy@k20: 0.16\n",
      "Epoch: 33 \t Validation precision@k25: 0.21, accuracy@k25: 0.21\n",
      "Epoch: 33 \t Validation precision@k30: 0.25, accuracy@k30: 0.25\n",
      "CPU: 23.62\n",
      "RAM %: 61.3\n",
      "Epoch: 34 \t Training Loss: 0.061860\n",
      "Epoch: 34 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 34 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 34 \t Validation precision@k15: 0.13, accuracy@k15: 0.13\n",
      "Epoch: 34 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 34 \t Validation precision@k25: 0.21, accuracy@k25: 0.21\n",
      "Epoch: 34 \t Validation precision@k30: 0.25, accuracy@k30: 0.25\n",
      "CPU: 23.93\n",
      "RAM %: 61.4\n",
      "Epoch: 35 \t Training Loss: 0.060196\n",
      "Epoch: 35 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 35 \t Validation precision@k10: 0.10, accuracy@k10: 0.08\n",
      "Epoch: 35 \t Validation precision@k15: 0.13, accuracy@k15: 0.13\n",
      "Epoch: 35 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 35 \t Validation precision@k25: 0.21, accuracy@k25: 0.21\n",
      "Epoch: 35 \t Validation precision@k30: 0.25, accuracy@k30: 0.25\n",
      "CPU: 23.95\n",
      "RAM %: 61.7\n",
      "Epoch: 36 \t Training Loss: 0.058591\n",
      "Epoch: 36 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 36 \t Validation precision@k10: 0.10, accuracy@k10: 0.09\n",
      "Epoch: 36 \t Validation precision@k15: 0.13, accuracy@k15: 0.13\n",
      "Epoch: 36 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 36 \t Validation precision@k25: 0.21, accuracy@k25: 0.21\n",
      "Epoch: 36 \t Validation precision@k30: 0.25, accuracy@k30: 0.25\n",
      "CPU: 23.88\n",
      "RAM %: 61.8\n",
      "Epoch: 37 \t Training Loss: 0.057039\n",
      "Epoch: 37 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 37 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 37 \t Validation precision@k15: 0.13, accuracy@k15: 0.13\n",
      "Epoch: 37 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 37 \t Validation precision@k25: 0.21, accuracy@k25: 0.21\n",
      "Epoch: 37 \t Validation precision@k30: 0.26, accuracy@k30: 0.26\n",
      "CPU: 24.31\n",
      "RAM %: 62.0\n",
      "Epoch: 38 \t Training Loss: 0.055480\n",
      "Epoch: 38 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 38 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 38 \t Validation precision@k15: 0.13, accuracy@k15: 0.13\n",
      "Epoch: 38 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 38 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation precision@k30: 0.26, accuracy@k30: 0.26\n",
      "CPU: 23.97\n",
      "RAM %: 62.5\n",
      "Epoch: 39 \t Training Loss: 0.053969\n",
      "Epoch: 39 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 39 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 39 \t Validation precision@k15: 0.14, accuracy@k15: 0.13\n",
      "Epoch: 39 \t Validation precision@k20: 0.17, accuracy@k20: 0.17\n",
      "Epoch: 39 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n",
      "Epoch: 39 \t Validation precision@k30: 0.26, accuracy@k30: 0.26\n",
      "CPU: 23.99\n",
      "RAM %: 62.7\n",
      "Epoch: 40 \t Training Loss: 0.052539\n",
      "Epoch: 40 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 40 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 40 \t Validation precision@k15: 0.14, accuracy@k15: 0.13\n",
      "Epoch: 40 \t Validation precision@k20: 0.18, accuracy@k20: 0.17\n",
      "Epoch: 40 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n",
      "Epoch: 40 \t Validation precision@k30: 0.26, accuracy@k30: 0.26\n",
      "CPU: 23.78\n",
      "RAM %: 62.8\n",
      "Epoch: 41 \t Training Loss: 0.051136\n",
      "Epoch: 41 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 41 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 41 \t Validation precision@k15: 0.14, accuracy@k15: 0.13\n",
      "Epoch: 41 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 41 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n",
      "Epoch: 41 \t Validation precision@k30: 0.26, accuracy@k30: 0.26\n",
      "CPU: 23.64\n",
      "RAM %: 63.1\n",
      "Epoch: 42 \t Training Loss: 0.049799\n",
      "Epoch: 42 \t Validation precision@k5: 0.09, accuracy@k5: 0.04\n",
      "Epoch: 42 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 42 \t Validation precision@k15: 0.14, accuracy@k15: 0.13\n",
      "Epoch: 42 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 42 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n",
      "Epoch: 42 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 23.43\n",
      "RAM %: 63.2\n",
      "Epoch: 43 \t Training Loss: 0.048417\n",
      "Epoch: 43 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 43 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 43 \t Validation precision@k15: 0.14, accuracy@k15: 0.13\n",
      "Epoch: 43 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 43 \t Validation precision@k25: 0.22, accuracy@k25: 0.22\n",
      "Epoch: 43 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 23.76\n",
      "RAM %: 63.5\n",
      "Epoch: 44 \t Training Loss: 0.047126\n",
      "Epoch: 44 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 44 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 44 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 44 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 44 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 44 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 24.28\n",
      "RAM %: 64.3\n",
      "Epoch: 45 \t Training Loss: 0.045828\n",
      "Epoch: 45 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 45 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 45 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 45 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 45 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 45 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 24.76\n",
      "RAM %: 64.2\n",
      "Epoch: 46 \t Training Loss: 0.044418\n",
      "Epoch: 46 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 46 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 46 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 46 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 46 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 46 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 24.84\n",
      "RAM %: 64.4\n",
      "Epoch: 47 \t Training Loss: 0.043089\n",
      "Epoch: 47 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 47 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 47 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 47 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 47 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 47 \t Validation precision@k30: 0.27, accuracy@k30: 0.27\n",
      "CPU: 24.97\n",
      "RAM %: 64.4\n",
      "Epoch: 48 \t Training Loss: 0.041888\n",
      "Epoch: 48 \t Validation precision@k5: 0.09, accuracy@k5: 0.05\n",
      "Epoch: 48 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 48 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 48 \t Validation precision@k20: 0.18, accuracy@k20: 0.18\n",
      "Epoch: 48 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 48 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 24.52\n",
      "RAM %: 64.5\n",
      "Epoch: 49 \t Training Loss: 0.040687\n",
      "Epoch: 49 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 49 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 49 \t Validation precision@k15: 0.14, accuracy@k15: 0.14\n",
      "Epoch: 49 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 49 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 49 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 25.08\n",
      "RAM %: 64.9\n",
      "Epoch: 50 \t Training Loss: 0.039544\n",
      "Epoch: 50 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 50 \t Validation precision@k10: 0.11, accuracy@k10: 0.09\n",
      "Epoch: 50 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 50 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 50 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 50 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 25.32\n",
      "RAM %: 65.0\n",
      "Epoch: 51 \t Training Loss: 0.038388\n",
      "Epoch: 51 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 51 \t Validation precision@k10: 0.12, accuracy@k10: 0.09\n",
      "Epoch: 51 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 51 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 51 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 51 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 24.93\n",
      "RAM %: 64.9\n",
      "Epoch: 52 \t Training Loss: 0.037247\n",
      "Epoch: 52 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 52 \t Validation precision@k10: 0.12, accuracy@k10: 0.09\n",
      "Epoch: 52 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 52 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 52 \t Validation precision@k25: 0.23, accuracy@k25: 0.23\n",
      "Epoch: 52 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 24.97\n",
      "RAM %: 65.3\n",
      "Epoch: 53 \t Training Loss: 0.036167\n",
      "Epoch: 53 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 53 \t Validation precision@k10: 0.12, accuracy@k10: 0.09\n",
      "Epoch: 53 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 53 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 53 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 53 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 25.11\n",
      "RAM %: 65.4\n",
      "Epoch: 54 \t Training Loss: 0.035122\n",
      "Epoch: 54 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 54 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 54 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 54 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 54 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 54 \t Validation precision@k30: 0.28, accuracy@k30: 0.28\n",
      "CPU: 25.28\n",
      "RAM %: 66.0\n",
      "Epoch: 55 \t Training Loss: 0.034138\n",
      "Epoch: 55 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 55 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 55 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 55 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 55 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 55 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 25.77\n",
      "RAM %: 66.4\n",
      "Epoch: 56 \t Training Loss: 0.033191\n",
      "Epoch: 56 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 56 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 56 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 56 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 56 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 56 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 25.68\n",
      "RAM %: 66.6\n",
      "Epoch: 57 \t Training Loss: 0.032240\n",
      "Epoch: 57 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 57 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 57 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 57 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 57 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 25.93\n",
      "RAM %: 67.3\n",
      "Epoch: 58 \t Training Loss: 0.031287\n",
      "Epoch: 58 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 58 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 58 \t Validation precision@k15: 0.15, accuracy@k15: 0.14\n",
      "Epoch: 58 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 58 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 58 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.06\n",
      "RAM %: 67.6\n",
      "Epoch: 59 \t Training Loss: 0.030392\n",
      "Epoch: 59 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 59 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 59 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 59 \t Validation precision@k20: 0.19, accuracy@k20: 0.19\n",
      "Epoch: 59 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 59 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.49\n",
      "RAM %: 67.8\n",
      "Epoch: 60 \t Training Loss: 0.029574\n",
      "Epoch: 60 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 60 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 60 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 60 \t Validation precision@k20: 0.20, accuracy@k20: 0.19\n",
      "Epoch: 60 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 60 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.78\n",
      "RAM %: 68.3\n",
      "Epoch: 61 \t Training Loss: 0.028770\n",
      "Epoch: 61 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 61 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 61 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 61 \t Validation precision@k20: 0.20, accuracy@k20: 0.19\n",
      "Epoch: 61 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 61 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.69\n",
      "RAM %: 67.5\n",
      "Epoch: 62 \t Training Loss: 0.027961\n",
      "Epoch: 62 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 62 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 62 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 62 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 62 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 62 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.40\n",
      "RAM %: 67.6\n",
      "Epoch: 63 \t Training Loss: 0.027123\n",
      "Epoch: 63 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 63 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 63 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 63 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 63 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 63 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.51\n",
      "RAM %: 70.0\n",
      "Epoch: 64 \t Training Loss: 0.026329\n",
      "Epoch: 64 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 64 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 64 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 64 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 64 \t Validation precision@k25: 0.24, accuracy@k25: 0.24\n",
      "Epoch: 64 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.52\n",
      "RAM %: 70.1\n",
      "Epoch: 65 \t Training Loss: 0.025614\n",
      "Epoch: 65 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 65 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 65 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 65 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 65 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 65 \t Validation precision@k30: 0.29, accuracy@k30: 0.29\n",
      "CPU: 26.42\n",
      "RAM %: 71.0\n",
      "Epoch: 66 \t Training Loss: 0.024916\n",
      "Epoch: 66 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 66 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 66 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 66 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 66 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 66 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.39\n",
      "RAM %: 72.1\n",
      "Epoch: 67 \t Training Loss: 0.024225\n",
      "Epoch: 67 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 67 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 67 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 67 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 67 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 67 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.71\n",
      "RAM %: 47.4\n",
      "Epoch: 68 \t Training Loss: 0.023520\n",
      "Epoch: 68 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 68 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 68 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 68 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 68 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 68 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.10\n",
      "RAM %: 51.9\n",
      "Epoch: 69 \t Training Loss: 0.022835\n",
      "Epoch: 69 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 69 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 69 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 69 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 69 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 69 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.72\n",
      "RAM %: 53.5\n",
      "Epoch: 70 \t Training Loss: 0.022204\n",
      "Epoch: 70 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 70 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 70 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 70 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 70 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 70 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.61\n",
      "RAM %: 49.0\n",
      "Epoch: 71 \t Training Loss: 0.021677\n",
      "Epoch: 71 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 71 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 71 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 71 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 71 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 71 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.35\n",
      "RAM %: 48.2\n",
      "Epoch: 72 \t Training Loss: 0.021079\n",
      "Epoch: 72 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 72 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 72 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 72 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 72 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 72 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.44\n",
      "RAM %: 48.0\n",
      "Epoch: 73 \t Training Loss: 0.020453\n",
      "Epoch: 73 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 73 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 73 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 73 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 73 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 73 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 26.98\n",
      "RAM %: 48.5\n",
      "Epoch: 74 \t Training Loss: 0.019856\n",
      "Epoch: 74 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 74 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 74 \t Validation precision@k15: 0.15, accuracy@k15: 0.15\n",
      "Epoch: 74 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 74 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 74 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.09\n",
      "RAM %: 48.5\n",
      "Epoch: 75 \t Training Loss: 0.019367\n",
      "Epoch: 75 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 75 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 75 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 75 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 75 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 75 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.11\n",
      "RAM %: 48.8\n",
      "Epoch: 76 \t Training Loss: 0.018838\n",
      "Epoch: 76 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 76 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 76 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 76 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 76 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 28.28\n",
      "RAM %: 51.6\n",
      "Epoch: 77 \t Training Loss: 0.018270\n",
      "Epoch: 77 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 77 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 77 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 77 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 77 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 77 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 28.10\n",
      "RAM %: 51.0\n",
      "Epoch: 78 \t Training Loss: 0.017733\n",
      "Epoch: 78 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 78 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 78 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 78 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 78 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 78 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.60\n",
      "RAM %: 52.1\n",
      "Epoch: 79 \t Training Loss: 0.017240\n",
      "Epoch: 79 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 79 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 79 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 79 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 79 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 79 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.70\n",
      "RAM %: 51.0\n",
      "Epoch: 80 \t Training Loss: 0.016855\n",
      "Epoch: 80 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 80 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 80 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 80 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 80 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 80 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 27.73\n",
      "RAM %: 51.3\n",
      "Epoch: 81 \t Training Loss: 0.016364\n",
      "Epoch: 81 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 81 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 81 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 81 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 81 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 81 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 28.60\n",
      "RAM %: 67.8\n",
      "Epoch: 82 \t Training Loss: 0.015848\n",
      "Epoch: 82 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 82 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 82 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 82 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 82 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 82 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 51.13\n",
      "RAM %: 65.7\n",
      "Epoch: 83 \t Training Loss: 0.015382\n",
      "Epoch: 83 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 83 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 83 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 83 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 83 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 83 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 51.05\n",
      "RAM %: 61.5\n",
      "Epoch: 84 \t Training Loss: 0.014923\n",
      "Epoch: 84 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 84 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 84 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 84 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 84 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 84 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 49.77\n",
      "RAM %: 56.9\n",
      "Epoch: 85 \t Training Loss: 0.014459\n",
      "Epoch: 85 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 85 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 85 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 85 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 85 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 85 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 48.97\n",
      "RAM %: 58.6\n",
      "Epoch: 86 \t Training Loss: 0.013991\n",
      "Epoch: 86 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 86 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 86 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 86 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 86 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 86 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 48.33\n",
      "RAM %: 60.2\n",
      "Epoch: 87 \t Training Loss: 0.013573\n",
      "Epoch: 87 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 87 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 87 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 87 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 87 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 87 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 47.88\n",
      "RAM %: 59.6\n",
      "Epoch: 88 \t Training Loss: 0.013173\n",
      "Epoch: 88 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 88 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 88 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 88 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 88 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 88 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 46.67\n",
      "RAM %: 59.0\n",
      "Epoch: 89 \t Training Loss: 0.012775\n",
      "Epoch: 89 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 89 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 89 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 89 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 89 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 89 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 46.14\n",
      "RAM %: 59.3\n",
      "Epoch: 90 \t Training Loss: 0.012386\n",
      "Epoch: 90 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 90 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 90 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 90 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 90 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 90 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 45.36\n",
      "RAM %: 59.1\n",
      "Epoch: 91 \t Training Loss: 0.012013\n",
      "Epoch: 91 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 91 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 91 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 91 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 91 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 91 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 44.91\n",
      "RAM %: 59.3\n",
      "Epoch: 92 \t Training Loss: 0.011691\n",
      "Epoch: 92 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 92 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 92 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 92 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 92 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 92 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 43.99\n",
      "RAM %: 59.3\n",
      "Epoch: 93 \t Training Loss: 0.011386\n",
      "Epoch: 93 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 93 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 93 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 93 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 93 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 93 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 43.80\n",
      "RAM %: 59.1\n",
      "Epoch: 94 \t Training Loss: 0.011065\n",
      "Epoch: 94 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 94 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 94 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 94 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 94 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 94 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 43.62\n",
      "RAM %: 59.2\n",
      "Epoch: 95 \t Training Loss: 0.010734\n",
      "Epoch: 95 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 95 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 95 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 95 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 95 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 42.32\n",
      "RAM %: 59.5\n",
      "Epoch: 96 \t Training Loss: 0.010425\n",
      "Epoch: 96 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 96 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 96 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 96 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 96 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 96 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 41.65\n",
      "RAM %: 59.2\n",
      "Epoch: 97 \t Training Loss: 0.010143\n",
      "Epoch: 97 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 97 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 97 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 97 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 97 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 97 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 40.72\n",
      "RAM %: 59.7\n",
      "Epoch: 98 \t Training Loss: 0.009874\n",
      "Epoch: 98 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 98 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 98 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 98 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 98 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 98 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 40.18\n",
      "RAM %: 59.1\n",
      "Epoch: 99 \t Training Loss: 0.009607\n",
      "Epoch: 99 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 99 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 99 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 99 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 99 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 99 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU: 39.27\n",
      "RAM %: 59.3\n",
      "Epoch: 100 \t Training Loss: 0.009341\n",
      "Epoch: 100 \t Validation precision@k5: 0.10, accuracy@k5: 0.05\n",
      "Epoch: 100 \t Validation precision@k10: 0.12, accuracy@k10: 0.10\n",
      "Epoch: 100 \t Validation precision@k15: 0.16, accuracy@k15: 0.15\n",
      "Epoch: 100 \t Validation precision@k20: 0.20, accuracy@k20: 0.20\n",
      "Epoch: 100 \t Validation precision@k25: 0.25, accuracy@k25: 0.25\n",
      "Epoch: 100 \t Validation precision@k30: 0.30, accuracy@k30: 0.30\n",
      "CPU times: user 1h 37min 23s, sys: 13min 31s, total: 1h 50min 54s\n",
      "Wall time: 1h 8min 56s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(naive_rnn, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(naive_rnn, test_loader, k=k)\n",
    "    print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.2f}, accuracy@k{k}: {accuracy_k:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acba7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2232eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b7486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23348197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
