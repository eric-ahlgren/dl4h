{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data/\"\n",
    "GRAM_DATA_PATH = \"../Project/code/processed_data/gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m categories \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategories.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m sub_categories \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_PATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubcategories.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(vids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtargets\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(seqs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'targets' is not defined"
     ]
    }
   ],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targs) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1709ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_level1 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level1.pk'), 'rb'))\n",
    "ccs_level2 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level2.pk'), 'rb'))\n",
    "ccs_level3 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level3.pk'), 'rb'))\n",
    "ccs_level4 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level4.pk'), 'rb'))\n",
    "ccs_level5 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.level5.pk'), 'rb'))\n",
    "ccs_seqs = ccs_level1 = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.seqs'), 'rb'))\n",
    "ccs_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'gram_ccs.types'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25767cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_3digit_seqs = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.3digitICD9.seqs'), 'rb'))\n",
    "gram_3digit_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.3digitICD9.types'), 'rb'))\n",
    "gram_dates = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.dates'), 'rb'), encoding='latin1')\n",
    "gram_pids = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.pids'), 'rb'))\n",
    "gram_seqs = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.seqs'), 'rb'))\n",
    "gram_types = pickle.load(open(os.path.join(GRAM_DATA_PATH,'processed_gram.types'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_3digit_types = {v:k for k,v in gram_3digit_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75dbde5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ccs_types' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rtypes_ccs \u001b[38;5;241m=\u001b[39m {v:k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mccs_types\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m      2\u001b[0m rtypes \u001b[38;5;241m=\u001b[39m {v:k \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m gram_types\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ccs_types' is not defined"
     ]
    }
   ],
   "source": [
    "rtypes_ccs = {v:k for k,v in ccs_types.items()}\n",
    "rtypes = {v:k for k,v in gram_types.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "#         x = []\n",
    "#         for i,patient in enumerate(seqs):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 x.append(visit)\n",
    "#         y = []\n",
    "#         for i,patient in enumerate(targets):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 y.append(patient[j+1])\n",
    "\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c9d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_YMask(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    #max_num_categories = max(num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_visits, num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y_masks = torch.zeros((num_patients, max_num_visits, num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             if j_visit == len(patient) - 2:\n",
    "#                 rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                 rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                 rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "    for i_patient, patient in enumerate(targets):   \n",
    "        for j_visit, visit in enumerate(patient[1:]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                y[i_patient, j_visit, k_code] = code\n",
    "                y_masks[i_patient, j_visit, k_code] = 1\n",
    "#             y[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             y_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d83bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    max_num_categories = max(batch_num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient[1:]):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "                \n",
    "    for i_patient, patient in enumerate(targets):   \n",
    "        for visit in patient[-1:]:\n",
    "            for k_code, code in enumerate(visit):\n",
    "                y[i_patient, k_code] = code\n",
    "                y_masks[i_patient, k_code] = 1\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 x[i_patient, j_visit, k_code] = code\n",
    "#                 x_masks[i_patient, j_visit, k_code] = 1\n",
    "#                 if j_visit == len(patient) - 1 and k_code == len(visit) - 1:\n",
    "#                     rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                     rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                     rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c323bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         x = []\n",
    "#         for i,patient in enumerate(seqs):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 x.append(visit)\n",
    "#         y = []\n",
    "#         for i,patient in enumerate(targets):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 y.append(patient[j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218c2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_old(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "    y = torch.tensor(targets, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "#     global categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     num_categories = len(categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     y = torch.zeros((num_patients, num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     y_masks = torch.zeros((num_patients, num_categories), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\" \n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 1 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for j_visit, visit in enumerate(targets):\n",
    "#         for k_cat, cat in enumerate(visit):\n",
    "#             y[j_visit, k_cat] = cat\n",
    "#             y_masks[j_visit, k_cat] = 1\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56cb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    x[~masks] = 0\n",
    "    return x.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3d8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: Consider using `torch.gather()`.\n",
    "    \"\"\"\n",
    "    idx_vector = masks.any(dim=2).sum(1) - 1\n",
    "    p_idx = torch.arange(0,len(hidden_states), dtype=torch.int64)\n",
    "    last_hidden_state = hidden_states[p_idx,idx_vector]\n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe43d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, v, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the hidden states for true visits (not padding visits) and then\n",
    "        sum the them up.\n",
    "\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "    \"\"\"\n",
    "#     import pdb; pdb.set_trace()\n",
    "    masks = masks[:,:].any(dim=2)\n",
    "    v_masked = v.clone()\n",
    "    v_masked[~masks] = 0\n",
    "    a_sum = torch.sum(alpha * v_masked, dim=1)\n",
    "    return a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47eb22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "        \"\"\"\n",
    "        m = nn.Softmax(dim=1)\n",
    "        return m(self.a_att(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineDipole(\n",
       "  (embedding): Embedding(4903, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=256, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineDipole(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.att_a = AlphaAttention(256)\n",
    "        self.fc = nn.Linear(256, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks, y_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "       \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "#         true_h_n = get_last_visit(output, masks)\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        rev_output, _ = self.rev_rnn(rev_x)\n",
    "#         true_h_n_rev = get_last_visit(rev_output, rev_masks)\n",
    "        output_cat = torch.cat([output, rev_output], -1)\n",
    "        x_cat = torch.cat([x, rev_x], -1)\n",
    "        masks_cat = torch.cat([masks, rev_masks], -1)\n",
    "        alpha = self.att_a(output_cat)\n",
    "        c = attention_sum(alpha, x_cat, masks_cat)\n",
    "        logits = self.fc(c)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.softmax(logits)\n",
    "        #probs = self.sigmoid(logits)\n",
    "        \n",
    "#         probs = probs.reshape(probs.shape[0]*probs.shape[1], probs.shape[2])\n",
    "#         y_masks = y_masks.reshape(y_masks.shape[0]*y_masks.shape[1], y_masks.shape[2])\n",
    "#         probs = probs[y_masks.any(dim=1)]\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "baseline_dipole = BaselineDipole(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_dipole.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad8a5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model_old(model, test_loader, threshold=0.5, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, rev_x, rev_masks, y, y_masks in test_loader:\n",
    "            #import pdb; pdb.set_trace()\n",
    "            nn = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, x_masks, rev_x, rev_masks, y_masks)\n",
    "#             num_categories = torch.count_nonzero(y, dim=2)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            num_predictions = 0\n",
    "            num_targets = 0\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "            precision = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "#             y_masks = y_masks.reshape(\n",
    "#                 y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "#             y = y[y_masks.any(dim=1)]\n",
    "#             y_masks = y_masks[y_masks.any(dim=1)]\n",
    "\n",
    "\n",
    "#             v_idx = masks.any(dim=2)\n",
    "#             v_idx = v_idx.sum(dim=1)\n",
    "#             v_idx = v_idx.unsqueeze(-1)\n",
    "#             v_idx = v_idx.repeat(1,y_hat.shape[2])\n",
    "#             v_idx = v_idx.unsqueeze(1)\n",
    "#             y_hat = torch.gather(y_hat,1,v_idx).squeeze()\n",
    "            for i in range(k):\n",
    "                \n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "\n",
    "                y_true = y[i, y_masks[i]].unique()\n",
    "                all_targets.extend(y_true.tolist())\n",
    "                _, y_pred = torch.topk(y_hat[i], len(y_true))\n",
    "                #y_pred = torch.nonzero(y_hat[0] > threshold).squeeze()\n",
    "                if y_pred.numel() > 0:\n",
    "                    try:\n",
    "                        all_predictions.extend(y_pred.tolist())\n",
    "                    except TypeError:\n",
    "                        y_pred = [y_pred.tolist()]\n",
    "                    all_predictions.extend(y_pred)\n",
    "#                     for v in y_pred:\n",
    "#                         if v in y_true:\n",
    "#                             visit_correct += 1\n",
    "                    for v in y_true:\n",
    "                        if v in y_pred:\n",
    "                            visit_correct += 1\n",
    "                    num_predictions += len(y_pred)\n",
    "\n",
    "                num_targets += len(y_true)\n",
    "                precision += visit_correct / min(k, len(y_true))\n",
    "                k_correct += visit_correct\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    " #           import pdb; pdb.set_trace()\n",
    "            precision_k = precision / k\n",
    "#             precision_k1 = k_correct / min(k, num_targets)\n",
    "            if num_predictions == 0:\n",
    "                accuracy_k = 0\n",
    "            else:\n",
    "                accuracy_k = k_correct / num_predictions\n",
    "            precision_k = total_precision / nn\n",
    "            accuracy_k = total_accuracy / nn\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "#     roc_auc = roc_auc_score(y_true, y_score)\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    #base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "#             import pdb; pdb.set_trace()\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks, y_masks)\n",
    "#             import pdb; pdb.set_trace()\n",
    "#             y[~y_masks] = criterion.ignore_index\n",
    "#             last_y = y_masks.any(dim=2).sum(dim=1) - 1\n",
    "#             indices = last_y.unsqueeze(-1)\n",
    "#             indices = indices.repeat(1, y.shape[2])\n",
    "#             indices = indices.unsqueeze(1)\n",
    "#             y_filt = torch.gather(y, 1, indices)\n",
    "\n",
    "#             n_visits = y_masks.any(dim=2).sum(dim=1)\n",
    "#             for i_patient, j_visit in enumerate(n_visits):\n",
    "#                 for visit in range(j_visit - 1):\n",
    "#                     mask = y_masks[i_patient, visit+1]\n",
    "\n",
    "#                     yh = y_hat[i_patient, visit]\n",
    "#                     y_tmp = indices_to_multihot(\n",
    "#                         y[i_patient, visit+1], mask, yh)\n",
    "            \n",
    "            # stack into visits\n",
    "#             yh = y_hat.reshape(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
    "#             y_masks = y_masks.reshape(y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "        \n",
    "                    \n",
    "            \n",
    "            y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "            loss = criterion(y_hat, y_mh)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.2f}, accuracy@k{k}: {accuracy_k:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e77ed755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks, y_masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             num_labels = y_hat.shape[1]\n",
    "#             num_categories = torch.count_nonzero(y, dim=1)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "            k_correct = 0\n",
    "#             predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "                y_true = y[i, y_masks[i]]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "#                 predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 17.34\n",
      "RAM %: 61.8\n",
      "Epoch: 1 \t Training Loss: 39.834602\n",
      "Epoch: 1 \t Validation precision@k5: 0.70, accuracy@k5: 0.37\n",
      "Epoch: 1 \t Validation precision@k10: 0.65, accuracy@k10: 0.56\n",
      "Epoch: 1 \t Validation precision@k15: 0.69, accuracy@k15: 0.67\n",
      "Epoch: 1 \t Validation precision@k20: 0.75, accuracy@k20: 0.75\n",
      "Epoch: 1 \t Validation precision@k25: 0.81, accuracy@k25: 0.80\n",
      "Epoch: 1 \t Validation precision@k30: 0.85, accuracy@k30: 0.85\n",
      "CPU: 17.93\n",
      "RAM %: 61.8\n",
      "Epoch: 2 \t Training Loss: 39.499804\n",
      "Epoch: 2 \t Validation precision@k5: 0.70, accuracy@k5: 0.38\n",
      "Epoch: 2 \t Validation precision@k10: 0.66, accuracy@k10: 0.56\n",
      "Epoch: 2 \t Validation precision@k15: 0.70, accuracy@k15: 0.68\n",
      "Epoch: 2 \t Validation precision@k20: 0.76, accuracy@k20: 0.75\n",
      "Epoch: 2 \t Validation precision@k25: 0.81, accuracy@k25: 0.81\n",
      "Epoch: 2 \t Validation precision@k30: 0.85, accuracy@k30: 0.85\n",
      "CPU: 17.85\n",
      "RAM %: 61.8\n",
      "Epoch: 3 \t Training Loss: 39.276302\n",
      "Epoch: 3 \t Validation precision@k5: 0.72, accuracy@k5: 0.38\n",
      "Epoch: 3 \t Validation precision@k10: 0.67, accuracy@k10: 0.57\n",
      "Epoch: 3 \t Validation precision@k15: 0.71, accuracy@k15: 0.68\n",
      "Epoch: 3 \t Validation precision@k20: 0.77, accuracy@k20: 0.76\n",
      "Epoch: 3 \t Validation precision@k25: 0.82, accuracy@k25: 0.82\n",
      "Epoch: 3 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 18.38\n",
      "RAM %: 61.8\n",
      "Epoch: 4 \t Training Loss: 39.025314\n",
      "Epoch: 4 \t Validation precision@k5: 0.72, accuracy@k5: 0.39\n",
      "Epoch: 4 \t Validation precision@k10: 0.68, accuracy@k10: 0.58\n",
      "Epoch: 4 \t Validation precision@k15: 0.71, accuracy@k15: 0.69\n",
      "Epoch: 4 \t Validation precision@k20: 0.77, accuracy@k20: 0.77\n",
      "Epoch: 4 \t Validation precision@k25: 0.82, accuracy@k25: 0.82\n",
      "Epoch: 4 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 19.10\n",
      "RAM %: 61.9\n",
      "Epoch: 5 \t Training Loss: 38.797397\n",
      "Epoch: 5 \t Validation precision@k5: 0.72, accuracy@k5: 0.39\n",
      "Epoch: 5 \t Validation precision@k10: 0.68, accuracy@k10: 0.58\n",
      "Epoch: 5 \t Validation precision@k15: 0.72, accuracy@k15: 0.70\n",
      "Epoch: 5 \t Validation precision@k20: 0.78, accuracy@k20: 0.77\n",
      "Epoch: 5 \t Validation precision@k25: 0.83, accuracy@k25: 0.83\n",
      "Epoch: 5 \t Validation precision@k30: 0.87, accuracy@k30: 0.87\n",
      "CPU: 19.57\n",
      "RAM %: 61.9\n",
      "Epoch: 6 \t Training Loss: 38.593402\n",
      "Epoch: 6 \t Validation precision@k5: 0.73, accuracy@k5: 0.39\n",
      "Epoch: 6 \t Validation precision@k10: 0.69, accuracy@k10: 0.59\n",
      "Epoch: 6 \t Validation precision@k15: 0.73, accuracy@k15: 0.70\n",
      "Epoch: 6 \t Validation precision@k20: 0.78, accuracy@k20: 0.78\n",
      "Epoch: 6 \t Validation precision@k25: 0.83, accuracy@k25: 0.83\n",
      "Epoch: 6 \t Validation precision@k30: 0.87, accuracy@k30: 0.87\n",
      "CPU: 19.91\n",
      "RAM %: 61.9\n",
      "Epoch: 7 \t Training Loss: 38.384697\n",
      "Epoch: 7 \t Validation precision@k5: 0.74, accuracy@k5: 0.40\n",
      "Epoch: 7 \t Validation precision@k10: 0.70, accuracy@k10: 0.59\n",
      "Epoch: 7 \t Validation precision@k15: 0.73, accuracy@k15: 0.71\n",
      "Epoch: 7 \t Validation precision@k20: 0.79, accuracy@k20: 0.79\n",
      "Epoch: 7 \t Validation precision@k25: 0.84, accuracy@k25: 0.84\n",
      "Epoch: 7 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 20.59\n",
      "RAM %: 61.9\n",
      "Epoch: 8 \t Training Loss: 38.188919\n",
      "Epoch: 8 \t Validation precision@k5: 0.75, accuracy@k5: 0.40\n",
      "Epoch: 8 \t Validation precision@k10: 0.70, accuracy@k10: 0.60\n",
      "Epoch: 8 \t Validation precision@k15: 0.74, accuracy@k15: 0.71\n",
      "Epoch: 8 \t Validation precision@k20: 0.80, accuracy@k20: 0.79\n",
      "Epoch: 8 \t Validation precision@k25: 0.84, accuracy@k25: 0.84\n",
      "Epoch: 8 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 21.23\n",
      "RAM %: 61.9\n",
      "Epoch: 9 \t Training Loss: 38.002388\n",
      "Epoch: 9 \t Validation precision@k5: 0.75, accuracy@k5: 0.40\n",
      "Epoch: 9 \t Validation precision@k10: 0.71, accuracy@k10: 0.61\n",
      "Epoch: 9 \t Validation precision@k15: 0.75, accuracy@k15: 0.72\n",
      "Epoch: 9 \t Validation precision@k20: 0.80, accuracy@k20: 0.80\n",
      "Epoch: 9 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 9 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 21.71\n",
      "RAM %: 61.9\n",
      "Epoch: 10 \t Training Loss: 37.794120\n",
      "Epoch: 10 \t Validation precision@k5: 0.76, accuracy@k5: 0.41\n",
      "Epoch: 10 \t Validation precision@k10: 0.72, accuracy@k10: 0.61\n",
      "Epoch: 10 \t Validation precision@k15: 0.75, accuracy@k15: 0.73\n",
      "Epoch: 10 \t Validation precision@k20: 0.81, accuracy@k20: 0.80\n",
      "Epoch: 10 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 10 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 21.51\n",
      "RAM %: 61.9\n",
      "Epoch: 11 \t Training Loss: 37.617382\n",
      "Epoch: 11 \t Validation precision@k5: 0.76, accuracy@k5: 0.41\n",
      "Epoch: 11 \t Validation precision@k10: 0.72, accuracy@k10: 0.62\n",
      "Epoch: 11 \t Validation precision@k15: 0.76, accuracy@k15: 0.73\n",
      "Epoch: 11 \t Validation precision@k20: 0.81, accuracy@k20: 0.81\n",
      "Epoch: 11 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 11 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 21.77\n",
      "RAM %: 61.9\n",
      "Epoch: 12 \t Training Loss: 37.432720\n",
      "Epoch: 12 \t Validation precision@k5: 0.77, accuracy@k5: 0.41\n",
      "Epoch: 12 \t Validation precision@k10: 0.73, accuracy@k10: 0.62\n",
      "Epoch: 12 \t Validation precision@k15: 0.76, accuracy@k15: 0.74\n",
      "Epoch: 12 \t Validation precision@k20: 0.82, accuracy@k20: 0.81\n",
      "Epoch: 12 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 12 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 22.31\n",
      "RAM %: 61.9\n",
      "Epoch: 13 \t Training Loss: 37.252265\n",
      "Epoch: 13 \t Validation precision@k5: 0.77, accuracy@k5: 0.42\n",
      "Epoch: 13 \t Validation precision@k10: 0.73, accuracy@k10: 0.63\n",
      "Epoch: 13 \t Validation precision@k15: 0.77, accuracy@k15: 0.75\n",
      "Epoch: 13 \t Validation precision@k20: 0.82, accuracy@k20: 0.82\n",
      "Epoch: 13 \t Validation precision@k25: 0.87, accuracy@k25: 0.87\n",
      "Epoch: 13 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 23.16\n",
      "RAM %: 61.7\n",
      "Epoch: 14 \t Training Loss: 37.075577\n",
      "Epoch: 14 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 14 \t Validation precision@k10: 0.74, accuracy@k10: 0.63\n",
      "Epoch: 14 \t Validation precision@k15: 0.77, accuracy@k15: 0.75\n",
      "Epoch: 14 \t Validation precision@k20: 0.83, accuracy@k20: 0.82\n",
      "Epoch: 14 \t Validation precision@k25: 0.87, accuracy@k25: 0.87\n",
      "Epoch: 14 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 23.01\n",
      "RAM %: 62.0\n",
      "Epoch: 15 \t Training Loss: 36.904189\n",
      "Epoch: 15 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 15 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 15 \t Validation precision@k15: 0.78, accuracy@k15: 0.76\n",
      "Epoch: 15 \t Validation precision@k20: 0.83, accuracy@k20: 0.83\n",
      "Epoch: 15 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 15 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 23.50\n",
      "RAM %: 62.1\n",
      "Epoch: 16 \t Training Loss: 36.746928\n",
      "Epoch: 16 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 16 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 16 \t Validation precision@k15: 0.78, accuracy@k15: 0.76\n",
      "Epoch: 16 \t Validation precision@k20: 0.84, accuracy@k20: 0.83\n",
      "Epoch: 16 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 16 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 23.66\n",
      "RAM %: 61.9\n",
      "Epoch: 17 \t Training Loss: 36.567104\n",
      "Epoch: 17 \t Validation precision@k5: 0.79, accuracy@k5: 0.43\n",
      "Epoch: 17 \t Validation precision@k10: 0.76, accuracy@k10: 0.65\n",
      "Epoch: 17 \t Validation precision@k15: 0.79, accuracy@k15: 0.77\n",
      "Epoch: 17 \t Validation precision@k20: 0.84, accuracy@k20: 0.84\n",
      "Epoch: 17 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 17 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 24.52\n",
      "RAM %: 61.9\n",
      "Epoch: 18 \t Training Loss: 36.417362\n",
      "Epoch: 18 \t Validation precision@k5: 0.79, accuracy@k5: 0.43\n",
      "Epoch: 18 \t Validation precision@k10: 0.76, accuracy@k10: 0.65\n",
      "Epoch: 18 \t Validation precision@k15: 0.80, accuracy@k15: 0.77\n",
      "Epoch: 18 \t Validation precision@k20: 0.85, accuracy@k20: 0.84\n",
      "Epoch: 18 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 18 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 25.43\n",
      "RAM %: 62.0\n",
      "Epoch: 19 \t Training Loss: 36.239702\n",
      "Epoch: 19 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 19 \t Validation precision@k10: 0.77, accuracy@k10: 0.66\n",
      "Epoch: 19 \t Validation precision@k15: 0.80, accuracy@k15: 0.78\n",
      "Epoch: 19 \t Validation precision@k20: 0.85, accuracy@k20: 0.85\n",
      "Epoch: 19 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 19 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 25.29\n",
      "RAM %: 61.9\n",
      "Epoch: 20 \t Training Loss: 36.095521\n",
      "Epoch: 20 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 20 \t Validation precision@k10: 0.77, accuracy@k10: 0.66\n",
      "Epoch: 20 \t Validation precision@k15: 0.81, accuracy@k15: 0.78\n",
      "Epoch: 20 \t Validation precision@k20: 0.86, accuracy@k20: 0.85\n",
      "Epoch: 20 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 20 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 25.72\n",
      "RAM %: 61.9\n",
      "Epoch: 21 \t Training Loss: 35.946842\n",
      "Epoch: 21 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 21 \t Validation precision@k10: 0.78, accuracy@k10: 0.66\n",
      "Epoch: 21 \t Validation precision@k15: 0.81, accuracy@k15: 0.79\n",
      "Epoch: 21 \t Validation precision@k20: 0.86, accuracy@k20: 0.86\n",
      "Epoch: 21 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 21 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 25.92\n",
      "RAM %: 62.0\n",
      "Epoch: 22 \t Training Loss: 35.809930\n",
      "Epoch: 22 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 22 \t Validation precision@k10: 0.78, accuracy@k10: 0.67\n",
      "Epoch: 22 \t Validation precision@k15: 0.82, accuracy@k15: 0.79\n",
      "Epoch: 22 \t Validation precision@k20: 0.87, accuracy@k20: 0.86\n",
      "Epoch: 22 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 22 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 26.16\n",
      "RAM %: 62.1\n",
      "Epoch: 23 \t Training Loss: 35.672350\n",
      "Epoch: 23 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 23 \t Validation precision@k10: 0.79, accuracy@k10: 0.67\n",
      "Epoch: 23 \t Validation precision@k15: 0.82, accuracy@k15: 0.79\n",
      "Epoch: 23 \t Validation precision@k20: 0.87, accuracy@k20: 0.87\n",
      "Epoch: 23 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 23 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 26.43\n",
      "RAM %: 62.1\n",
      "Epoch: 24 \t Training Loss: 35.520527\n",
      "Epoch: 24 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 24 \t Validation precision@k10: 0.79, accuracy@k10: 0.68\n",
      "Epoch: 24 \t Validation precision@k15: 0.83, accuracy@k15: 0.80\n",
      "Epoch: 24 \t Validation precision@k20: 0.87, accuracy@k20: 0.87\n",
      "Epoch: 24 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 24 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 26.61\n",
      "RAM %: 61.9\n",
      "Epoch: 25 \t Training Loss: 35.385110\n",
      "Epoch: 25 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 25 \t Validation precision@k10: 0.79, accuracy@k10: 0.68\n",
      "Epoch: 25 \t Validation precision@k15: 0.83, accuracy@k15: 0.80\n",
      "Epoch: 25 \t Validation precision@k20: 0.88, accuracy@k20: 0.87\n",
      "Epoch: 25 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 25 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 26.86\n",
      "RAM %: 61.9\n",
      "Epoch: 26 \t Training Loss: 35.246041\n",
      "Epoch: 26 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 26 \t Validation precision@k10: 0.80, accuracy@k10: 0.68\n",
      "Epoch: 26 \t Validation precision@k15: 0.83, accuracy@k15: 0.81\n",
      "Epoch: 26 \t Validation precision@k20: 0.88, accuracy@k20: 0.88\n",
      "Epoch: 26 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 26 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 27.75\n",
      "RAM %: 62.1\n",
      "Epoch: 27 \t Training Loss: 35.129080\n",
      "Epoch: 27 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 27 \t Validation precision@k10: 0.80, accuracy@k10: 0.69\n",
      "Epoch: 27 \t Validation precision@k15: 0.84, accuracy@k15: 0.81\n",
      "Epoch: 27 \t Validation precision@k20: 0.88, accuracy@k20: 0.88\n",
      "Epoch: 27 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 27 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 28.07\n",
      "RAM %: 62.1\n",
      "Epoch: 28 \t Training Loss: 34.990283\n",
      "Epoch: 28 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 28 \t Validation precision@k10: 0.81, accuracy@k10: 0.69\n",
      "Epoch: 28 \t Validation precision@k15: 0.84, accuracy@k15: 0.82\n",
      "Epoch: 28 \t Validation precision@k20: 0.89, accuracy@k20: 0.88\n",
      "Epoch: 28 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 28 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 28.13\n",
      "RAM %: 62.1\n",
      "Epoch: 29 \t Training Loss: 34.858977\n",
      "Epoch: 29 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 29 \t Validation precision@k10: 0.81, accuracy@k10: 0.69\n",
      "Epoch: 29 \t Validation precision@k15: 0.85, accuracy@k15: 0.82\n",
      "Epoch: 29 \t Validation precision@k20: 0.89, accuracy@k20: 0.89\n",
      "Epoch: 29 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 29 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 28.33\n",
      "RAM %: 62.1\n",
      "Epoch: 30 \t Training Loss: 34.725626\n",
      "Epoch: 30 \t Validation precision@k5: 0.83, accuracy@k5: 0.46\n",
      "Epoch: 30 \t Validation precision@k10: 0.81, accuracy@k10: 0.70\n",
      "Epoch: 30 \t Validation precision@k15: 0.85, accuracy@k15: 0.82\n",
      "Epoch: 30 \t Validation precision@k20: 0.90, accuracy@k20: 0.89\n",
      "Epoch: 30 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 30 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 28.89\n",
      "RAM %: 62.1\n",
      "Epoch: 31 \t Training Loss: 34.601672\n",
      "Epoch: 31 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 31 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 31 \t Validation precision@k15: 0.85, accuracy@k15: 0.83\n",
      "Epoch: 31 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 31 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 31 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 29.64\n",
      "RAM %: 62.2\n",
      "Epoch: 32 \t Training Loss: 34.498242\n",
      "Epoch: 32 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 32 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 32 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 32 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 32 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 32 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 29.77\n",
      "RAM %: 62.3\n",
      "Epoch: 33 \t Training Loss: 34.371312\n",
      "Epoch: 33 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 33 \t Validation precision@k10: 0.82, accuracy@k10: 0.71\n",
      "Epoch: 33 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 33 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 33 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 33 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 30.11\n",
      "RAM %: 62.3\n",
      "Epoch: 34 \t Training Loss: 34.260834\n",
      "Epoch: 34 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 34 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 34 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 34 \t Validation precision@k20: 0.91, accuracy@k20: 0.90\n",
      "Epoch: 34 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 34 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 31.48\n",
      "RAM %: 62.2\n",
      "Epoch: 35 \t Training Loss: 34.139523\n",
      "Epoch: 35 \t Validation precision@k5: 0.85, accuracy@k5: 0.47\n",
      "Epoch: 35 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 35 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 35 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 35 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 35 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 31.86\n",
      "RAM %: 62.3\n",
      "Epoch: 36 \t Training Loss: 34.020923\n",
      "Epoch: 36 \t Validation precision@k5: 0.85, accuracy@k5: 0.47\n",
      "Epoch: 36 \t Validation precision@k10: 0.83, accuracy@k10: 0.72\n",
      "Epoch: 36 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 36 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 36 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 36 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 32.03\n",
      "RAM %: 62.3\n",
      "Epoch: 37 \t Training Loss: 33.915110\n",
      "Epoch: 37 \t Validation precision@k5: 0.85, accuracy@k5: 0.47\n",
      "Epoch: 37 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 37 \t Validation precision@k15: 0.87, accuracy@k15: 0.85\n",
      "Epoch: 37 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 37 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 37 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 33.13\n",
      "RAM %: 62.3\n",
      "Epoch: 38 \t Training Loss: 33.818502\n",
      "Epoch: 38 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 38 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 38 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 38 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 38 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 33.13\n",
      "RAM %: 62.3\n",
      "Epoch: 39 \t Training Loss: 33.719159\n",
      "Epoch: 39 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 39 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 39 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 39 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 39 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 39 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 33.10\n",
      "RAM %: 62.3\n",
      "Epoch: 40 \t Training Loss: 33.604300\n",
      "Epoch: 40 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 40 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 40 \t Validation precision@k15: 0.88, accuracy@k15: 0.86\n",
      "Epoch: 40 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 40 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 40 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.84\n",
      "RAM %: 62.3\n",
      "Epoch: 41 \t Training Loss: 33.506895\n",
      "Epoch: 41 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 41 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 41 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 41 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 41 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 41 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.68\n",
      "RAM %: 62.3\n",
      "Epoch: 42 \t Training Loss: 33.418422\n",
      "Epoch: 42 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 42 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 42 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 42 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 42 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 42 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.52\n",
      "RAM %: 62.3\n",
      "Epoch: 43 \t Training Loss: 33.329441\n",
      "Epoch: 43 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 43 \t Validation precision@k10: 0.86, accuracy@k10: 0.73\n",
      "Epoch: 43 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 43 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 43 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 43 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.29\n",
      "RAM %: 62.2\n",
      "Epoch: 44 \t Training Loss: 33.231799\n",
      "Epoch: 44 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 44 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 44 \t Validation precision@k15: 0.89, accuracy@k15: 0.87\n",
      "Epoch: 44 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 44 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 44 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.12\n",
      "RAM %: 62.3\n",
      "Epoch: 45 \t Training Loss: 33.136678\n",
      "Epoch: 45 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 45 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 45 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 45 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 45 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 45 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 32.10\n",
      "RAM %: 62.2\n",
      "Epoch: 46 \t Training Loss: 33.051735\n",
      "Epoch: 46 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 46 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 46 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 46 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 46 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 46 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.36\n",
      "RAM %: 62.3\n",
      "Epoch: 47 \t Training Loss: 32.972616\n",
      "Epoch: 47 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 47 \t Validation precision@k10: 0.87, accuracy@k10: 0.74\n",
      "Epoch: 47 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 47 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 47 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 47 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.50\n",
      "RAM %: 62.3\n",
      "Epoch: 48 \t Training Loss: 32.894613\n",
      "Epoch: 48 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 48 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 48 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 48 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 48 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 48 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.53\n",
      "RAM %: 62.3\n",
      "Epoch: 49 \t Training Loss: 32.814613\n",
      "Epoch: 49 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 49 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 49 \t Validation precision@k15: 0.90, accuracy@k15: 0.88\n",
      "Epoch: 49 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 49 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 49 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 33.11\n",
      "RAM %: 62.3\n",
      "Epoch: 50 \t Training Loss: 32.738321\n",
      "Epoch: 50 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 50 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 50 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 50 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 50 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 50 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.71\n",
      "RAM %: 62.3\n",
      "Epoch: 51 \t Training Loss: 32.668458\n",
      "Epoch: 51 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 51 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 51 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 51 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 51 \t Validation precision@k25: 0.97, accuracy@k25: 0.96\n",
      "Epoch: 51 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.27\n",
      "RAM %: 62.3\n",
      "Epoch: 52 \t Training Loss: 32.594855\n",
      "Epoch: 52 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 52 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 52 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 52 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 52 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 52 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.07\n",
      "RAM %: 62.3\n",
      "Epoch: 53 \t Training Loss: 32.544303\n",
      "Epoch: 53 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 53 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 53 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 53 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 53 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 53 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 32.12\n",
      "RAM %: 63.2\n",
      "Epoch: 54 \t Training Loss: 32.461618\n",
      "Epoch: 54 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 54 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 54 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 54 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 54 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 54 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.98\n",
      "RAM %: 58.7\n",
      "Epoch: 55 \t Training Loss: 32.393790\n",
      "Epoch: 55 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 55 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 55 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 55 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 55 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 55 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.57\n",
      "RAM %: 59.1\n",
      "Epoch: 56 \t Training Loss: 32.336933\n",
      "Epoch: 56 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 56 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 56 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 56 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 56 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 56 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.01\n",
      "RAM %: 59.4\n",
      "Epoch: 57 \t Training Loss: 32.283724\n",
      "Epoch: 57 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 57 \t Validation precision@k10: 0.89, accuracy@k10: 0.76\n",
      "Epoch: 57 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 57 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 57 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 42.14\n",
      "RAM %: 38.4\n",
      "Epoch: 58 \t Training Loss: 32.218733\n",
      "Epoch: 58 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 58 \t Validation precision@k10: 0.89, accuracy@k10: 0.76\n",
      "Epoch: 58 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 58 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 58 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 58 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 42.58\n",
      "RAM %: 41.6\n",
      "Epoch: 59 \t Training Loss: 32.167483\n",
      "Epoch: 59 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 59 \t Validation precision@k10: 0.89, accuracy@k10: 0.76\n",
      "Epoch: 59 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 59 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 59 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 59 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 42.59\n",
      "RAM %: 41.6\n",
      "Epoch: 60 \t Training Loss: 32.115014\n",
      "Epoch: 60 \t Validation precision@k5: 0.90, accuracy@k5: 0.49\n",
      "Epoch: 60 \t Validation precision@k10: 0.89, accuracy@k10: 0.77\n",
      "Epoch: 60 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 60 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 60 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 60 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 42.68\n",
      "RAM %: 41.9\n",
      "Epoch: 61 \t Training Loss: 32.061720\n",
      "Epoch: 61 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 61 \t Validation precision@k10: 0.89, accuracy@k10: 0.77\n",
      "Epoch: 61 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 61 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 61 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 61 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 42.21\n",
      "RAM %: 42.1\n",
      "Epoch: 62 \t Training Loss: 32.011786\n",
      "Epoch: 62 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 62 \t Validation precision@k10: 0.89, accuracy@k10: 0.77\n",
      "Epoch: 62 \t Validation precision@k15: 0.92, accuracy@k15: 0.90\n",
      "Epoch: 62 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 62 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 62 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 53.51\n",
      "RAM %: 42.5\n",
      "Epoch: 63 \t Training Loss: 31.970159\n",
      "Epoch: 63 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 63 \t Validation precision@k10: 0.89, accuracy@k10: 0.77\n",
      "Epoch: 63 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 63 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 63 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 63 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 52.47\n",
      "RAM %: 42.7\n",
      "Epoch: 64 \t Training Loss: 31.912211\n",
      "Epoch: 64 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 64 \t Validation precision@k10: 0.90, accuracy@k10: 0.77\n",
      "Epoch: 64 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 64 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 64 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 64 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 51.70\n",
      "RAM %: 42.8\n",
      "Epoch: 65 \t Training Loss: 31.869939\n",
      "Epoch: 65 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 65 \t Validation precision@k10: 0.90, accuracy@k10: 0.77\n",
      "Epoch: 65 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 65 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 65 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 65 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 50.39\n",
      "RAM %: 42.7\n",
      "Epoch: 66 \t Training Loss: 31.829152\n",
      "Epoch: 66 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 66 \t Validation precision@k10: 0.90, accuracy@k10: 0.77\n",
      "Epoch: 66 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 66 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 66 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 66 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 49.68\n",
      "RAM %: 43.0\n",
      "Epoch: 67 \t Training Loss: 31.785811\n",
      "Epoch: 67 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 67 \t Validation precision@k10: 0.90, accuracy@k10: 0.77\n",
      "Epoch: 67 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 67 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 67 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 67 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 48.75\n",
      "RAM %: 43.2\n",
      "Epoch: 68 \t Training Loss: 31.743733\n",
      "Epoch: 68 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 68 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 68 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 68 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 68 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 68 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 47.86\n",
      "RAM %: 43.3\n",
      "Epoch: 69 \t Training Loss: 31.708041\n",
      "Epoch: 69 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 69 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 69 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 69 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 69 \t Validation precision@k25: 0.98, accuracy@k25: 0.97\n",
      "Epoch: 69 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 47.20\n",
      "RAM %: 43.4\n",
      "Epoch: 70 \t Training Loss: 31.652678\n",
      "Epoch: 70 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 70 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 70 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 70 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 70 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 70 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 46.96\n",
      "RAM %: 43.4\n",
      "Epoch: 71 \t Training Loss: 31.628950\n",
      "Epoch: 71 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 71 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 71 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 71 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 71 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 71 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 46.43\n",
      "RAM %: 43.6\n",
      "Epoch: 72 \t Training Loss: 31.602421\n",
      "Epoch: 72 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 72 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 72 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 72 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 72 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 72 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 45.85\n",
      "RAM %: 43.5\n",
      "Epoch: 73 \t Training Loss: 31.556148\n",
      "Epoch: 73 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 73 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 73 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 73 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 73 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 73 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 45.79\n",
      "RAM %: 43.6\n",
      "Epoch: 74 \t Training Loss: 31.521559\n",
      "Epoch: 74 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 74 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 74 \t Validation precision@k15: 0.93, accuracy@k15: 0.91\n",
      "Epoch: 74 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 74 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 74 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 45.13\n",
      "RAM %: 43.7\n",
      "Epoch: 75 \t Training Loss: 31.494572\n",
      "Epoch: 75 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 75 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 75 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 75 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 75 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 75 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 44.41\n",
      "RAM %: 43.8\n",
      "Epoch: 76 \t Training Loss: 31.459426\n",
      "Epoch: 76 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 76 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 76 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 76 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 76 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 43.41\n",
      "RAM %: 43.9\n",
      "Epoch: 77 \t Training Loss: 31.432040\n",
      "Epoch: 77 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 77 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 77 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 77 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 77 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 77 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 43.16\n",
      "RAM %: 44.0\n",
      "Epoch: 78 \t Training Loss: 31.400644\n",
      "Epoch: 78 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 78 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 78 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 78 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 78 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 78 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 43.00\n",
      "RAM %: 44.2\n",
      "Epoch: 79 \t Training Loss: 31.371394\n",
      "Epoch: 79 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 79 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 79 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 79 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 79 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 79 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 42.59\n",
      "RAM %: 44.1\n",
      "Epoch: 80 \t Training Loss: 31.344957\n",
      "Epoch: 80 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 80 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 80 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 80 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 80 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 80 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 42.49\n",
      "RAM %: 44.3\n",
      "Epoch: 81 \t Training Loss: 31.314461\n",
      "Epoch: 81 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 81 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 81 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 81 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 81 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 81 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 41.65\n",
      "RAM %: 44.3\n",
      "Epoch: 82 \t Training Loss: 31.283850\n",
      "Epoch: 82 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 82 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 82 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 82 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 82 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 82 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 41.59\n",
      "RAM %: 44.3\n",
      "Epoch: 83 \t Training Loss: 31.273598\n",
      "Epoch: 83 \t Validation precision@k5: 0.91, accuracy@k5: 0.51\n",
      "Epoch: 83 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 83 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 83 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 83 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 83 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.75\n",
      "RAM %: 44.4\n",
      "Epoch: 84 \t Training Loss: 31.255442\n",
      "Epoch: 84 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 84 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 84 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 84 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 84 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 84 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.70\n",
      "RAM %: 44.4\n",
      "Epoch: 85 \t Training Loss: 31.218651\n",
      "Epoch: 85 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 85 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 85 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 85 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 85 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 85 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.81\n",
      "RAM %: 44.5\n",
      "Epoch: 86 \t Training Loss: 31.203678\n",
      "Epoch: 86 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 86 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 86 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 86 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 86 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 86 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 41.05\n",
      "RAM %: 45.8\n",
      "Epoch: 87 \t Training Loss: 31.171196\n",
      "Epoch: 87 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 87 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 87 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 87 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 87 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 87 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.74\n",
      "RAM %: 45.9\n",
      "Epoch: 88 \t Training Loss: 31.148942\n",
      "Epoch: 88 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 88 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 88 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 88 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 88 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 88 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.44\n",
      "RAM %: 45.9\n",
      "Epoch: 89 \t Training Loss: 31.127109\n",
      "Epoch: 89 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 89 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 89 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 89 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 89 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 89 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.86\n",
      "RAM %: 46.0\n",
      "Epoch: 90 \t Training Loss: 31.118030\n",
      "Epoch: 90 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 90 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 90 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 90 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 90 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 90 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 40.62\n",
      "RAM %: 45.7\n",
      "Epoch: 91 \t Training Loss: 31.097658\n",
      "Epoch: 91 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 91 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 91 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 91 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 91 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 91 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.94\n",
      "RAM %: 45.7\n",
      "Epoch: 92 \t Training Loss: 31.073953\n",
      "Epoch: 92 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 92 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 92 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 92 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 92 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 92 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.50\n",
      "RAM %: 45.8\n",
      "Epoch: 93 \t Training Loss: 31.056016\n",
      "Epoch: 93 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 93 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 93 \t Validation precision@k15: 0.94, accuracy@k15: 0.92\n",
      "Epoch: 93 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 93 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 93 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.50\n",
      "RAM %: 45.8\n",
      "Epoch: 94 \t Training Loss: 31.037060\n",
      "Epoch: 94 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 94 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 94 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 94 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 94 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 94 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.50\n",
      "RAM %: 45.9\n",
      "Epoch: 95 \t Training Loss: 31.022424\n",
      "Epoch: 95 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 95 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 95 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 95 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 95 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 38.92\n",
      "RAM %: 45.9\n",
      "Epoch: 96 \t Training Loss: 31.001593\n",
      "Epoch: 96 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 96 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 96 \t Validation precision@k15: 0.94, accuracy@k15: 0.92\n",
      "Epoch: 96 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 96 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 96 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 38.86\n",
      "RAM %: 45.9\n",
      "Epoch: 97 \t Training Loss: 30.993626\n",
      "Epoch: 97 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 97 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 97 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 97 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 97 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 97 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.05\n",
      "RAM %: 45.9\n",
      "Epoch: 98 \t Training Loss: 30.966360\n",
      "Epoch: 98 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 98 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 98 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 98 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 98 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 98 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.16\n",
      "RAM %: 46.0\n",
      "Epoch: 99 \t Training Loss: 30.947201\n",
      "Epoch: 99 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 99 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 99 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 99 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 99 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 99 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 39.46\n",
      "RAM %: 46.3\n",
      "Epoch: 100 \t Training Loss: 30.934437\n",
      "Epoch: 100 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 100 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 100 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 100 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 100 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 100 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU times: user 1h 53min 5s, sys: 17min 45s, total: 2h 10min 51s\n",
      "Wall time: 1h 27min 36s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_dipole, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.6190, accuracy@k5: 0.3413\n",
      "Validation precision@k10: 0.6149, accuracy@k10: 0.5300\n",
      "Validation precision@k15: 0.6679, accuracy@k15: 0.6458\n",
      "Validation precision@k20: 0.7288, accuracy@k20: 0.7249\n",
      "Validation precision@k25: 0.7774, accuracy@k25: 0.7770\n",
      "Validation precision@k30: 0.8222, accuracy@k30: 0.8222\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_dipole, val_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acba7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2232eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b7486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23348197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
