{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a324384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c55c9d",
   "metadata": {},
   "source": [
    "### Set local paths for input data and output models\n",
    "NOTE: Download the `fastText` pretrained model `crawl-300d-2M-subword.zip` [here](https://fasttext.cc/docs/en/english-vectors.html) and place it in `DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f738ec",
   "metadata": {},
   "source": [
    "### Load input data and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'text_seqs.pkl'), 'rb'))\n",
    "num_seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "text = pickle.load(open(os.path.join(DATA_PATH,'icd9_text.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "pretrained_word_model = os.path.join(DATA_PATH, 'crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a199763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.load_model(pretrained_word_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43e1c9",
   "metadata": {},
   "source": [
    "### Define and load custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3069c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "\n",
    "        self.x = text\n",
    "        self.y = [i for i in range(len(text))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276ed97",
   "metadata": {},
   "source": [
    "### Define `build_vocab` function and build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, ft_model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        text: full corpus of text with all ICD9 code descriptions\n",
    "        ft_model: the pretrained fastText model\n",
    "        \n",
    "    Outputs:\n",
    "        vocab: a tensor of shape (# words in corpus, word embedding dim) of type torch.float\n",
    "        words: a sorted list of all words in corpus\n",
    "        lookup: a dict which returns the index value for each word\n",
    "    \"\"\"\n",
    "    word_embed_dim = ft_model.get_dimension()\n",
    "    num_codes = len(text)\n",
    "    num_words = [len(words.split()) for words in text]\n",
    "    max_num_words = max(num_words)\n",
    "    \n",
    "    words = set()\n",
    "    for w in text:\n",
    "        words.update(w.split())\n",
    "    words = sorted(list(words))\n",
    "    lookup = dict(zip(words, range(len(words))))\n",
    "    num_words = len(words)\n",
    "    \n",
    "    vocab = torch.zeros((num_words, word_embed_dim), dtype=torch.float)\n",
    "    for i, word in enumerate(words):\n",
    "        word_embed = ft_model[word]\n",
    "        vocab[i] = torch.tensor(word_embed, dtype=torch.float)\n",
    "    \n",
    "    return vocab, words, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_built, words, lookup = build_vocab(text, ft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2f1d9a",
   "metadata": {},
   "source": [
    "### Define collate and data loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.float\n",
    "        y: a tensor of shape (# icd9 codes) of type torch.long\n",
    "        masks: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.bool\n",
    "    \"\"\"\n",
    "    text, indices = zip(*data)\n",
    "    word_embed_dim = 300\n",
    "\n",
    "    y = torch.tensor(indices, dtype=torch.long)\n",
    "    num_codes = len(text)\n",
    "    num_words = [len(words.split()) for words in text]\n",
    "\n",
    "    max_num_words = max(max(num_words), 4)\n",
    "\n",
    "    global lookup\n",
    "    x = torch.zeros((num_codes, max_num_words), dtype=torch.long)\n",
    "    masks = torch.zeros((num_codes, max_num_words), dtype=torch.bool)\n",
    "    for i, code in enumerate(text):\n",
    "        for j, word in enumerate(code.split()):\n",
    "            x[i,j] = lookup[word]\n",
    "            masks[i,j] = 1\n",
    "\n",
    "    return x, y, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef27be",
   "metadata": {},
   "source": [
    "NOTE: `train_loader` is batches of size 100 while `test_loader` loads the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, collate_fn):\n",
    "    '''\n",
    "    The model is trained on the full dataset shuffled in batches of 100, the test\n",
    "    dataset is the full dataset delievered in one large batch not shuffled.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, test_loader: train and test dataloaders\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=4903,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = load_data(dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2984a88",
   "metadata": {},
   "source": [
    "### Define masking function to set masked indices to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_conv2d(outputs, masks):\n",
    "    masks = masks.unsqueeze(1)\n",
    "    masks = masks.repeat(1,100,1)\n",
    "    x = []\n",
    "    for mat in outputs:\n",
    "        outmat = mat.clone()\n",
    "        dim = outmat.shape[2]\n",
    "        outmat[~masks[:,:,:dim]] = 0\n",
    "        x.append(outmat)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7109fe",
   "metadata": {},
   "source": [
    "### Define helper function to convert indices to multihot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf606e2f",
   "metadata": {},
   "source": [
    "### Define EmbeddingCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcde57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_descriptions, max_num_words, vocab_built,\n",
    "                 embedding_dim, num_class, num_kernel, kernel_sizes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "            num_descriptions: number of icd9 descrptions\n",
    "            embedding_dim: size of word embedding dim (from fastText)\n",
    "            num_class: number of classes to predict\n",
    "            num_kernel: number of filters for each kernel size\n",
    "            kernel_sizes: list of sizes to iterate on\n",
    "        \"\"\"\n",
    "        self.embed = nn.Embedding(len(vocab_built), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, num_kernel, (K, embedding_dim)) for K in kernel_sizes]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_kernel, num_descriptions)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Both the logit for training and the embedding matrix are output, so the embedding matrix\n",
    "        can be obtained once training is complete.\n",
    "        \n",
    "        Arguments:\n",
    "            x: the input tensor of icd9 description of size (batch_size, max_num_words, word_embedding_dim) \n",
    "            masks: masks for the padded words of size (batch_size, max_num_words, word_embedding_dim)\n",
    "        \n",
    "        Outputs:\n",
    "            logit: logits for cross entropy loss function to for training iterations\n",
    "            embedding: embedding matrix of learned wieghts for icd9 descriptions\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = mask_conv2d(x, masks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        embedding = torch.cat(x, 1)\n",
    "        x = self.dropout(embedding)\n",
    "        logit = self.fc(x)\n",
    "        return logit, embedding\n",
    "\n",
    "embedding_cnn = EmbeddingCNN(\n",
    "    num_descriptions=len(codes), max_num_words=30, vocab_built=vocab_built,\n",
    "    embedding_dim=300, num_class=len(codes), num_kernel=100, kernel_sizes=[2,3,4])\n",
    "embedding_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eda591",
   "metadata": {},
   "source": [
    "### Define loss and optimization, and train and evalutaion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(embedding_cnn.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.Adadelta(baseline_retain.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs, n_class):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "        model: the CNN model\n",
    "        train_loader: training dataloder\n",
    "        n_epochs: total number of epochs\n",
    "        n_class: num of classes to learn\n",
    "    \"\"\"\n",
    "    max_cpu, max_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for feature, target, masks in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit, embedding = model(feature, masks)\n",
    "            loss = criterion(logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        cpu, ram = print_cpu_usage()\n",
    "        max_cpu = cpu if cpu > max_cpu else max_cpu\n",
    "        max_ram = ram if ram > max_ram else max_ram\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()\n",
    "    print(f\"Max CPU usage: {max_cpu:.3f}\\tMax RAM % usage: {max_ram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477179c0",
   "metadata": {},
   "source": [
    "NOTE: Evaluation function retruns the embedding matrix $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader): \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the EmbeddingCNN model\n",
    "        test_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        embedding: learned embedding matrix of the evaluated model\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        for feature, target, masks in test_loader:\n",
    "            logit, embedding = model(feature, masks)\n",
    "            y_hat = F.softmax(logit, dim=-1)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            n_correct += (pred == target).sum()\n",
    "            n_total += pred.shape[0]\n",
    "        success = n_correct / n_total\n",
    "        print(f'{n_correct}/{n_total} correct \\t success rate: {success:.4f}')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10751e5",
   "metadata": {},
   "source": [
    "### Set num epochs and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972345ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "%time train(embedding_cnn, train_loader, n_epochs, len(codes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34819126",
   "metadata": {},
   "source": [
    "### Evaluate model and extract embedding matrix, then transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803336bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = eval_model(embedding_cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = embedding.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5de8e4",
   "metadata": {},
   "source": [
    "### Save embedding matrix $E$ and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a694216",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(E, os.path.join(DATA_PATH, 'embedding_matrix.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding_cnn, os.path.join(CHECKPOINT_PATH, \"EmbeddingCNN_250.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
