{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a324384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31efc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC_DATA_PATH = \"/Users/ericahlgren/Documents/UIUC/CS598/Project/data/mimic-iii-clinical-database-1.4\"\n",
    "ICU_CSV = \"icu_diag_merge.csv\"\n",
    "OUTPUT_TEXT = \"data/icd_long_title.txt\"\n",
    "DATA_PATH = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aac7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df = pd.read_csv(os.path.join(MIMIC_DATA_PATH, ICU_CSV))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'text_seqs.pkl'), 'rb'))\n",
    "num_seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "text = pickle.load(open(os.path.join(DATA_PATH,'icd9_text.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b35329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ICD9_CODE</th>\n",
       "      <th>LONG_TITLE_REPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V3001</td>\n",
       "      <td>single liveborn born in hospital delivered by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V053</td>\n",
       "      <td>need for prophylactic vaccination and inoculat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V290</td>\n",
       "      <td>observation for suspected infectious condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V3001</td>\n",
       "      <td>single liveborn born in hospital delivered by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V053</td>\n",
       "      <td>need for prophylactic vaccination and inoculat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ICD9_CODE                                    LONG_TITLE_REPL\n",
       "0     V3001  single liveborn born in hospital delivered by ...\n",
       "1      V053  need for prophylactic vaccination and inoculat...\n",
       "2      V290     observation for suspected infectious condition\n",
       "3     V3001  single liveborn born in hospital delivered by ...\n",
       "4      V053  need for prophylactic vaccination and inoculat..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_df[['ICD9_CODE','LONG_TITLE_REPL']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c620ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4903"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icu_df.ICD9_CODE.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a61bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_df.to_csv(OUTPUT_TEXT, columns=[\"LONG_TITLE_REPL\"], header=False, index=False, sep='\\n')\n",
    "#icu_df.to_csv(OUTPUT_TEXT, columns=[\"ICD_SUBCATEGORY_DESC_REPL\"], header=False, index=False, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "addb0549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  3165\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  146126 lr:  0.000000 avg.loss:  0.779818 ETA:   0h 0m 0s  6.7% words/sec/thread:  151550 lr:  0.046634 avg.loss:  2.081994 ETA:   0h 0m 4s\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.train_unsupervised(OUTPUT_TEXT, model='skipgram', dim=300, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3069c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "\n",
    "        self.x = text\n",
    "        self.y = [i for i in range(len(text))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d1d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d7c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.float\n",
    "        y: a tensor of shape (# icd9 codes) of type torch.long\n",
    "        masks: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.bool\n",
    "    \"\"\"\n",
    "    text, indices = zip(*data)\n",
    "    word_embed_dim = 300\n",
    "\n",
    "    y = torch.tensor(indices, dtype=torch.long)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_codes = len(text)\n",
    "    num_words = [len(words.split()) for words in text]\n",
    "\n",
    "    max_num_words = max(max(num_words), 4)\n",
    "\n",
    "    global ft_model\n",
    "#     x = torch.zeros((num_codes, word_embed_dim * max_num_words), dtype=torch.float)\n",
    "#     x_masks = torch.zeros((num_codes, word_embed_dim * max_num_words), dtype=torch.bool)\n",
    "#     for i, code in enumerate(text):\n",
    "#         for j, word in enumerate(code.split()):\n",
    "#             word_embed = ft_model[word]\n",
    "#             x[i, j*300:j*300+300] = torch.tensor(word_embed, dtype=torch.float)\n",
    "#             x_masks[i, j*300:j*300+300] = 1\n",
    "    x = torch.zeros((num_codes, max_num_words, word_embed_dim), dtype=torch.float)\n",
    "    masks = torch.zeros((num_codes, max_num_words, word_embed_dim), dtype=torch.bool)\n",
    "    for i, code in enumerate(text):\n",
    "        for j, word in enumerate(code.split()):\n",
    "            word_embed = ft_model[word]\n",
    "            x[i,j] = torch.tensor(word_embed, dtype=torch.float)\n",
    "            masks[i,j] = torch.ones(word_embed_dim)\n",
    "\n",
    "    \n",
    "    return x, y, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8dc4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, test_loader: train and test dataloaders\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=4903,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = load_data(dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_conv2d(outputs, masks):\n",
    "    masks = masks.any(dim=2)\n",
    "    masks = masks.unsqueeze(1)\n",
    "    masks = masks.repeat(1,100,1)\n",
    "    x = []\n",
    "    for mat in outputs:\n",
    "        outmat = mat.clone()\n",
    "        dim = outmat.shape[2]\n",
    "        outmat[~masks[:,:,:dim]] = 0\n",
    "        x.append(outmat)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfcde57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingCNN(\n",
       "  (embed): Embedding(4903, 300)\n",
       "  (conv1): Conv2d(1, 100, kernel_size=(2, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=4903, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_descriptions, embedding_dim, num_class, num_kernel, kernel_sizes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "            num_descriptions: number of icd9 descrptions\n",
    "            embedding_dim: size of word embedding dim (from fastText)\n",
    "            num_class: number of classes to predict\n",
    "            num_kernel: number of filters for each kernel size\n",
    "            kernel_sizes: list of sizes to iterate on\n",
    "        \"\"\"\n",
    "        self.embed = nn.Embedding(num_descriptions, embedding_dim)\n",
    "#         self.conv1 = nn.Conv2d(1, num_kernel, (2, embedding_dim))\n",
    "#         self.conv2 = nn.Conv2d(1, num_kernel, (3, embedding_dim))\n",
    "#         self.conv3 = nn.Conv2d(1, num_kernel, (4, embedding_dim))\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, num_kernel, (K, embedding_dim)) for K in kernel_sizes]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_kernel, num_descriptions)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the input tensor of icd9 description of size (batch_size, max_num_words, word_embedding_dim) \n",
    "            masks: masks for the padded words of size (batch_size, max_num_words, word_embedding_dim)\n",
    "        \n",
    "        Outputs:\n",
    "            logit: logits for cross entropy loss function to for training iterations\n",
    "            embedding: embedding matrix of learned wieghts for icd9 descriptions\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = mask_conv2d(x, masks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        embedding = torch.cat(x, 1)\n",
    "        x = self.dropout(embedding)\n",
    "        logit = self.fc(x)\n",
    "        return logit, embedding\n",
    "\n",
    "embedding_cnn = EmbeddingCNN(\n",
    "    num_descriptions=len(codes), embedding_dim=300, num_class=len(codes),\n",
    "    num_kernel=100, kernel_sizes=[2,3,4])\n",
    "embedding_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32fa97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(embedding_cnn.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.Adadelta(baseline_retain.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e991d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs, n_class):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "        model: the CNN model\n",
    "        train_loader: training dataloder\n",
    "        n_epochs: total number of epochs\n",
    "        n_class: num of classes to learn\n",
    "    \"\"\"\n",
    "    #base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for feature, target, masks in train_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            optimizer.zero_grad()\n",
    "            logit, embedding = model(feature, masks)\n",
    "\n",
    "#             y_mh = indices_to_multihot(target, masks, logit)\n",
    "#             y_hat = F.one_hot(target, n_class)\n",
    "            loss = criterion(logit, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "#         eval_model(model, train_loader)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f1f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the CNN model\n",
    "        test_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        embedding: learned embedding matrix of the evaluated model\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "#     import pdb; pdb.set_trace()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        for feature, target, masks in test_loader:\n",
    "            logit, embedding = model(feature, masks)\n",
    "            y_hat = F.softmax(logit, dim=-1)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            n_correct += (pred == target).sum()\n",
    "            n_total += pred.shape[0]\n",
    "        success = n_correct / n_total\n",
    "        print(f'{n_correct}/{n_total} correct \\t success rate: {success:.4f}')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1256cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "972345ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 8.552966\n",
      "Epoch: 2 \t Training Loss: 8.478482\n",
      "Epoch: 3 \t Training Loss: 8.302736\n",
      "Epoch: 4 \t Training Loss: 7.672985\n",
      "Epoch: 5 \t Training Loss: 6.665219\n",
      "Epoch: 6 \t Training Loss: 5.490110\n",
      "Epoch: 7 \t Training Loss: 4.447985\n",
      "Epoch: 8 \t Training Loss: 3.671042\n",
      "Epoch: 9 \t Training Loss: 3.120104\n",
      "Epoch: 10 \t Training Loss: 2.695646\n",
      "Epoch: 11 \t Training Loss: 2.389871\n",
      "Epoch: 12 \t Training Loss: 2.127404\n",
      "Epoch: 13 \t Training Loss: 1.939855\n",
      "Epoch: 14 \t Training Loss: 1.726716\n",
      "Epoch: 15 \t Training Loss: 1.630548\n",
      "Epoch: 16 \t Training Loss: 1.501281\n",
      "Epoch: 17 \t Training Loss: 1.455339\n",
      "Epoch: 18 \t Training Loss: 1.309868\n",
      "Epoch: 19 \t Training Loss: 1.223125\n",
      "Epoch: 20 \t Training Loss: 1.203669\n",
      "Epoch: 21 \t Training Loss: 1.107222\n",
      "Epoch: 22 \t Training Loss: 1.057209\n",
      "Epoch: 23 \t Training Loss: 1.020391\n",
      "Epoch: 24 \t Training Loss: 0.969140\n",
      "Epoch: 25 \t Training Loss: 0.965282\n",
      "Epoch: 26 \t Training Loss: 0.889821\n",
      "Epoch: 27 \t Training Loss: 0.845714\n",
      "Epoch: 28 \t Training Loss: 0.769169\n",
      "Epoch: 29 \t Training Loss: 0.761219\n",
      "Epoch: 30 \t Training Loss: 0.762230\n",
      "Epoch: 31 \t Training Loss: 0.734725\n",
      "Epoch: 32 \t Training Loss: 0.689291\n",
      "Epoch: 33 \t Training Loss: 0.660733\n",
      "Epoch: 34 \t Training Loss: 0.633515\n",
      "Epoch: 35 \t Training Loss: 0.627408\n",
      "Epoch: 36 \t Training Loss: 0.608509\n",
      "Epoch: 37 \t Training Loss: 0.581044\n",
      "Epoch: 38 \t Training Loss: 0.590949\n",
      "Epoch: 39 \t Training Loss: 0.545952\n",
      "Epoch: 40 \t Training Loss: 0.544238\n",
      "Epoch: 41 \t Training Loss: 0.529865\n",
      "Epoch: 42 \t Training Loss: 0.474997\n",
      "Epoch: 43 \t Training Loss: 0.498264\n",
      "Epoch: 44 \t Training Loss: 0.454346\n",
      "Epoch: 45 \t Training Loss: 0.437745\n",
      "Epoch: 46 \t Training Loss: 0.441756\n",
      "Epoch: 47 \t Training Loss: 0.432029\n",
      "Epoch: 48 \t Training Loss: 0.427232\n",
      "Epoch: 49 \t Training Loss: 0.408994\n",
      "Epoch: 50 \t Training Loss: 0.457000\n",
      "Epoch: 51 \t Training Loss: 0.442200\n",
      "Epoch: 52 \t Training Loss: 0.433162\n",
      "Epoch: 53 \t Training Loss: 0.395802\n",
      "Epoch: 54 \t Training Loss: 0.373431\n",
      "Epoch: 55 \t Training Loss: 0.364851\n",
      "Epoch: 56 \t Training Loss: 0.357550\n",
      "Epoch: 57 \t Training Loss: 0.353809\n",
      "Epoch: 58 \t Training Loss: 0.347983\n",
      "Epoch: 59 \t Training Loss: 0.354301\n",
      "Epoch: 60 \t Training Loss: 0.362435\n",
      "Epoch: 61 \t Training Loss: 0.321704\n",
      "Epoch: 62 \t Training Loss: 0.297476\n",
      "Epoch: 63 \t Training Loss: 0.285858\n",
      "Epoch: 64 \t Training Loss: 0.296418\n",
      "Epoch: 65 \t Training Loss: 0.300972\n",
      "Epoch: 66 \t Training Loss: 0.275899\n",
      "Epoch: 67 \t Training Loss: 0.294446\n",
      "Epoch: 68 \t Training Loss: 0.305349\n",
      "Epoch: 69 \t Training Loss: 0.319499\n",
      "Epoch: 70 \t Training Loss: 0.295012\n",
      "Epoch: 71 \t Training Loss: 0.290505\n",
      "Epoch: 72 \t Training Loss: 0.268860\n",
      "Epoch: 73 \t Training Loss: 0.257898\n",
      "Epoch: 74 \t Training Loss: 0.247910\n",
      "Epoch: 75 \t Training Loss: 0.252379\n",
      "Epoch: 76 \t Training Loss: 0.221750\n",
      "Epoch: 77 \t Training Loss: 0.247019\n",
      "Epoch: 78 \t Training Loss: 0.252457\n",
      "Epoch: 79 \t Training Loss: 0.247312\n",
      "Epoch: 80 \t Training Loss: 0.222594\n",
      "Epoch: 81 \t Training Loss: 0.233727\n",
      "Epoch: 82 \t Training Loss: 0.233744\n",
      "Epoch: 83 \t Training Loss: 0.214039\n",
      "Epoch: 84 \t Training Loss: 0.255077\n",
      "Epoch: 85 \t Training Loss: 0.242909\n",
      "Epoch: 86 \t Training Loss: 0.220026\n",
      "Epoch: 87 \t Training Loss: 0.240419\n",
      "Epoch: 88 \t Training Loss: 0.203728\n",
      "Epoch: 89 \t Training Loss: 0.209731\n",
      "Epoch: 90 \t Training Loss: 0.196750\n",
      "Epoch: 91 \t Training Loss: 0.185929\n",
      "Epoch: 92 \t Training Loss: 0.195187\n",
      "Epoch: 93 \t Training Loss: 0.205411\n",
      "Epoch: 94 \t Training Loss: 0.197404\n",
      "Epoch: 95 \t Training Loss: 0.198627\n",
      "Epoch: 96 \t Training Loss: 0.177636\n",
      "Epoch: 97 \t Training Loss: 0.233443\n",
      "Epoch: 98 \t Training Loss: 0.205062\n",
      "Epoch: 99 \t Training Loss: 0.192633\n",
      "Epoch: 100 \t Training Loss: 0.184397\n",
      "CPU times: user 13min 6s, sys: 1min 48s, total: 14min 55s\n",
      "Wall time: 11min 2s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(embedding_cnn, train_loader, n_epochs, len(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "803336bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4835/4903 correct \t success rate: 0.9861\n"
     ]
    }
   ],
   "source": [
    "embedding = eval_model(embedding_cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00da3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87286abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1408, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0120],\n",
       "        [0.8244, 0.0000, 4.8920,  ..., 0.5002, 0.0000, 4.2985],\n",
       "        [1.4374, 0.2580, 0.1317,  ..., 2.3848, 2.0505, 0.0000],\n",
       "        ...,\n",
       "        [0.8518, 0.0000, 0.3237,  ..., 0.9934, 0.5658, 0.1184],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1696, 2.2176, 0.0000,  ..., 2.6231, 0.6750, 1.3424]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38338927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 4903])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a694216",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(E, os.path.join(DATA_PATH, 'embedding_matrix.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d7abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
