{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a324384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31efc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC_DATA_PATH = \"/Users/ericahlgren/Documents/UIUC/CS598/Project/data/mimic-iii-clinical-database-1.4\"\n",
    "# ICU_CSV = \"icu_diag_merge.csv\"\n",
    "# OUTPUT_TEXT = \"data/icd_long_title.txt\"\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aac7a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icu_df = pd.read_csv(os.path.join(MIMIC_DATA_PATH, ICU_CSV))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'text_seqs.pkl'), 'rb'))\n",
    "num_seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "text = pickle.load(open(os.path.join(DATA_PATH,'icd9_text.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "#pretrained_word_vecs = os.path.join(DATA_PATH, 'wiki-news-300d-1M-subword.vec')\n",
    "# pretrained_word_vecs = os.path.join(DATA_PATH, 'crawl-300d-2M-subword.vec')\n",
    "pretrained_word_model = os.path.join(DATA_PATH, 'crawl-300d-2M-subword.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2272843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(pretrained_word_vecs, 'r') as fin:\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     ft_model = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         ft_model[tokens[0]] = torch.Tensor([float(t) for t in tokens[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a199763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft_model = fasttext.load_model(pretrained_word_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a61bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icu_df.to_csv(OUTPUT_TEXT, columns=[\"LONG_TITLE_REPL\"], header=False, index=False, sep='\\n')\n",
    "#icu_df.to_csv(OUTPUT_TEXT, columns=[\"ICD_SUBCATEGORY_DESC_REPL\"], header=False, index=False, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "addb0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model = fasttext.train_unsupervised(OUTPUT_TEXT, model='skipgram', dim=300, minCount=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3069c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text):\n",
    "\n",
    "        self.x = text\n",
    "        self.y = [i for i in range(len(text))]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d1d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ac4a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, ft_model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        text: full corpus of text with all ICD9 code descriptions\n",
    "        ft_model: the pretrained fastText model\n",
    "        \n",
    "    Outputs:\n",
    "        vocab: a tensor of shape (# words in corpus, word embedding dim) of type torch.float\n",
    "        words: a sorted list of all words in corpus\n",
    "        lookup: a dict which returns the index value for each word\n",
    "    \"\"\"\n",
    "    word_embed_dim = ft_model.get_dimension()\n",
    "    num_codes = len(text)\n",
    "    num_words = [len(words.split()) for words in text]\n",
    "    max_num_words = max(num_words)\n",
    "    \n",
    "    words = set()\n",
    "    for w in text:\n",
    "        words.update(w.split())\n",
    "    words = sorted(list(words))\n",
    "    lookup = dict(zip(words, range(len(words))))\n",
    "    num_words = len(words)\n",
    "    \n",
    "    vocab = torch.zeros((num_words, word_embed_dim), dtype=torch.float)\n",
    "    for i, word in enumerate(words):\n",
    "        word_embed = ft_model[word]\n",
    "        vocab[i] = torch.tensor(word_embed, dtype=torch.float)\n",
    "    \n",
    "    return vocab, words, lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ba396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_built, words, lookup = build_vocab(text, ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfec2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.float\n",
    "        y: a tensor of shape (# icd9 codes) of type torch.long\n",
    "        masks: a tensor of shape (# icd9 codes, max # words, word embedding dim) of type torch.bool\n",
    "    \"\"\"\n",
    "    text, indices = zip(*data)\n",
    "    word_embed_dim = 300\n",
    "\n",
    "    y = torch.tensor(indices, dtype=torch.long)\n",
    "    num_codes = len(text)\n",
    "    num_words = [len(words.split()) for words in text]\n",
    "\n",
    "    max_num_words = max(max(num_words), 4)\n",
    "\n",
    "    global lookup\n",
    "    x = torch.zeros((num_codes, max_num_words), dtype=torch.long)\n",
    "    masks = torch.zeros((num_codes, max_num_words), dtype=torch.bool)\n",
    "    for i, code in enumerate(text):\n",
    "        for j, word in enumerate(code.split()):\n",
    "            x[i,j] = lookup[word]\n",
    "            masks[i,j] = 1\n",
    "\n",
    "    return x, y, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8dc4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, collate_fn):\n",
    "    '''\n",
    "    The model is trained on the full dataset shuffled in batches of 100, the test\n",
    "    dataset is the full dataset delievered in one large batch not shuffled.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, test_loader: train and test dataloaders\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=4903,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = load_data(dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_conv2d(outputs, masks):\n",
    "    masks = masks.unsqueeze(1)\n",
    "    masks = masks.repeat(1,100,1)\n",
    "    x = []\n",
    "    for mat in outputs:\n",
    "        outmat = mat.clone()\n",
    "        dim = outmat.shape[2]\n",
    "        outmat[~masks[:,:,:dim]] = 0\n",
    "        x.append(outmat)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfcde57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingCNN(\n",
       "  (embed): Embedding(3164, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(2, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=4903, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_descriptions, max_num_words, vocab_built,\n",
    "                 embedding_dim, num_class, num_kernel, kernel_sizes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "            num_descriptions: number of icd9 descrptions\n",
    "            embedding_dim: size of word embedding dim (from fastText)\n",
    "            num_class: number of classes to predict\n",
    "            num_kernel: number of filters for each kernel size\n",
    "            kernel_sizes: list of sizes to iterate on\n",
    "        \"\"\"\n",
    "        self.embed = nn.Embedding(len(vocab_built), embedding_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(1, num_kernel, (K, embedding_dim)) for K in kernel_sizes]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_kernel, num_descriptions)\n",
    "\n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Both the logit for training and the embedding matrix are output, so the embedding matrix\n",
    "        can be obtained once training is complete.\n",
    "        \n",
    "        Arguments:\n",
    "            x: the input tensor of icd9 description of size (batch_size, max_num_words, word_embedding_dim) \n",
    "            masks: masks for the padded words of size (batch_size, max_num_words, word_embedding_dim)\n",
    "        \n",
    "        Outputs:\n",
    "            logit: logits for cross entropy loss function to for training iterations\n",
    "            embedding: embedding matrix of learned wieghts for icd9 descriptions\n",
    "        \"\"\"\n",
    "        x = self.embed(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = mask_conv2d(x, masks)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        embedding = torch.cat(x, 1)\n",
    "        x = self.dropout(embedding)\n",
    "        logit = self.fc(x)\n",
    "        return logit, embedding\n",
    "\n",
    "embedding_cnn = EmbeddingCNN(\n",
    "    num_descriptions=len(codes), max_num_words=30, vocab_built=vocab_built,\n",
    "    embedding_dim=300, num_class=len(codes), num_kernel=100, kernel_sizes=[2,3,4])\n",
    "embedding_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8fb8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3164, 300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_built.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32fa97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(embedding_cnn.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.Adadelta(baseline_retain.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e991d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, n_epochs, n_class):\n",
    "    \"\"\" \n",
    "    Arguments:\n",
    "        model: the CNN model\n",
    "        train_loader: training dataloder\n",
    "        n_epochs: total number of epochs\n",
    "        n_class: num of classes to learn\n",
    "    \"\"\"\n",
    "    max_cpu, max_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for feature, target, masks in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit, embedding = model(feature, masks)\n",
    "            loss = criterion(logit, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        cpu, ram = print_cpu_usage()\n",
    "        max_cpu = cpu if cpu > max_cpu else max_cpu\n",
    "        max_ram = ram if ram > max_ram else max_ram\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()\n",
    "    print(f\"Max CPU usage: {max_cpu:.3f}\\tMax RAM % usage: {max_ram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03f1f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader): \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the EmbeddingCNN model\n",
    "        test_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        embedding: learned embedding matrix of the evaluated model\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "        for feature, target, masks in test_loader:\n",
    "            logit, embedding = model(feature, masks)\n",
    "            y_hat = F.softmax(logit, dim=-1)\n",
    "            pred = torch.argmax(y_hat, dim=1)\n",
    "            n_correct += (pred == target).sum()\n",
    "            n_total += pred.shape[0]\n",
    "        success = n_correct / n_total\n",
    "        print(f'{n_correct}/{n_total} correct \\t success rate: {success:.4f}')\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1256cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972345ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 8.527969\n",
      "Epoch: 2 \t Training Loss: 8.485851\n",
      "Epoch: 3 \t Training Loss: 8.357236\n",
      "Epoch: 4 \t Training Loss: 7.961706\n",
      "Epoch: 5 \t Training Loss: 7.381756\n",
      "Epoch: 6 \t Training Loss: 6.573472\n",
      "Epoch: 7 \t Training Loss: 5.590941\n",
      "Epoch: 8 \t Training Loss: 4.656707\n",
      "Epoch: 9 \t Training Loss: 3.782525\n",
      "Epoch: 10 \t Training Loss: 3.079560\n",
      "Epoch: 11 \t Training Loss: 2.493061\n",
      "Epoch: 12 \t Training Loss: 2.069575\n",
      "Epoch: 13 \t Training Loss: 1.827362\n",
      "Epoch: 14 \t Training Loss: 1.498974\n",
      "Epoch: 15 \t Training Loss: 1.300489\n",
      "Epoch: 16 \t Training Loss: 1.166075\n",
      "Epoch: 17 \t Training Loss: 1.052268\n",
      "Epoch: 18 \t Training Loss: 0.942415\n",
      "Epoch: 19 \t Training Loss: 0.846413\n",
      "Epoch: 20 \t Training Loss: 0.837249\n",
      "Epoch: 21 \t Training Loss: 0.684282\n",
      "Epoch: 22 \t Training Loss: 0.649651\n",
      "Epoch: 23 \t Training Loss: 0.604320\n",
      "Epoch: 24 \t Training Loss: 0.539677\n",
      "Epoch: 25 \t Training Loss: 0.523210\n",
      "Epoch: 26 \t Training Loss: 0.456316\n",
      "Epoch: 27 \t Training Loss: 0.412293\n",
      "Epoch: 28 \t Training Loss: 0.391364\n",
      "Epoch: 29 \t Training Loss: 0.377700\n",
      "Epoch: 30 \t Training Loss: 0.344389\n",
      "Epoch: 31 \t Training Loss: 0.321862\n",
      "Epoch: 32 \t Training Loss: 0.290615\n",
      "Epoch: 33 \t Training Loss: 0.288236\n",
      "Epoch: 34 \t Training Loss: 0.272822\n",
      "Epoch: 35 \t Training Loss: 0.271151\n",
      "Epoch: 36 \t Training Loss: 0.229866\n",
      "Epoch: 37 \t Training Loss: 0.226674\n",
      "Epoch: 38 \t Training Loss: 0.207369\n",
      "Epoch: 39 \t Training Loss: 0.200631\n",
      "Epoch: 40 \t Training Loss: 0.223546\n",
      "Epoch: 41 \t Training Loss: 0.189562\n",
      "Epoch: 42 \t Training Loss: 0.172473\n",
      "Epoch: 43 \t Training Loss: 0.168468\n",
      "Epoch: 44 \t Training Loss: 0.152221\n",
      "Epoch: 45 \t Training Loss: 0.143772\n",
      "Epoch: 46 \t Training Loss: 0.129405\n",
      "Epoch: 47 \t Training Loss: 0.139662\n",
      "Epoch: 48 \t Training Loss: 0.147840\n",
      "Epoch: 49 \t Training Loss: 0.125060\n",
      "Epoch: 50 \t Training Loss: 0.109520\n",
      "Epoch: 51 \t Training Loss: 0.123288\n",
      "Epoch: 52 \t Training Loss: 0.114695\n",
      "Epoch: 53 \t Training Loss: 0.110324\n",
      "Epoch: 54 \t Training Loss: 0.105583\n",
      "Epoch: 55 \t Training Loss: 0.095050\n",
      "Epoch: 56 \t Training Loss: 0.102023\n",
      "Epoch: 57 \t Training Loss: 0.095997\n",
      "Epoch: 58 \t Training Loss: 0.093963\n",
      "Epoch: 59 \t Training Loss: 0.094972\n",
      "Epoch: 60 \t Training Loss: 0.091075\n",
      "Epoch: 61 \t Training Loss: 0.084480\n",
      "Epoch: 62 \t Training Loss: 0.082012\n",
      "Epoch: 63 \t Training Loss: 0.095543\n",
      "Epoch: 64 \t Training Loss: 0.077323\n",
      "Epoch: 65 \t Training Loss: 0.088524\n",
      "Epoch: 66 \t Training Loss: 0.082741\n",
      "Epoch: 67 \t Training Loss: 0.077825\n",
      "Epoch: 68 \t Training Loss: 0.068630\n",
      "Epoch: 69 \t Training Loss: 0.067110\n",
      "Epoch: 70 \t Training Loss: 0.087226\n",
      "Epoch: 71 \t Training Loss: 0.072492\n",
      "Epoch: 72 \t Training Loss: 0.074759\n",
      "Epoch: 73 \t Training Loss: 0.067819\n",
      "Epoch: 74 \t Training Loss: 0.077200\n",
      "Epoch: 75 \t Training Loss: 0.069432\n",
      "Epoch: 76 \t Training Loss: 0.068515\n",
      "Epoch: 77 \t Training Loss: 0.063583\n",
      "Epoch: 78 \t Training Loss: 0.062232\n",
      "Epoch: 79 \t Training Loss: 0.058512\n",
      "Epoch: 80 \t Training Loss: 0.058946\n",
      "Epoch: 81 \t Training Loss: 0.059522\n",
      "Epoch: 82 \t Training Loss: 0.061383\n",
      "Epoch: 83 \t Training Loss: 0.055270\n",
      "Epoch: 84 \t Training Loss: 0.050571\n",
      "Epoch: 85 \t Training Loss: 0.059032\n",
      "Epoch: 86 \t Training Loss: 0.052467\n",
      "Epoch: 87 \t Training Loss: 0.055622\n",
      "Epoch: 88 \t Training Loss: 0.056571\n",
      "Epoch: 89 \t Training Loss: 0.050991\n",
      "Epoch: 90 \t Training Loss: 0.055526\n",
      "Epoch: 91 \t Training Loss: 0.054100\n",
      "Epoch: 92 \t Training Loss: 0.048141\n",
      "Epoch: 93 \t Training Loss: 0.044668\n",
      "Epoch: 94 \t Training Loss: 0.046719\n",
      "Epoch: 95 \t Training Loss: 0.046051\n",
      "Epoch: 96 \t Training Loss: 0.051889\n",
      "Epoch: 97 \t Training Loss: 0.044512\n",
      "Epoch: 98 \t Training Loss: 0.043869\n",
      "Epoch: 99 \t Training Loss: 0.047748\n",
      "Epoch: 100 \t Training Loss: 0.043910\n",
      "Epoch: 101 \t Training Loss: 0.045332\n",
      "Epoch: 102 \t Training Loss: 0.046160\n",
      "Epoch: 103 \t Training Loss: 0.052032\n",
      "Epoch: 104 \t Training Loss: 0.046680\n",
      "Epoch: 105 \t Training Loss: 0.049964\n",
      "Epoch: 106 \t Training Loss: 0.044661\n",
      "Epoch: 107 \t Training Loss: 0.044641\n",
      "Epoch: 108 \t Training Loss: 0.043710\n",
      "Epoch: 109 \t Training Loss: 0.047335\n",
      "Epoch: 110 \t Training Loss: 0.043840\n",
      "Epoch: 111 \t Training Loss: 0.043681\n",
      "Epoch: 112 \t Training Loss: 0.043903\n",
      "Epoch: 113 \t Training Loss: 0.042090\n",
      "Epoch: 114 \t Training Loss: 0.044032\n",
      "Epoch: 115 \t Training Loss: 0.044505\n",
      "Epoch: 116 \t Training Loss: 0.040033\n",
      "Epoch: 117 \t Training Loss: 0.052147\n",
      "Epoch: 118 \t Training Loss: 0.044271\n",
      "Epoch: 119 \t Training Loss: 0.044573\n",
      "Epoch: 120 \t Training Loss: 0.041346\n",
      "Epoch: 121 \t Training Loss: 0.041697\n",
      "Epoch: 122 \t Training Loss: 0.039777\n",
      "Epoch: 123 \t Training Loss: 0.039904\n",
      "Epoch: 124 \t Training Loss: 0.058213\n",
      "Epoch: 125 \t Training Loss: 0.043865\n",
      "Epoch: 126 \t Training Loss: 0.045677\n",
      "Epoch: 127 \t Training Loss: 0.049017\n",
      "Epoch: 128 \t Training Loss: 0.041234\n",
      "Epoch: 129 \t Training Loss: 0.043243\n",
      "Epoch: 130 \t Training Loss: 0.038888\n",
      "Epoch: 131 \t Training Loss: 0.038153\n",
      "Epoch: 132 \t Training Loss: 0.039775\n",
      "Epoch: 133 \t Training Loss: 0.040467\n",
      "Epoch: 134 \t Training Loss: 0.037974\n",
      "Epoch: 135 \t Training Loss: 0.040119\n",
      "Epoch: 136 \t Training Loss: 0.039979\n",
      "Epoch: 137 \t Training Loss: 0.038945\n",
      "Epoch: 138 \t Training Loss: 0.037999\n",
      "Epoch: 139 \t Training Loss: 0.039420\n",
      "Epoch: 140 \t Training Loss: 0.036840\n",
      "Epoch: 141 \t Training Loss: 0.039709\n",
      "Epoch: 142 \t Training Loss: 0.040071\n",
      "Epoch: 143 \t Training Loss: 0.037851\n",
      "Epoch: 144 \t Training Loss: 0.043181\n",
      "Epoch: 145 \t Training Loss: 0.037533\n",
      "Epoch: 146 \t Training Loss: 0.040436\n",
      "Epoch: 147 \t Training Loss: 0.036276\n",
      "Epoch: 148 \t Training Loss: 0.035435\n",
      "Epoch: 149 \t Training Loss: 0.058108\n",
      "Epoch: 150 \t Training Loss: 0.040646\n",
      "Epoch: 151 \t Training Loss: 0.041004\n",
      "Epoch: 152 \t Training Loss: 0.039095\n",
      "Epoch: 153 \t Training Loss: 0.038432\n",
      "Epoch: 154 \t Training Loss: 0.039490\n",
      "Epoch: 155 \t Training Loss: 0.037673\n",
      "Epoch: 156 \t Training Loss: 0.034335\n",
      "Epoch: 157 \t Training Loss: 0.036654\n",
      "Epoch: 158 \t Training Loss: 0.036335\n",
      "Epoch: 159 \t Training Loss: 0.051787\n",
      "Epoch: 160 \t Training Loss: 0.039183\n",
      "Epoch: 161 \t Training Loss: 0.037690\n",
      "Epoch: 162 \t Training Loss: 0.035195\n",
      "Epoch: 163 \t Training Loss: 0.036628\n",
      "Epoch: 164 \t Training Loss: 0.034193\n",
      "Epoch: 165 \t Training Loss: 0.032021\n",
      "Epoch: 166 \t Training Loss: 0.035216\n",
      "Epoch: 167 \t Training Loss: 0.033003\n",
      "Epoch: 168 \t Training Loss: 0.050973\n",
      "Epoch: 169 \t Training Loss: 0.039116\n",
      "Epoch: 170 \t Training Loss: 0.036095\n",
      "Epoch: 171 \t Training Loss: 0.035487\n",
      "Epoch: 172 \t Training Loss: 0.036547\n",
      "Epoch: 173 \t Training Loss: 0.035922\n",
      "Epoch: 174 \t Training Loss: 0.037740\n",
      "Epoch: 175 \t Training Loss: 0.034381\n",
      "Epoch: 176 \t Training Loss: 0.034902\n",
      "Epoch: 177 \t Training Loss: 0.032805\n",
      "Epoch: 178 \t Training Loss: 0.034819\n",
      "Epoch: 179 \t Training Loss: 0.033378\n",
      "Epoch: 180 \t Training Loss: 0.034404\n",
      "Epoch: 181 \t Training Loss: 0.031469\n",
      "Epoch: 182 \t Training Loss: 0.032730\n",
      "Epoch: 183 \t Training Loss: 0.034847\n",
      "Epoch: 184 \t Training Loss: 0.031686\n",
      "Epoch: 185 \t Training Loss: 0.034444\n",
      "Epoch: 186 \t Training Loss: 0.033380\n",
      "Epoch: 187 \t Training Loss: 0.032809\n",
      "Epoch: 188 \t Training Loss: 0.038947\n",
      "Epoch: 189 \t Training Loss: 0.033021\n",
      "Epoch: 190 \t Training Loss: 0.032985\n",
      "Epoch: 191 \t Training Loss: 0.034584\n",
      "Epoch: 192 \t Training Loss: 0.032974\n",
      "Epoch: 193 \t Training Loss: 0.036804\n",
      "Epoch: 194 \t Training Loss: 0.037650\n",
      "Epoch: 195 \t Training Loss: 0.034996\n",
      "Epoch: 196 \t Training Loss: 0.033322\n",
      "Epoch: 197 \t Training Loss: 0.033051\n",
      "Epoch: 198 \t Training Loss: 0.031224\n",
      "Epoch: 199 \t Training Loss: 0.033658\n",
      "Epoch: 200 \t Training Loss: 0.028813\n",
      "Epoch: 201 \t Training Loss: 0.032835\n",
      "Epoch: 202 \t Training Loss: 0.033080\n",
      "Epoch: 203 \t Training Loss: 0.034816\n",
      "Epoch: 204 \t Training Loss: 0.031632\n",
      "Epoch: 205 \t Training Loss: 0.052506\n",
      "Epoch: 206 \t Training Loss: 0.033719\n",
      "Epoch: 207 \t Training Loss: 0.030861\n",
      "Epoch: 208 \t Training Loss: 0.038575\n",
      "Epoch: 209 \t Training Loss: 0.033615\n",
      "Epoch: 210 \t Training Loss: 0.032108\n",
      "Epoch: 211 \t Training Loss: 0.028476\n",
      "Epoch: 212 \t Training Loss: 0.031843\n",
      "Epoch: 213 \t Training Loss: 0.031307\n",
      "Epoch: 214 \t Training Loss: 0.034011\n",
      "Epoch: 215 \t Training Loss: 0.029973\n",
      "Epoch: 216 \t Training Loss: 0.030811\n",
      "Epoch: 217 \t Training Loss: 0.030137\n",
      "Epoch: 218 \t Training Loss: 0.031542\n",
      "Epoch: 219 \t Training Loss: 0.032890\n",
      "Epoch: 220 \t Training Loss: 0.033295\n",
      "Epoch: 221 \t Training Loss: 0.030928\n",
      "Epoch: 222 \t Training Loss: 0.027182\n",
      "Epoch: 223 \t Training Loss: 0.029288\n",
      "Epoch: 224 \t Training Loss: 0.032307\n",
      "Epoch: 225 \t Training Loss: 0.029751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226 \t Training Loss: 0.031662\n",
      "Epoch: 227 \t Training Loss: 0.030190\n",
      "Epoch: 228 \t Training Loss: 0.027645\n",
      "Epoch: 229 \t Training Loss: 0.029301\n",
      "Epoch: 230 \t Training Loss: 0.028920\n",
      "Epoch: 231 \t Training Loss: 0.027884\n",
      "Epoch: 232 \t Training Loss: 0.043787\n",
      "Epoch: 233 \t Training Loss: 0.029446\n",
      "Epoch: 234 \t Training Loss: 0.031361\n",
      "Epoch: 235 \t Training Loss: 0.032797\n",
      "Epoch: 236 \t Training Loss: 0.030525\n",
      "Epoch: 237 \t Training Loss: 0.030813\n",
      "Epoch: 238 \t Training Loss: 0.031792\n",
      "Epoch: 239 \t Training Loss: 0.026598\n",
      "Epoch: 240 \t Training Loss: 0.031852\n",
      "Epoch: 241 \t Training Loss: 0.029479\n",
      "Epoch: 242 \t Training Loss: 0.030377\n",
      "Epoch: 243 \t Training Loss: 0.029373\n",
      "Epoch: 244 \t Training Loss: 0.031467\n",
      "Epoch: 245 \t Training Loss: 0.032780\n",
      "Epoch: 246 \t Training Loss: 0.029207\n",
      "Epoch: 247 \t Training Loss: 0.030976\n",
      "Epoch: 248 \t Training Loss: 0.028690\n",
      "Epoch: 249 \t Training Loss: 0.035247\n",
      "Epoch: 250 \t Training Loss: 0.031400\n",
      "CPU times: user 47min 6s, sys: 12min 6s, total: 59min 12s\n",
      "Wall time: 43min 34s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 250\n",
    "%time train(embedding_cnn, train_loader, n_epochs, len(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803336bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4856/4903 correct \t success rate: 0.9904\n"
     ]
    }
   ],
   "source": [
    "embedding = eval_model(embedding_cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00da3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38338927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 4903])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a694216",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(E, os.path.join(DATA_PATH, 'embedding_matrix.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding_cnn, os.path.join(CHECKPOINT_PATH, \"EmbeddingCNN_250.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
