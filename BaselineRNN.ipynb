{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data\"\n",
    "GRAM_DATA_PATH = \"../Project/code/processed_data/gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targs) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "#         x = []\n",
    "#         for i,patient in enumerate(seqs):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 x.append(visit)\n",
    "#         y = []\n",
    "#         for i,patient in enumerate(targets):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 y.append(patient[j+1])\n",
    "\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c9d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    max_num_categories = max(batch_num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             if j_visit == len(patient) - 2:\n",
    "#                 rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                 rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                 rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "\n",
    "    for i_patient, patient in enumerate(targets):\n",
    "        last_visit = patient[-1]\n",
    "        y[i_patient,:len(last_visit)] = torch.LongTensor(last_visit)\n",
    "        y_masks[i_patient,:len(last_visit)] = torch.BoolTensor(np.ones(len(last_visit)))\n",
    "#         for j_visit, visit in enumerate(patient[-1:]):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "#             y[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             y_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "    \n",
    "    \n",
    "    return x, x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c323bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         x = []\n",
    "#         for i,patient in enumerate(seqs):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 x.append(visit)\n",
    "#         y = []\n",
    "#         for i,patient in enumerate(targets):\n",
    "#             for j,visit in enumerate(patient):\n",
    "#                 if j == len(patient) - 1:\n",
    "#                     break\n",
    "#                 y.append(patient[j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    x[~masks] = 0\n",
    "    return x.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b3d8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    idx_vector = masks.any(dim=2).sum(1) - 1\n",
    "    p_idx = torch.arange(0,len(hidden_states), dtype=torch.int64)\n",
    "    last_hidden_state = hidden_states[p_idx,idx_vector]\n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineRNN(\n",
       "  (embedding): Embedding(4903, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        #import pdb; pdb.set_trace()\n",
    "        logits = self.fc(true_h_n)        \n",
    "        probs = self.softmax(logits)\n",
    "        #probs = self.sigmoid(logits)\n",
    "        \n",
    "#         probs = probs.reshape(probs.shape[0]*probs.shape[1], probs.shape[2])\n",
    "#         y_masks = y_masks.reshape(y_masks.shape[0]*y_masks.shape[1], y_masks.shape[2])\n",
    "#         probs = probs[y_masks.any(dim=1)]\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "baseline_rnn = BaselineRNN(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(baseline_rnn.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_rnn.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8a5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_old(model, test_loader, threshold=0.5, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "   Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, rev_x, rev_masks, y, y_masks in test_loader:\n",
    "            #import pdb; pdb.set_trace()\n",
    "            nn = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks, rev_x, rev_masks, y_masks)\n",
    "#             num_categories = torch.count_nonzero(y, dim=2)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            num_predictions = 0\n",
    "            num_targets = 0\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "            precision = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "#             y_masks = y_masks.reshape(\n",
    "#                 y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "#             y = y[y_masks.any(dim=1)]\n",
    "#             y_masks = y_masks[y_masks.any(dim=1)]\n",
    "\n",
    "\n",
    "#             v_idx = masks.any(dim=2)\n",
    "#             v_idx = v_idx.sum(dim=1)\n",
    "#             v_idx = v_idx.unsqueeze(-1)\n",
    "#             v_idx = v_idx.repeat(1,y_hat.shape[2])\n",
    "#             v_idx = v_idx.unsqueeze(1)\n",
    "#             y_hat = torch.gather(y_hat,1,v_idx).squeeze()\n",
    "            for i in range(k):\n",
    "                \n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "\n",
    "                y_true = y[i, y_masks[i]].unique()\n",
    "                all_targets.extend(y_true.tolist())\n",
    "                _, y_pred = torch.topk(y_hat[i], len(y_true))\n",
    "                #y_pred = torch.nonzero(y_hat[0] > threshold).squeeze()\n",
    "                if y_pred.numel() > 0:\n",
    "                    try:\n",
    "                        all_predictions.extend(y_pred.tolist())\n",
    "                    except TypeError:\n",
    "                        y_pred = [y_pred.tolist()]\n",
    "                    all_predictions.extend(y_pred)\n",
    "#                     for v in y_pred:\n",
    "#                         if v in y_true:\n",
    "#                             visit_correct += 1\n",
    "                    for v in y_true:\n",
    "                        if v in y_pred:\n",
    "                            visit_correct += 1\n",
    "                    num_predictions += len(y_pred)\n",
    "\n",
    "                num_targets += len(y_true)\n",
    "                precision += visit_correct / min(k, len(y_true))\n",
    "                k_correct += visit_correct\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    " #           import pdb; pdb.set_trace()\n",
    "            precision_k = precision / k\n",
    "#             precision_k1 = k_correct / min(k, num_targets)\n",
    "            if num_predictions == 0:\n",
    "                accuracy_k = 0\n",
    "            else:\n",
    "                accuracy_k = k_correct / num_predictions\n",
    "            precision_k = total_precision / nn\n",
    "            accuracy_k = total_accuracy / nn\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "#     roc_auc = roc_auc_score(y_true, y_score)\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b4c00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, y, y_masks in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             num_labels = y_hat.shape[1]\n",
    "#             num_categories = torch.count_nonzero(y, dim=1)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "            k_correct = 0\n",
    "#             predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "                y_true = y[i, y_masks[i]]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "#                 predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "    #base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, y, y_masks in train_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            y_hat = model(x, x_masks)\n",
    "#             import pdb; pdb.set_trace()\n",
    "#             y[~y_masks] = criterion.ignore_index\n",
    "#             last_y = y_masks.any(dim=2).sum(dim=1) - 1\n",
    "#             indices = last_y.unsqueeze(-1)\n",
    "#             indices = indices.repeat(1, y.shape[2])\n",
    "#             indices = indices.unsqueeze(1)\n",
    "#             y_filt = torch.gather(y, 1, indices)\n",
    "\n",
    "#             n_visits = y_masks.any(dim=2).sum(dim=1)\n",
    "#             for i_patient, j_visit in enumerate(n_visits):\n",
    "#                 for visit in range(j_visit - 1):\n",
    "#                     mask = y_masks[i_patient, visit+1]\n",
    "\n",
    "#                     yh = y_hat[i_patient, visit]\n",
    "#                     y_tmp = indices_to_multihot(\n",
    "#                         y[i_patient, visit+1], mask, yh)\n",
    "            \n",
    "            # stack into visits\n",
    "#             yh = y_hat.reshape(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
    "#             y_masks = y_masks.reshape(y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "        \n",
    "                    \n",
    "            \n",
    "            y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "            loss = criterion(y_hat, y_mh)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.2f}, accuracy@k{k}: {accuracy_k:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 14.57\n",
      "RAM %: 59.7\n",
      "Epoch: 1 \t Training Loss: 40.140969\n",
      "Epoch: 1 \t Validation precision@k5: 0.70, accuracy@k5: 0.37\n",
      "Epoch: 1 \t Validation precision@k10: 0.65, accuracy@k10: 0.55\n",
      "Epoch: 1 \t Validation precision@k15: 0.69, accuracy@k15: 0.67\n",
      "Epoch: 1 \t Validation precision@k20: 0.75, accuracy@k20: 0.75\n",
      "Epoch: 1 \t Validation precision@k25: 0.80, accuracy@k25: 0.80\n",
      "Epoch: 1 \t Validation precision@k30: 0.84, accuracy@k30: 0.84\n",
      "CPU: 14.93\n",
      "RAM %: 59.8\n",
      "Epoch: 2 \t Training Loss: 39.573646\n",
      "Epoch: 2 \t Validation precision@k5: 0.72, accuracy@k5: 0.38\n",
      "Epoch: 2 \t Validation precision@k10: 0.67, accuracy@k10: 0.57\n",
      "Epoch: 2 \t Validation precision@k15: 0.71, accuracy@k15: 0.68\n",
      "Epoch: 2 \t Validation precision@k20: 0.77, accuracy@k20: 0.76\n",
      "Epoch: 2 \t Validation precision@k25: 0.82, accuracy@k25: 0.82\n",
      "Epoch: 2 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 15.20\n",
      "RAM %: 59.9\n",
      "Epoch: 3 \t Training Loss: 39.102801\n",
      "Epoch: 3 \t Validation precision@k5: 0.73, accuracy@k5: 0.39\n",
      "Epoch: 3 \t Validation precision@k10: 0.69, accuracy@k10: 0.59\n",
      "Epoch: 3 \t Validation precision@k15: 0.72, accuracy@k15: 0.70\n",
      "Epoch: 3 \t Validation precision@k20: 0.78, accuracy@k20: 0.78\n",
      "Epoch: 3 \t Validation precision@k25: 0.83, accuracy@k25: 0.83\n",
      "Epoch: 3 \t Validation precision@k30: 0.87, accuracy@k30: 0.87\n",
      "CPU: 15.11\n",
      "RAM %: 59.8\n",
      "Epoch: 4 \t Training Loss: 38.692591\n",
      "Epoch: 4 \t Validation precision@k5: 0.75, accuracy@k5: 0.40\n",
      "Epoch: 4 \t Validation precision@k10: 0.70, accuracy@k10: 0.60\n",
      "Epoch: 4 \t Validation precision@k15: 0.73, accuracy@k15: 0.71\n",
      "Epoch: 4 \t Validation precision@k20: 0.79, accuracy@k20: 0.79\n",
      "Epoch: 4 \t Validation precision@k25: 0.84, accuracy@k25: 0.84\n",
      "Epoch: 4 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 15.05\n",
      "RAM %: 60.0\n",
      "Epoch: 5 \t Training Loss: 38.323916\n",
      "Epoch: 5 \t Validation precision@k5: 0.76, accuracy@k5: 0.40\n",
      "Epoch: 5 \t Validation precision@k10: 0.71, accuracy@k10: 0.61\n",
      "Epoch: 5 \t Validation precision@k15: 0.75, accuracy@k15: 0.72\n",
      "Epoch: 5 \t Validation precision@k20: 0.80, accuracy@k20: 0.80\n",
      "Epoch: 5 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 5 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 14.94\n",
      "RAM %: 60.0\n",
      "Epoch: 6 \t Training Loss: 37.988158\n",
      "Epoch: 6 \t Validation precision@k5: 0.77, accuracy@k5: 0.41\n",
      "Epoch: 6 \t Validation precision@k10: 0.72, accuracy@k10: 0.62\n",
      "Epoch: 6 \t Validation precision@k15: 0.76, accuracy@k15: 0.73\n",
      "Epoch: 6 \t Validation precision@k20: 0.81, accuracy@k20: 0.81\n",
      "Epoch: 6 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 6 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 14.81\n",
      "RAM %: 60.0\n",
      "Epoch: 7 \t Training Loss: 37.676833\n",
      "Epoch: 7 \t Validation precision@k5: 0.77, accuracy@k5: 0.42\n",
      "Epoch: 7 \t Validation precision@k10: 0.73, accuracy@k10: 0.62\n",
      "Epoch: 7 \t Validation precision@k15: 0.77, accuracy@k15: 0.74\n",
      "Epoch: 7 \t Validation precision@k20: 0.82, accuracy@k20: 0.81\n",
      "Epoch: 7 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 7 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 14.92\n",
      "RAM %: 60.3\n",
      "Epoch: 8 \t Training Loss: 37.385154\n",
      "Epoch: 8 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 8 \t Validation precision@k10: 0.74, accuracy@k10: 0.63\n",
      "Epoch: 8 \t Validation precision@k15: 0.77, accuracy@k15: 0.75\n",
      "Epoch: 8 \t Validation precision@k20: 0.83, accuracy@k20: 0.82\n",
      "Epoch: 8 \t Validation precision@k25: 0.87, accuracy@k25: 0.87\n",
      "Epoch: 8 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 15.47\n",
      "RAM %: 60.6\n",
      "Epoch: 9 \t Training Loss: 37.107706\n",
      "Epoch: 9 \t Validation precision@k5: 0.79, accuracy@k5: 0.42\n",
      "Epoch: 9 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 9 \t Validation precision@k15: 0.78, accuracy@k15: 0.76\n",
      "Epoch: 9 \t Validation precision@k20: 0.83, accuracy@k20: 0.83\n",
      "Epoch: 9 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 9 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 16.05\n",
      "RAM %: 60.7\n",
      "Epoch: 10 \t Training Loss: 36.841864\n",
      "Epoch: 10 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 10 \t Validation precision@k10: 0.76, accuracy@k10: 0.65\n",
      "Epoch: 10 \t Validation precision@k15: 0.79, accuracy@k15: 0.77\n",
      "Epoch: 10 \t Validation precision@k20: 0.84, accuracy@k20: 0.84\n",
      "Epoch: 10 \t Validation precision@k25: 0.89, accuracy@k25: 0.88\n",
      "Epoch: 10 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 16.60\n",
      "RAM %: 60.7\n",
      "Epoch: 11 \t Training Loss: 36.585113\n",
      "Epoch: 11 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 11 \t Validation precision@k10: 0.77, accuracy@k10: 0.65\n",
      "Epoch: 11 \t Validation precision@k15: 0.80, accuracy@k15: 0.77\n",
      "Epoch: 11 \t Validation precision@k20: 0.85, accuracy@k20: 0.85\n",
      "Epoch: 11 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 11 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 16.56\n",
      "RAM %: 60.8\n",
      "Epoch: 12 \t Training Loss: 36.336538\n",
      "Epoch: 12 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 12 \t Validation precision@k10: 0.77, accuracy@k10: 0.66\n",
      "Epoch: 12 \t Validation precision@k15: 0.81, accuracy@k15: 0.78\n",
      "Epoch: 12 \t Validation precision@k20: 0.86, accuracy@k20: 0.85\n",
      "Epoch: 12 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 12 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 16.91\n",
      "RAM %: 60.8\n",
      "Epoch: 13 \t Training Loss: 36.095594\n",
      "Epoch: 13 \t Validation precision@k5: 0.82, accuracy@k5: 0.44\n",
      "Epoch: 13 \t Validation precision@k10: 0.78, accuracy@k10: 0.67\n",
      "Epoch: 13 \t Validation precision@k15: 0.81, accuracy@k15: 0.79\n",
      "Epoch: 13 \t Validation precision@k20: 0.86, accuracy@k20: 0.86\n",
      "Epoch: 13 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 13 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 17.00\n",
      "RAM %: 60.9\n",
      "Epoch: 14 \t Training Loss: 35.860695\n",
      "Epoch: 14 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 14 \t Validation precision@k10: 0.79, accuracy@k10: 0.67\n",
      "Epoch: 14 \t Validation precision@k15: 0.82, accuracy@k15: 0.80\n",
      "Epoch: 14 \t Validation precision@k20: 0.87, accuracy@k20: 0.87\n",
      "Epoch: 14 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 14 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 17.08\n",
      "RAM %: 60.9\n",
      "Epoch: 15 \t Training Loss: 35.631520\n",
      "Epoch: 15 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 15 \t Validation precision@k10: 0.80, accuracy@k10: 0.68\n",
      "Epoch: 15 \t Validation precision@k15: 0.83, accuracy@k15: 0.80\n",
      "Epoch: 15 \t Validation precision@k20: 0.88, accuracy@k20: 0.87\n",
      "Epoch: 15 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 15 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 17.24\n",
      "RAM %: 60.8\n",
      "Epoch: 16 \t Training Loss: 35.407987\n",
      "Epoch: 16 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 16 \t Validation precision@k10: 0.80, accuracy@k10: 0.69\n",
      "Epoch: 16 \t Validation precision@k15: 0.84, accuracy@k15: 0.81\n",
      "Epoch: 16 \t Validation precision@k20: 0.88, accuracy@k20: 0.88\n",
      "Epoch: 16 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 16 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 17.64\n",
      "RAM %: 60.8\n",
      "Epoch: 17 \t Training Loss: 35.188153\n",
      "Epoch: 17 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 17 \t Validation precision@k10: 0.81, accuracy@k10: 0.69\n",
      "Epoch: 17 \t Validation precision@k15: 0.84, accuracy@k15: 0.82\n",
      "Epoch: 17 \t Validation precision@k20: 0.89, accuracy@k20: 0.88\n",
      "Epoch: 17 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 17 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 17.64\n",
      "RAM %: 60.9\n",
      "Epoch: 18 \t Training Loss: 34.972426\n",
      "Epoch: 18 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 18 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 18 \t Validation precision@k15: 0.85, accuracy@k15: 0.82\n",
      "Epoch: 18 \t Validation precision@k20: 0.89, accuracy@k20: 0.89\n",
      "Epoch: 18 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 18 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 18.03\n",
      "RAM %: 61.0\n",
      "Epoch: 19 \t Training Loss: 34.760888\n",
      "Epoch: 19 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 19 \t Validation precision@k10: 0.82, accuracy@k10: 0.71\n",
      "Epoch: 19 \t Validation precision@k15: 0.85, accuracy@k15: 0.83\n",
      "Epoch: 19 \t Validation precision@k20: 0.90, accuracy@k20: 0.89\n",
      "Epoch: 19 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 19 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 17.86\n",
      "RAM %: 61.1\n",
      "Epoch: 20 \t Training Loss: 34.555583\n",
      "Epoch: 20 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 20 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 20 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 20 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 20 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 20 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 30.66\n",
      "RAM %: 61.2\n",
      "Epoch: 21 \t Training Loss: 34.361229\n",
      "Epoch: 21 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 21 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 21 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 21 \t Validation precision@k20: 0.91, accuracy@k20: 0.90\n",
      "Epoch: 21 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 21 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 30.45\n",
      "RAM %: 61.4\n",
      "Epoch: 22 \t Training Loss: 34.182402\n",
      "Epoch: 22 \t Validation precision@k5: 0.87, accuracy@k5: 0.47\n",
      "Epoch: 22 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 22 \t Validation precision@k15: 0.87, accuracy@k15: 0.85\n",
      "Epoch: 22 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 22 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 22 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 30.40\n",
      "RAM %: 61.4\n",
      "Epoch: 23 \t Training Loss: 33.991842\n",
      "Epoch: 23 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 23 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 23 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 23 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 23 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 23 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 30.06\n",
      "RAM %: 61.4\n",
      "Epoch: 24 \t Training Loss: 33.798984\n",
      "Epoch: 24 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 24 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 24 \t Validation precision@k15: 0.88, accuracy@k15: 0.86\n",
      "Epoch: 24 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 24 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 24 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 29.76\n",
      "RAM %: 61.7\n",
      "Epoch: 25 \t Training Loss: 33.614725\n",
      "Epoch: 25 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 25 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 25 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 25 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 25 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 25 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 29.22\n",
      "RAM %: 61.8\n",
      "Epoch: 26 \t Training Loss: 33.437412\n",
      "Epoch: 26 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 26 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 26 \t Validation precision@k15: 0.89, accuracy@k15: 0.87\n",
      "Epoch: 26 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 26 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 26 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 28.98\n",
      "RAM %: 61.8\n",
      "Epoch: 27 \t Training Loss: 33.268289\n",
      "Epoch: 27 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 27 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 27 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 27 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 27 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 27 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 28.47\n",
      "RAM %: 61.8\n",
      "Epoch: 28 \t Training Loss: 33.101776\n",
      "Epoch: 28 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 28 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 28 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 28 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 28 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 28 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 28.97\n",
      "RAM %: 61.8\n",
      "Epoch: 29 \t Training Loss: 32.938202\n",
      "Epoch: 29 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 29 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 29 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 29 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 29 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 29 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 29.01\n",
      "RAM %: 61.6\n",
      "Epoch: 30 \t Training Loss: 32.773247\n",
      "Epoch: 30 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 30 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 30 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 30 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 30 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 30 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 28.50\n",
      "RAM %: 61.7\n",
      "Epoch: 31 \t Training Loss: 32.618849\n",
      "Epoch: 31 \t Validation precision@k5: 0.90, accuracy@k5: 0.50\n",
      "Epoch: 31 \t Validation precision@k10: 0.89, accuracy@k10: 0.76\n",
      "Epoch: 31 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 31 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 31 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 31 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 28.13\n",
      "RAM %: 61.7\n",
      "Epoch: 32 \t Training Loss: 32.473101\n",
      "Epoch: 32 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 32 \t Validation precision@k10: 0.89, accuracy@k10: 0.77\n",
      "Epoch: 32 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 32 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 32 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 32 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 28.09\n",
      "RAM %: 61.7\n",
      "Epoch: 33 \t Training Loss: 32.323736\n",
      "Epoch: 33 \t Validation precision@k5: 0.91, accuracy@k5: 0.50\n",
      "Epoch: 33 \t Validation precision@k10: 0.90, accuracy@k10: 0.77\n",
      "Epoch: 33 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 33 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 33 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 33 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 28.14\n",
      "RAM %: 61.7\n",
      "Epoch: 34 \t Training Loss: 32.188501\n",
      "Epoch: 34 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 34 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 34 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 34 \t Validation precision@k20: 0.95, accuracy@k20: 0.95\n",
      "Epoch: 34 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 34 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 27.74\n",
      "RAM %: 61.8\n",
      "Epoch: 35 \t Training Loss: 32.050820\n",
      "Epoch: 35 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 35 \t Validation precision@k10: 0.90, accuracy@k10: 0.78\n",
      "Epoch: 35 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 35 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 35 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 35 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 27.76\n",
      "RAM %: 61.7\n",
      "Epoch: 36 \t Training Loss: 31.926723\n",
      "Epoch: 36 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 36 \t Validation precision@k10: 0.91, accuracy@k10: 0.78\n",
      "Epoch: 36 \t Validation precision@k15: 0.93, accuracy@k15: 0.90\n",
      "Epoch: 36 \t Validation precision@k20: 0.96, accuracy@k20: 0.95\n",
      "Epoch: 36 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 36 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 27.84\n",
      "RAM %: 61.7\n",
      "Epoch: 37 \t Training Loss: 31.806704\n",
      "Epoch: 37 \t Validation precision@k5: 0.92, accuracy@k5: 0.51\n",
      "Epoch: 37 \t Validation precision@k10: 0.91, accuracy@k10: 0.79\n",
      "Epoch: 37 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 37 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 37 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 37 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.60\n",
      "RAM %: 61.7\n",
      "Epoch: 38 \t Training Loss: 31.685677\n",
      "Epoch: 38 \t Validation precision@k5: 0.93, accuracy@k5: 0.51\n",
      "Epoch: 38 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 38 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 38 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 38 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.55\n",
      "RAM %: 61.7\n",
      "Epoch: 39 \t Training Loss: 31.568220\n",
      "Epoch: 39 \t Validation precision@k5: 0.93, accuracy@k5: 0.52\n",
      "Epoch: 39 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 39 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 39 \t Validation precision@k20: 0.96, accuracy@k20: 0.96\n",
      "Epoch: 39 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 39 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.33\n",
      "RAM %: 62.0\n",
      "Epoch: 40 \t Training Loss: 31.452974\n",
      "Epoch: 40 \t Validation precision@k5: 0.93, accuracy@k5: 0.52\n",
      "Epoch: 40 \t Validation precision@k10: 0.92, accuracy@k10: 0.79\n",
      "Epoch: 40 \t Validation precision@k15: 0.94, accuracy@k15: 0.91\n",
      "Epoch: 40 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 40 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 40 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.05\n",
      "RAM %: 62.0\n",
      "Epoch: 41 \t Training Loss: 31.351469\n",
      "Epoch: 41 \t Validation precision@k5: 0.93, accuracy@k5: 0.52\n",
      "Epoch: 41 \t Validation precision@k10: 0.93, accuracy@k10: 0.80\n",
      "Epoch: 41 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 41 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 41 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 41 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.98\n",
      "RAM %: 61.9\n",
      "Epoch: 42 \t Training Loss: 31.251975\n",
      "Epoch: 42 \t Validation precision@k5: 0.94, accuracy@k5: 0.52\n",
      "Epoch: 42 \t Validation precision@k10: 0.93, accuracy@k10: 0.80\n",
      "Epoch: 42 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 42 \t Validation precision@k20: 0.97, accuracy@k20: 0.96\n",
      "Epoch: 42 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 42 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.35\n",
      "RAM %: 61.8\n",
      "Epoch: 43 \t Training Loss: 31.160992\n",
      "Epoch: 43 \t Validation precision@k5: 0.94, accuracy@k5: 0.52\n",
      "Epoch: 43 \t Validation precision@k10: 0.93, accuracy@k10: 0.80\n",
      "Epoch: 43 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 43 \t Validation precision@k20: 0.97, accuracy@k20: 0.97\n",
      "Epoch: 43 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 43 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.29\n",
      "RAM %: 61.9\n",
      "Epoch: 44 \t Training Loss: 31.066248\n",
      "Epoch: 44 \t Validation precision@k5: 0.94, accuracy@k5: 0.52\n",
      "Epoch: 44 \t Validation precision@k10: 0.93, accuracy@k10: 0.80\n",
      "Epoch: 44 \t Validation precision@k15: 0.95, accuracy@k15: 0.92\n",
      "Epoch: 44 \t Validation precision@k20: 0.97, accuracy@k20: 0.97\n",
      "Epoch: 44 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 44 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.04\n",
      "RAM %: 62.0\n",
      "Epoch: 45 \t Training Loss: 30.981116\n",
      "Epoch: 45 \t Validation precision@k5: 0.94, accuracy@k5: 0.53\n",
      "Epoch: 45 \t Validation precision@k10: 0.94, accuracy@k10: 0.81\n",
      "Epoch: 45 \t Validation precision@k15: 0.95, accuracy@k15: 0.93\n",
      "Epoch: 45 \t Validation precision@k20: 0.97, accuracy@k20: 0.97\n",
      "Epoch: 45 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 45 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.96\n",
      "RAM %: 62.0\n",
      "Epoch: 46 \t Training Loss: 30.889549\n",
      "Epoch: 46 \t Validation precision@k5: 0.94, accuracy@k5: 0.53\n",
      "Epoch: 46 \t Validation precision@k10: 0.94, accuracy@k10: 0.81\n",
      "Epoch: 46 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 46 \t Validation precision@k20: 0.97, accuracy@k20: 0.97\n",
      "Epoch: 46 \t Validation precision@k25: 0.98, accuracy@k25: 0.98\n",
      "Epoch: 46 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 27.08\n",
      "RAM %: 62.0\n",
      "Epoch: 47 \t Training Loss: 30.803493\n",
      "Epoch: 47 \t Validation precision@k5: 0.95, accuracy@k5: 0.53\n",
      "Epoch: 47 \t Validation precision@k10: 0.94, accuracy@k10: 0.81\n",
      "Epoch: 47 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 47 \t Validation precision@k20: 0.98, accuracy@k20: 0.97\n",
      "Epoch: 47 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 47 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.89\n",
      "RAM %: 62.1\n",
      "Epoch: 48 \t Training Loss: 30.719717\n",
      "Epoch: 48 \t Validation precision@k5: 0.95, accuracy@k5: 0.53\n",
      "Epoch: 48 \t Validation precision@k10: 0.94, accuracy@k10: 0.81\n",
      "Epoch: 48 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 48 \t Validation precision@k20: 0.98, accuracy@k20: 0.97\n",
      "Epoch: 48 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 48 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.64\n",
      "RAM %: 62.0\n",
      "Epoch: 49 \t Training Loss: 30.652565\n",
      "Epoch: 49 \t Validation precision@k5: 0.95, accuracy@k5: 0.53\n",
      "Epoch: 49 \t Validation precision@k10: 0.94, accuracy@k10: 0.81\n",
      "Epoch: 49 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 49 \t Validation precision@k20: 0.98, accuracy@k20: 0.97\n",
      "Epoch: 49 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 49 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.54\n",
      "RAM %: 61.9\n",
      "Epoch: 50 \t Training Loss: 30.572025\n",
      "Epoch: 50 \t Validation precision@k5: 0.95, accuracy@k5: 0.53\n",
      "Epoch: 50 \t Validation precision@k10: 0.94, accuracy@k10: 0.82\n",
      "Epoch: 50 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 50 \t Validation precision@k20: 0.98, accuracy@k20: 0.97\n",
      "Epoch: 50 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 50 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.46\n",
      "RAM %: 62.1\n",
      "Epoch: 51 \t Training Loss: 30.488832\n",
      "Epoch: 51 \t Validation precision@k5: 0.95, accuracy@k5: 0.53\n",
      "Epoch: 51 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 51 \t Validation precision@k15: 0.96, accuracy@k15: 0.93\n",
      "Epoch: 51 \t Validation precision@k20: 0.98, accuracy@k20: 0.97\n",
      "Epoch: 51 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 51 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.04\n",
      "RAM %: 62.0\n",
      "Epoch: 52 \t Training Loss: 30.445432\n",
      "Epoch: 52 \t Validation precision@k5: 0.95, accuracy@k5: 0.54\n",
      "Epoch: 52 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 52 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 52 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 52 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 52 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.33\n",
      "RAM %: 61.9\n",
      "Epoch: 53 \t Training Loss: 30.393563\n",
      "Epoch: 53 \t Validation precision@k5: 0.95, accuracy@k5: 0.54\n",
      "Epoch: 53 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 53 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 53 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 53 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 53 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.09\n",
      "RAM %: 61.8\n",
      "Epoch: 54 \t Training Loss: 30.319883\n",
      "Epoch: 54 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 54 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 54 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 54 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 54 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 54 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 25.76\n",
      "RAM %: 61.8\n",
      "Epoch: 55 \t Training Loss: 30.254405\n",
      "Epoch: 55 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 55 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 55 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 55 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 55 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 55 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.13\n",
      "RAM %: 62.2\n",
      "Epoch: 56 \t Training Loss: 30.182986\n",
      "Epoch: 56 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 56 \t Validation precision@k10: 0.95, accuracy@k10: 0.82\n",
      "Epoch: 56 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 56 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 56 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 56 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.02\n",
      "RAM %: 62.0\n",
      "Epoch: 57 \t Training Loss: 30.128460\n",
      "Epoch: 57 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 57 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 57 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 57 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 57 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.11\n",
      "RAM %: 62.0\n",
      "Epoch: 58 \t Training Loss: 30.098470\n",
      "Epoch: 58 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 58 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 58 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 58 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 58 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 58 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 25.99\n",
      "RAM %: 62.1\n",
      "Epoch: 59 \t Training Loss: 30.056477\n",
      "Epoch: 59 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 59 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 59 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 59 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 59 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 59 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 25.92\n",
      "RAM %: 62.1\n",
      "Epoch: 60 \t Training Loss: 29.988439\n",
      "Epoch: 60 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 60 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 60 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 60 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 60 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 60 \t Validation precision@k30: 0.99, accuracy@k30: 0.99\n",
      "CPU: 26.14\n",
      "RAM %: 61.9\n",
      "Epoch: 61 \t Training Loss: 29.959662\n",
      "Epoch: 61 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 61 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 61 \t Validation precision@k15: 0.97, accuracy@k15: 0.94\n",
      "Epoch: 61 \t Validation precision@k20: 0.98, accuracy@k20: 0.98\n",
      "Epoch: 61 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 61 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.49\n",
      "RAM %: 62.1\n",
      "Epoch: 62 \t Training Loss: 29.913799\n",
      "Epoch: 62 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 62 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 62 \t Validation precision@k15: 0.97, accuracy@k15: 0.95\n",
      "Epoch: 62 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 62 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 62 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.52\n",
      "RAM %: 62.0\n",
      "Epoch: 63 \t Training Loss: 29.845541\n",
      "Epoch: 63 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 63 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 63 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 63 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 63 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 63 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.39\n",
      "RAM %: 61.8\n",
      "Epoch: 64 \t Training Loss: 29.822548\n",
      "Epoch: 64 \t Validation precision@k5: 0.96, accuracy@k5: 0.54\n",
      "Epoch: 64 \t Validation precision@k10: 0.97, accuracy@k10: 0.83\n",
      "Epoch: 64 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 64 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 64 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 64 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.38\n",
      "RAM %: 61.9\n",
      "Epoch: 65 \t Training Loss: 29.798818\n",
      "Epoch: 65 \t Validation precision@k5: 0.97, accuracy@k5: 0.54\n",
      "Epoch: 65 \t Validation precision@k10: 0.96, accuracy@k10: 0.83\n",
      "Epoch: 65 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 65 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 65 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 65 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.10\n",
      "RAM %: 62.0\n",
      "Epoch: 66 \t Training Loss: 29.751738\n",
      "Epoch: 66 \t Validation precision@k5: 0.97, accuracy@k5: 0.54\n",
      "Epoch: 66 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 66 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 66 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 66 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 66 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 26.56\n",
      "RAM %: 62.0\n",
      "Epoch: 67 \t Training Loss: 29.691814\n",
      "Epoch: 67 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 67 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 67 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 67 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 67 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 67 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 39.75\n",
      "RAM %: 55.2\n",
      "Epoch: 68 \t Training Loss: 29.668878\n",
      "Epoch: 68 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 68 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 68 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 68 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 68 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 68 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 39.11\n",
      "RAM %: 55.8\n",
      "Epoch: 69 \t Training Loss: 29.640816\n",
      "Epoch: 69 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 69 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 69 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 69 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 69 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 69 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 37.69\n",
      "RAM %: 55.7\n",
      "Epoch: 70 \t Training Loss: 29.584895\n",
      "Epoch: 70 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 70 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 70 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 70 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 70 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 70 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 51.85\n",
      "RAM %: 56.7\n",
      "Epoch: 71 \t Training Loss: 29.580502\n",
      "Epoch: 71 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 71 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 71 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 71 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 71 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 71 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 51.01\n",
      "RAM %: 56.8\n",
      "Epoch: 72 \t Training Loss: 29.538246\n",
      "Epoch: 72 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 72 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 72 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 72 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 72 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 72 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 49.36\n",
      "RAM %: 56.8\n",
      "Epoch: 73 \t Training Loss: 29.524959\n",
      "Epoch: 73 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 73 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 73 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 73 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 73 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 73 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 48.20\n",
      "RAM %: 57.2\n",
      "Epoch: 74 \t Training Loss: 29.467991\n",
      "Epoch: 74 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 74 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 74 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 74 \t Validation precision@k20: 0.99, accuracy@k20: 0.98\n",
      "Epoch: 74 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 74 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 46.79\n",
      "RAM %: 57.3\n",
      "Epoch: 75 \t Training Loss: 29.442430\n",
      "Epoch: 75 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 75 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 75 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 75 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 75 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 75 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 47.15\n",
      "RAM %: 57.3\n",
      "Epoch: 76 \t Training Loss: 29.428351\n",
      "Epoch: 76 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 76 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 76 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 76 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 76 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 46.78\n",
      "RAM %: 57.5\n",
      "Epoch: 77 \t Training Loss: 29.403412\n",
      "Epoch: 77 \t Validation precision@k5: 0.97, accuracy@k5: 0.55\n",
      "Epoch: 77 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 77 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 77 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 77 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 77 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 46.75\n",
      "RAM %: 57.6\n",
      "Epoch: 78 \t Training Loss: 29.383380\n",
      "Epoch: 78 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 78 \t Validation precision@k10: 0.98, accuracy@k10: 0.84\n",
      "Epoch: 78 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 78 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 78 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 78 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 62.29\n",
      "RAM %: 62.3\n",
      "Epoch: 79 \t Training Loss: 29.334544\n",
      "Epoch: 79 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 79 \t Validation precision@k10: 0.97, accuracy@k10: 0.84\n",
      "Epoch: 79 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 79 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 79 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 79 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 61.82\n",
      "RAM %: 61.3\n",
      "Epoch: 80 \t Training Loss: 29.313482\n",
      "Epoch: 80 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 80 \t Validation precision@k10: 0.98, accuracy@k10: 0.84\n",
      "Epoch: 80 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 80 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 80 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 80 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 60.35\n",
      "RAM %: 59.6\n",
      "Epoch: 81 \t Training Loss: 29.302004\n",
      "Epoch: 81 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 81 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 81 \t Validation precision@k15: 0.98, accuracy@k15: 0.96\n",
      "Epoch: 81 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 81 \t Validation precision@k25: 1.00, accuracy@k25: 0.99\n",
      "Epoch: 81 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 59.02\n",
      "RAM %: 59.7\n",
      "Epoch: 82 \t Training Loss: 29.275696\n",
      "Epoch: 82 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 82 \t Validation precision@k10: 0.98, accuracy@k10: 0.84\n",
      "Epoch: 82 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 82 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 82 \t Validation precision@k25: 0.99, accuracy@k25: 0.99\n",
      "Epoch: 82 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 58.27\n",
      "RAM %: 59.7\n",
      "Epoch: 83 \t Training Loss: 29.248923\n",
      "Epoch: 83 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 83 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 83 \t Validation precision@k15: 0.98, accuracy@k15: 0.96\n",
      "Epoch: 83 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 83 \t Validation precision@k25: 1.00, accuracy@k25: 0.99\n",
      "Epoch: 83 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 56.90\n",
      "RAM %: 60.0\n",
      "Epoch: 84 \t Training Loss: 29.229201\n",
      "Epoch: 84 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 84 \t Validation precision@k10: 0.98, accuracy@k10: 0.84\n",
      "Epoch: 84 \t Validation precision@k15: 0.98, accuracy@k15: 0.95\n",
      "Epoch: 84 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 84 \t Validation precision@k25: 1.00, accuracy@k25: 0.99\n",
      "Epoch: 84 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 56.22\n",
      "RAM %: 60.1\n",
      "Epoch: 85 \t Training Loss: 29.213004\n",
      "Epoch: 85 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 85 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 85 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 85 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 85 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 85 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 54.95\n",
      "RAM %: 60.2\n",
      "Epoch: 86 \t Training Loss: 29.187579\n",
      "Epoch: 86 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 86 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 86 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 86 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 86 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 86 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 53.93\n",
      "RAM %: 60.2\n",
      "Epoch: 87 \t Training Loss: 29.156534\n",
      "Epoch: 87 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 87 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 87 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 87 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 87 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 87 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 53.21\n",
      "RAM %: 60.3\n",
      "Epoch: 88 \t Training Loss: 29.163336\n",
      "Epoch: 88 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 88 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 88 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 88 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 88 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 88 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 52.38\n",
      "RAM %: 60.3\n",
      "Epoch: 89 \t Training Loss: 29.145424\n",
      "Epoch: 89 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 89 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 89 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 89 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 89 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 89 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 51.28\n",
      "RAM %: 60.3\n",
      "Epoch: 90 \t Training Loss: 29.106542\n",
      "Epoch: 90 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 90 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 90 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 90 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 90 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 90 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 50.61\n",
      "RAM %: 60.3\n",
      "Epoch: 91 \t Training Loss: 29.096222\n",
      "Epoch: 91 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 91 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 91 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 91 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 91 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 91 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 49.64\n",
      "RAM %: 60.2\n",
      "Epoch: 92 \t Training Loss: 29.072195\n",
      "Epoch: 92 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 92 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 92 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 92 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 92 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 92 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 49.00\n",
      "RAM %: 60.3\n",
      "Epoch: 93 \t Training Loss: 29.063695\n",
      "Epoch: 93 \t Validation precision@k5: 0.98, accuracy@k5: 0.55\n",
      "Epoch: 93 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 93 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 93 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 93 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 93 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 48.07\n",
      "RAM %: 60.3\n",
      "Epoch: 94 \t Training Loss: 29.062215\n",
      "Epoch: 94 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 94 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 94 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 94 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 94 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 94 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 46.91\n",
      "RAM %: 60.3\n",
      "Epoch: 95 \t Training Loss: 29.046600\n",
      "Epoch: 95 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 95 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 95 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 95 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 95 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 46.23\n",
      "RAM %: 60.3\n",
      "Epoch: 96 \t Training Loss: 28.986934\n",
      "Epoch: 96 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 96 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 96 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 96 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 96 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 96 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 45.09\n",
      "RAM %: 60.4\n",
      "Epoch: 97 \t Training Loss: 28.976161\n",
      "Epoch: 97 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 97 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 97 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 97 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 97 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 97 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 43.99\n",
      "RAM %: 60.3\n",
      "Epoch: 98 \t Training Loss: 28.976493\n",
      "Epoch: 98 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 98 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 98 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 98 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 98 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 98 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 43.28\n",
      "RAM %: 60.4\n",
      "Epoch: 99 \t Training Loss: 28.974552\n",
      "Epoch: 99 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 99 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 99 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 99 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 99 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 99 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU: 42.71\n",
      "RAM %: 60.4\n",
      "Epoch: 100 \t Training Loss: 28.959401\n",
      "Epoch: 100 \t Validation precision@k5: 0.98, accuracy@k5: 0.56\n",
      "Epoch: 100 \t Validation precision@k10: 0.98, accuracy@k10: 0.85\n",
      "Epoch: 100 \t Validation precision@k15: 0.99, accuracy@k15: 0.96\n",
      "Epoch: 100 \t Validation precision@k20: 0.99, accuracy@k20: 0.99\n",
      "Epoch: 100 \t Validation precision@k25: 1.00, accuracy@k25: 1.00\n",
      "Epoch: 100 \t Validation precision@k30: 1.00, accuracy@k30: 1.00\n",
      "CPU times: user 1h 10min 16s, sys: 10min 3s, total: 1h 20min 20s\n",
      "Wall time: 56min 16s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_rnn, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.5799, accuracy@k5: 0.3228\n",
      "Validation precision@k10: 0.5775, accuracy@k10: 0.4986\n",
      "Validation precision@k15: 0.6306, accuracy@k15: 0.6099\n",
      "Validation precision@k20: 0.6937, accuracy@k20: 0.6901\n",
      "Validation precision@k25: 0.7456, accuracy@k25: 0.7452\n",
      "Validation precision@k30: 0.7879, accuracy@k30: 0.7879\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_rnn, val_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acba7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2232eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b7486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23348197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
