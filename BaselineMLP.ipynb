{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07324f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1156a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targets = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "prob_targets = pickle.load(open(os.path.join(DATA_PATH,'prob_targets.pkl'), 'rb'))\n",
    "prob_targets_allvisits = pickle.load(open(os.path.join(DATA_PATH,'prob_targets_allvisits.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targets) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4bc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206aa1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, prob_targets_allvisits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    num_categories = len(targets[0][0])\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    sum_visits = sum(num_visits)\n",
    "    \n",
    "    x = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.int)\n",
    "    y = torch.zeros((sum_visits - num_patients, num_categories), dtype=torch.float32)\n",
    "    x_masks = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\" \n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             #x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             x_masks[i_patient, j_visit] = 1\n",
    "#     import pdb; pdb.set_trace()\n",
    "    n = 0\n",
    "    for i,patient in enumerate(sequences):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            for k,code in enumerate(visit):\n",
    "                x[n,k] = code\n",
    "                x_masks[n,k] = 1\n",
    "            n+=1\n",
    "    n = 0\n",
    "    for i,patient in enumerate(targets):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            y[n] = torch.tensor(patient[j+1])\n",
    "            n += 1\n",
    "    \n",
    "    \n",
    "    return x, x_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7834769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e400730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e1a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27668422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d1d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, dim):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros((indices.shape[0], dim), dtype=torch.int)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx.to(torch.int64), multihot.shape[1]).sum(0)\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "637c36f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineMLP(\n",
       "  (embedding): Embedding(4903, 128, padding_idx=0)\n",
       "  (fc): Linear(in_features=128, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "#         self.padding_idx = 0\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128, padding_idx=0)\n",
    "        self.fc = nn.Linear(128, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         num_codes = self.embedding.weight.shape[0]\n",
    "#         x = indices_to_multihot(x, masks, num_codes)\n",
    "#         x[~masks] = self.padding_idx\n",
    "        x[masks] += 1\n",
    "        x = self.embedding(x)\n",
    "        x = x.sum(dim=1)\n",
    "        #x = sum_embeddings_with_mask(x, masks)\n",
    "        logits = self.fc(x)\n",
    "#         logits = logits.mean(dim=1)\n",
    "        probs = self.softmax(logits)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "baseline_mlp = BaselineMLP(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "375a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(baseline_mlp.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_mlp.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1bc112d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, y in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             num_labels = y_hat.shape[1]\n",
    "#             num_categories = torch.count_nonzero(y, dim=1)\n",
    "            nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "#             predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "                y_true = nz_cols[nz_rows == i]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "#                 predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17a85ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for x, masks, y in train_loader:\n",
    "\n",
    "            y_hat = model(x, masks)\n",
    "#             mask_idxs = masks.sum(dim=1) - 1\n",
    "#             y_hat = y_hat[range(len(masks)), mask_idxs]\n",
    "            loss = criterion(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, val_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3097025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1800d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 12.32\n",
      "RAM %: 63.1\n",
      "CPU: 12.32\n",
      "RAM %: 63.1\n",
      "Epoch: 1 \t Training Loss: 4.771595\n",
      "Epoch: 1 \t Validation precision@k5: 0.5344, accuracy@k5: 0.2719\n",
      "Epoch: 1 \t Validation precision@k10: 0.5039, accuracy@k10: 0.4238\n",
      "Epoch: 1 \t Validation precision@k15: 0.5470, accuracy@k15: 0.5286\n",
      "Epoch: 1 \t Validation precision@k20: 0.6132, accuracy@k20: 0.6103\n",
      "Epoch: 1 \t Validation precision@k25: 0.6761, accuracy@k25: 0.6758\n",
      "Epoch: 1 \t Validation precision@k30: 0.7260, accuracy@k30: 0.7260\n",
      "CPU: 12.35\n",
      "RAM %: 63.1\n",
      "Epoch: 2 \t Training Loss: 3.892900\n",
      "Epoch: 2 \t Validation precision@k5: 0.5961, accuracy@k5: 0.3053\n",
      "Epoch: 2 \t Validation precision@k10: 0.5551, accuracy@k10: 0.4689\n",
      "Epoch: 2 \t Validation precision@k15: 0.5969, accuracy@k15: 0.5774\n",
      "Epoch: 2 \t Validation precision@k20: 0.6643, accuracy@k20: 0.6611\n",
      "Epoch: 2 \t Validation precision@k25: 0.7235, accuracy@k25: 0.7232\n",
      "Epoch: 2 \t Validation precision@k30: 0.7728, accuracy@k30: 0.7728\n",
      "CPU: 12.35\n",
      "RAM %: 62.6\n",
      "Epoch: 3 \t Training Loss: 3.782546\n",
      "Epoch: 3 \t Validation precision@k5: 0.6104, accuracy@k5: 0.3128\n",
      "Epoch: 3 \t Validation precision@k10: 0.5670, accuracy@k10: 0.4792\n",
      "Epoch: 3 \t Validation precision@k15: 0.6106, accuracy@k15: 0.5908\n",
      "Epoch: 3 \t Validation precision@k20: 0.6779, accuracy@k20: 0.6747\n",
      "Epoch: 3 \t Validation precision@k25: 0.7385, accuracy@k25: 0.7382\n",
      "Epoch: 3 \t Validation precision@k30: 0.7866, accuracy@k30: 0.7866\n",
      "CPU: 12.32\n",
      "RAM %: 62.7\n",
      "Epoch: 4 \t Training Loss: 3.742609\n",
      "Epoch: 4 \t Validation precision@k5: 0.6179, accuracy@k5: 0.3176\n",
      "Epoch: 4 \t Validation precision@k10: 0.5737, accuracy@k10: 0.4850\n",
      "Epoch: 4 \t Validation precision@k15: 0.6175, accuracy@k15: 0.5974\n",
      "Epoch: 4 \t Validation precision@k20: 0.6816, accuracy@k20: 0.6784\n",
      "Epoch: 4 \t Validation precision@k25: 0.7442, accuracy@k25: 0.7439\n",
      "Epoch: 4 \t Validation precision@k30: 0.7927, accuracy@k30: 0.7927\n",
      "CPU: 12.28\n",
      "RAM %: 62.7\n",
      "Epoch: 5 \t Training Loss: 3.718154\n",
      "Epoch: 5 \t Validation precision@k5: 0.6201, accuracy@k5: 0.3184\n",
      "Epoch: 5 \t Validation precision@k10: 0.5780, accuracy@k10: 0.4886\n",
      "Epoch: 5 \t Validation precision@k15: 0.6232, accuracy@k15: 0.6030\n",
      "Epoch: 5 \t Validation precision@k20: 0.6864, accuracy@k20: 0.6832\n",
      "Epoch: 5 \t Validation precision@k25: 0.7471, accuracy@k25: 0.7468\n",
      "Epoch: 5 \t Validation precision@k30: 0.7952, accuracy@k30: 0.7952\n",
      "CPU: 12.24\n",
      "RAM %: 62.2\n",
      "Epoch: 6 \t Training Loss: 3.699704\n",
      "Epoch: 6 \t Validation precision@k5: 0.6225, accuracy@k5: 0.3198\n",
      "Epoch: 6 \t Validation precision@k10: 0.5797, accuracy@k10: 0.4900\n",
      "Epoch: 6 \t Validation precision@k15: 0.6279, accuracy@k15: 0.6075\n",
      "Epoch: 6 \t Validation precision@k20: 0.6896, accuracy@k20: 0.6864\n",
      "Epoch: 6 \t Validation precision@k25: 0.7496, accuracy@k25: 0.7493\n",
      "Epoch: 6 \t Validation precision@k30: 0.7980, accuracy@k30: 0.7980\n",
      "CPU: 12.24\n",
      "RAM %: 62.2\n",
      "Epoch: 7 \t Training Loss: 3.684527\n",
      "Epoch: 7 \t Validation precision@k5: 0.6257, accuracy@k5: 0.3213\n",
      "Epoch: 7 \t Validation precision@k10: 0.5821, accuracy@k10: 0.4922\n",
      "Epoch: 7 \t Validation precision@k15: 0.6311, accuracy@k15: 0.6106\n",
      "Epoch: 7 \t Validation precision@k20: 0.6925, accuracy@k20: 0.6893\n",
      "Epoch: 7 \t Validation precision@k25: 0.7534, accuracy@k25: 0.7531\n",
      "Epoch: 7 \t Validation precision@k30: 0.8003, accuracy@k30: 0.8003\n",
      "CPU: 12.24\n",
      "RAM %: 62.3\n",
      "Epoch: 8 \t Training Loss: 3.671481\n",
      "Epoch: 8 \t Validation precision@k5: 0.6293, accuracy@k5: 0.3231\n",
      "Epoch: 8 \t Validation precision@k10: 0.5840, accuracy@k10: 0.4938\n",
      "Epoch: 8 \t Validation precision@k15: 0.6324, accuracy@k15: 0.6119\n",
      "Epoch: 8 \t Validation precision@k20: 0.6957, accuracy@k20: 0.6925\n",
      "Epoch: 8 \t Validation precision@k25: 0.7559, accuracy@k25: 0.7556\n",
      "Epoch: 8 \t Validation precision@k30: 0.8013, accuracy@k30: 0.8013\n",
      "CPU: 12.28\n",
      "RAM %: 62.8\n",
      "Epoch: 9 \t Training Loss: 3.659964\n",
      "Epoch: 9 \t Validation precision@k5: 0.6320, accuracy@k5: 0.3244\n",
      "Epoch: 9 \t Validation precision@k10: 0.5868, accuracy@k10: 0.4963\n",
      "Epoch: 9 \t Validation precision@k15: 0.6335, accuracy@k15: 0.6130\n",
      "Epoch: 9 \t Validation precision@k20: 0.6978, accuracy@k20: 0.6945\n",
      "Epoch: 9 \t Validation precision@k25: 0.7584, accuracy@k25: 0.7581\n",
      "Epoch: 9 \t Validation precision@k30: 0.8032, accuracy@k30: 0.8032\n",
      "CPU: 12.32\n",
      "RAM %: 62.8\n",
      "Epoch: 10 \t Training Loss: 3.649602\n",
      "Epoch: 10 \t Validation precision@k5: 0.6339, accuracy@k5: 0.3249\n",
      "Epoch: 10 \t Validation precision@k10: 0.5883, accuracy@k10: 0.4973\n",
      "Epoch: 10 \t Validation precision@k15: 0.6357, accuracy@k15: 0.6151\n",
      "Epoch: 10 \t Validation precision@k20: 0.6991, accuracy@k20: 0.6959\n",
      "Epoch: 10 \t Validation precision@k25: 0.7600, accuracy@k25: 0.7597\n",
      "Epoch: 10 \t Validation precision@k30: 0.8049, accuracy@k30: 0.8049\n",
      "CPU: 12.28\n",
      "RAM %: 62.7\n",
      "Epoch: 11 \t Training Loss: 3.640144\n",
      "Epoch: 11 \t Validation precision@k5: 0.6356, accuracy@k5: 0.3255\n",
      "Epoch: 11 \t Validation precision@k10: 0.5908, accuracy@k10: 0.4994\n",
      "Epoch: 11 \t Validation precision@k15: 0.6369, accuracy@k15: 0.6162\n",
      "Epoch: 11 \t Validation precision@k20: 0.7011, accuracy@k20: 0.6979\n",
      "Epoch: 11 \t Validation precision@k25: 0.7617, accuracy@k25: 0.7614\n",
      "Epoch: 11 \t Validation precision@k30: 0.8054, accuracy@k30: 0.8054\n",
      "CPU: 12.28\n",
      "RAM %: 62.7\n",
      "Epoch: 12 \t Training Loss: 3.631410\n",
      "Epoch: 12 \t Validation precision@k5: 0.6381, accuracy@k5: 0.3271\n",
      "Epoch: 12 \t Validation precision@k10: 0.5935, accuracy@k10: 0.5017\n",
      "Epoch: 12 \t Validation precision@k15: 0.6388, accuracy@k15: 0.6181\n",
      "Epoch: 12 \t Validation precision@k20: 0.7040, accuracy@k20: 0.7007\n",
      "Epoch: 12 \t Validation precision@k25: 0.7630, accuracy@k25: 0.7626\n",
      "Epoch: 12 \t Validation precision@k30: 0.8070, accuracy@k30: 0.8070\n",
      "CPU: 12.24\n",
      "RAM %: 62.7\n",
      "Epoch: 13 \t Training Loss: 3.623267\n",
      "Epoch: 13 \t Validation precision@k5: 0.6407, accuracy@k5: 0.3288\n",
      "Epoch: 13 \t Validation precision@k10: 0.5956, accuracy@k10: 0.5035\n",
      "Epoch: 13 \t Validation precision@k15: 0.6411, accuracy@k15: 0.6202\n",
      "Epoch: 13 \t Validation precision@k20: 0.7054, accuracy@k20: 0.7021\n",
      "Epoch: 13 \t Validation precision@k25: 0.7641, accuracy@k25: 0.7638\n",
      "Epoch: 13 \t Validation precision@k30: 0.8089, accuracy@k30: 0.8089\n",
      "CPU: 12.35\n",
      "RAM %: 62.7\n",
      "Epoch: 14 \t Training Loss: 3.615613\n",
      "Epoch: 14 \t Validation precision@k5: 0.6430, accuracy@k5: 0.3302\n",
      "Epoch: 14 \t Validation precision@k10: 0.5975, accuracy@k10: 0.5050\n",
      "Epoch: 14 \t Validation precision@k15: 0.6433, accuracy@k15: 0.6224\n",
      "Epoch: 14 \t Validation precision@k20: 0.7073, accuracy@k20: 0.7041\n",
      "Epoch: 14 \t Validation precision@k25: 0.7647, accuracy@k25: 0.7643\n",
      "Epoch: 14 \t Validation precision@k30: 0.8099, accuracy@k30: 0.8099\n",
      "CPU: 12.61\n",
      "RAM %: 62.9\n",
      "Epoch: 15 \t Training Loss: 3.608371\n",
      "Epoch: 15 \t Validation precision@k5: 0.6454, accuracy@k5: 0.3314\n",
      "Epoch: 15 \t Validation precision@k10: 0.5983, accuracy@k10: 0.5057\n",
      "Epoch: 15 \t Validation precision@k15: 0.6446, accuracy@k15: 0.6237\n",
      "Epoch: 15 \t Validation precision@k20: 0.7093, accuracy@k20: 0.7061\n",
      "Epoch: 15 \t Validation precision@k25: 0.7657, accuracy@k25: 0.7654\n",
      "Epoch: 15 \t Validation precision@k30: 0.8113, accuracy@k30: 0.8113\n",
      "CPU: 12.57\n",
      "RAM %: 62.9\n",
      "Epoch: 16 \t Training Loss: 3.601483\n",
      "Epoch: 16 \t Validation precision@k5: 0.6487, accuracy@k5: 0.3336\n",
      "Epoch: 16 \t Validation precision@k10: 0.6000, accuracy@k10: 0.5071\n",
      "Epoch: 16 \t Validation precision@k15: 0.6467, accuracy@k15: 0.6257\n",
      "Epoch: 16 \t Validation precision@k20: 0.7104, accuracy@k20: 0.7071\n",
      "Epoch: 16 \t Validation precision@k25: 0.7672, accuracy@k25: 0.7669\n",
      "Epoch: 16 \t Validation precision@k30: 0.8121, accuracy@k30: 0.8121\n",
      "CPU: 12.60\n",
      "RAM %: 62.9\n",
      "Epoch: 17 \t Training Loss: 3.594904\n",
      "Epoch: 17 \t Validation precision@k5: 0.6511, accuracy@k5: 0.3349\n",
      "Epoch: 17 \t Validation precision@k10: 0.6023, accuracy@k10: 0.5091\n",
      "Epoch: 17 \t Validation precision@k15: 0.6480, accuracy@k15: 0.6269\n",
      "Epoch: 17 \t Validation precision@k20: 0.7112, accuracy@k20: 0.7080\n",
      "Epoch: 17 \t Validation precision@k25: 0.7679, accuracy@k25: 0.7675\n",
      "Epoch: 17 \t Validation precision@k30: 0.8140, accuracy@k30: 0.8140\n",
      "CPU: 12.60\n",
      "RAM %: 62.8\n",
      "Epoch: 18 \t Training Loss: 3.588601\n",
      "Epoch: 18 \t Validation precision@k5: 0.6521, accuracy@k5: 0.3350\n",
      "Epoch: 18 \t Validation precision@k10: 0.6037, accuracy@k10: 0.5100\n",
      "Epoch: 18 \t Validation precision@k15: 0.6495, accuracy@k15: 0.6284\n",
      "Epoch: 18 \t Validation precision@k20: 0.7113, accuracy@k20: 0.7081\n",
      "Epoch: 18 \t Validation precision@k25: 0.7696, accuracy@k25: 0.7693\n",
      "Epoch: 18 \t Validation precision@k30: 0.8153, accuracy@k30: 0.8153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 12.67\n",
      "RAM %: 62.7\n",
      "Epoch: 19 \t Training Loss: 3.582549\n",
      "Epoch: 19 \t Validation precision@k5: 0.6535, accuracy@k5: 0.3357\n",
      "Epoch: 19 \t Validation precision@k10: 0.6068, accuracy@k10: 0.5129\n",
      "Epoch: 19 \t Validation precision@k15: 0.6510, accuracy@k15: 0.6299\n",
      "Epoch: 19 \t Validation precision@k20: 0.7123, accuracy@k20: 0.7091\n",
      "Epoch: 19 \t Validation precision@k25: 0.7717, accuracy@k25: 0.7714\n",
      "Epoch: 19 \t Validation precision@k30: 0.8166, accuracy@k30: 0.8166\n",
      "CPU: 12.63\n",
      "RAM %: 62.5\n",
      "Epoch: 20 \t Training Loss: 3.576729\n",
      "Epoch: 20 \t Validation precision@k5: 0.6557, accuracy@k5: 0.3367\n",
      "Epoch: 20 \t Validation precision@k10: 0.6093, accuracy@k10: 0.5148\n",
      "Epoch: 20 \t Validation precision@k15: 0.6532, accuracy@k15: 0.6320\n",
      "Epoch: 20 \t Validation precision@k20: 0.7136, accuracy@k20: 0.7104\n",
      "Epoch: 20 \t Validation precision@k25: 0.7724, accuracy@k25: 0.7721\n",
      "Epoch: 20 \t Validation precision@k30: 0.8174, accuracy@k30: 0.8174\n",
      "CPU: 12.63\n",
      "RAM %: 62.4\n",
      "Epoch: 21 \t Training Loss: 3.571130\n",
      "Epoch: 21 \t Validation precision@k5: 0.6573, accuracy@k5: 0.3375\n",
      "Epoch: 21 \t Validation precision@k10: 0.6118, accuracy@k10: 0.5171\n",
      "Epoch: 21 \t Validation precision@k15: 0.6551, accuracy@k15: 0.6338\n",
      "Epoch: 21 \t Validation precision@k20: 0.7149, accuracy@k20: 0.7116\n",
      "Epoch: 21 \t Validation precision@k25: 0.7730, accuracy@k25: 0.7727\n",
      "Epoch: 21 \t Validation precision@k30: 0.8186, accuracy@k30: 0.8186\n",
      "CPU: 12.63\n",
      "RAM %: 62.5\n",
      "Epoch: 22 \t Training Loss: 3.565742\n",
      "Epoch: 22 \t Validation precision@k5: 0.6580, accuracy@k5: 0.3377\n",
      "Epoch: 22 \t Validation precision@k10: 0.6137, accuracy@k10: 0.5187\n",
      "Epoch: 22 \t Validation precision@k15: 0.6554, accuracy@k15: 0.6341\n",
      "Epoch: 22 \t Validation precision@k20: 0.7164, accuracy@k20: 0.7131\n",
      "Epoch: 22 \t Validation precision@k25: 0.7739, accuracy@k25: 0.7736\n",
      "Epoch: 22 \t Validation precision@k30: 0.8198, accuracy@k30: 0.8198\n",
      "CPU: 12.63\n",
      "RAM %: 62.3\n",
      "Epoch: 23 \t Training Loss: 3.560560\n",
      "Epoch: 23 \t Validation precision@k5: 0.6601, accuracy@k5: 0.3390\n",
      "Epoch: 23 \t Validation precision@k10: 0.6153, accuracy@k10: 0.5201\n",
      "Epoch: 23 \t Validation precision@k15: 0.6572, accuracy@k15: 0.6358\n",
      "Epoch: 23 \t Validation precision@k20: 0.7176, accuracy@k20: 0.7144\n",
      "Epoch: 23 \t Validation precision@k25: 0.7744, accuracy@k25: 0.7741\n",
      "Epoch: 23 \t Validation precision@k30: 0.8211, accuracy@k30: 0.8211\n",
      "CPU: 12.63\n",
      "RAM %: 62.7\n",
      "Epoch: 24 \t Training Loss: 3.555579\n",
      "Epoch: 24 \t Validation precision@k5: 0.6632, accuracy@k5: 0.3410\n",
      "Epoch: 24 \t Validation precision@k10: 0.6168, accuracy@k10: 0.5215\n",
      "Epoch: 24 \t Validation precision@k15: 0.6587, accuracy@k15: 0.6373\n",
      "Epoch: 24 \t Validation precision@k20: 0.7185, accuracy@k20: 0.7152\n",
      "Epoch: 24 \t Validation precision@k25: 0.7751, accuracy@k25: 0.7748\n",
      "Epoch: 24 \t Validation precision@k30: 0.8215, accuracy@k30: 0.8215\n",
      "CPU: 12.59\n",
      "RAM %: 62.8\n",
      "Epoch: 25 \t Training Loss: 3.550796\n",
      "Epoch: 25 \t Validation precision@k5: 0.6643, accuracy@k5: 0.3415\n",
      "Epoch: 25 \t Validation precision@k10: 0.6180, accuracy@k10: 0.5222\n",
      "Epoch: 25 \t Validation precision@k15: 0.6596, accuracy@k15: 0.6381\n",
      "Epoch: 25 \t Validation precision@k20: 0.7198, accuracy@k20: 0.7165\n",
      "Epoch: 25 \t Validation precision@k25: 0.7761, accuracy@k25: 0.7758\n",
      "Epoch: 25 \t Validation precision@k30: 0.8222, accuracy@k30: 0.8222\n",
      "CPU: 12.69\n",
      "RAM %: 62.8\n",
      "Epoch: 26 \t Training Loss: 3.546209\n",
      "Epoch: 26 \t Validation precision@k5: 0.6656, accuracy@k5: 0.3422\n",
      "Epoch: 26 \t Validation precision@k10: 0.6206, accuracy@k10: 0.5241\n",
      "Epoch: 26 \t Validation precision@k15: 0.6610, accuracy@k15: 0.6393\n",
      "Epoch: 26 \t Validation precision@k20: 0.7215, accuracy@k20: 0.7182\n",
      "Epoch: 26 \t Validation precision@k25: 0.7780, accuracy@k25: 0.7777\n",
      "Epoch: 26 \t Validation precision@k30: 0.8225, accuracy@k30: 0.8225\n",
      "CPU: 12.69\n",
      "RAM %: 62.7\n",
      "Epoch: 27 \t Training Loss: 3.541817\n",
      "Epoch: 27 \t Validation precision@k5: 0.6678, accuracy@k5: 0.3434\n",
      "Epoch: 27 \t Validation precision@k10: 0.6218, accuracy@k10: 0.5252\n",
      "Epoch: 27 \t Validation precision@k15: 0.6613, accuracy@k15: 0.6397\n",
      "Epoch: 27 \t Validation precision@k20: 0.7228, accuracy@k20: 0.7195\n",
      "Epoch: 27 \t Validation precision@k25: 0.7787, accuracy@k25: 0.7784\n",
      "Epoch: 27 \t Validation precision@k30: 0.8235, accuracy@k30: 0.8235\n",
      "CPU: 12.65\n",
      "RAM %: 62.7\n",
      "Epoch: 28 \t Training Loss: 3.537618\n",
      "Epoch: 28 \t Validation precision@k5: 0.6690, accuracy@k5: 0.3437\n",
      "Epoch: 28 \t Validation precision@k10: 0.6239, accuracy@k10: 0.5271\n",
      "Epoch: 28 \t Validation precision@k15: 0.6629, accuracy@k15: 0.6412\n",
      "Epoch: 28 \t Validation precision@k20: 0.7231, accuracy@k20: 0.7198\n",
      "Epoch: 28 \t Validation precision@k25: 0.7797, accuracy@k25: 0.7793\n",
      "Epoch: 28 \t Validation precision@k30: 0.8242, accuracy@k30: 0.8242\n",
      "CPU: 12.65\n",
      "RAM %: 62.7\n",
      "Epoch: 29 \t Training Loss: 3.533611\n",
      "Epoch: 29 \t Validation precision@k5: 0.6702, accuracy@k5: 0.3443\n",
      "Epoch: 29 \t Validation precision@k10: 0.6257, accuracy@k10: 0.5286\n",
      "Epoch: 29 \t Validation precision@k15: 0.6643, accuracy@k15: 0.6425\n",
      "Epoch: 29 \t Validation precision@k20: 0.7248, accuracy@k20: 0.7215\n",
      "Epoch: 29 \t Validation precision@k25: 0.7812, accuracy@k25: 0.7809\n",
      "Epoch: 29 \t Validation precision@k30: 0.8247, accuracy@k30: 0.8247\n",
      "CPU: 12.87\n",
      "RAM %: 62.7\n",
      "Epoch: 30 \t Training Loss: 3.529794\n",
      "Epoch: 30 \t Validation precision@k5: 0.6712, accuracy@k5: 0.3449\n",
      "Epoch: 30 \t Validation precision@k10: 0.6265, accuracy@k10: 0.5290\n",
      "Epoch: 30 \t Validation precision@k15: 0.6652, accuracy@k15: 0.6434\n",
      "Epoch: 30 \t Validation precision@k20: 0.7255, accuracy@k20: 0.7222\n",
      "Epoch: 30 \t Validation precision@k25: 0.7825, accuracy@k25: 0.7822\n",
      "Epoch: 30 \t Validation precision@k30: 0.8254, accuracy@k30: 0.8254\n",
      "CPU: 12.83\n",
      "RAM %: 62.7\n",
      "Epoch: 31 \t Training Loss: 3.526166\n",
      "Epoch: 31 \t Validation precision@k5: 0.6720, accuracy@k5: 0.3454\n",
      "Epoch: 31 \t Validation precision@k10: 0.6270, accuracy@k10: 0.5295\n",
      "Epoch: 31 \t Validation precision@k15: 0.6670, accuracy@k15: 0.6450\n",
      "Epoch: 31 \t Validation precision@k20: 0.7262, accuracy@k20: 0.7228\n",
      "Epoch: 31 \t Validation precision@k25: 0.7831, accuracy@k25: 0.7827\n",
      "Epoch: 31 \t Validation precision@k30: 0.8260, accuracy@k30: 0.8260\n",
      "CPU: 12.79\n",
      "RAM %: 62.7\n",
      "Epoch: 32 \t Training Loss: 3.522724\n",
      "Epoch: 32 \t Validation precision@k5: 0.6740, accuracy@k5: 0.3463\n",
      "Epoch: 32 \t Validation precision@k10: 0.6295, accuracy@k10: 0.5316\n",
      "Epoch: 32 \t Validation precision@k15: 0.6678, accuracy@k15: 0.6459\n",
      "Epoch: 32 \t Validation precision@k20: 0.7270, accuracy@k20: 0.7237\n",
      "Epoch: 32 \t Validation precision@k25: 0.7839, accuracy@k25: 0.7835\n",
      "Epoch: 32 \t Validation precision@k30: 0.8269, accuracy@k30: 0.8269\n",
      "CPU: 12.93\n",
      "RAM %: 62.7\n",
      "Epoch: 33 \t Training Loss: 3.519466\n",
      "Epoch: 33 \t Validation precision@k5: 0.6757, accuracy@k5: 0.3471\n",
      "Epoch: 33 \t Validation precision@k10: 0.6304, accuracy@k10: 0.5325\n",
      "Epoch: 33 \t Validation precision@k15: 0.6690, accuracy@k15: 0.6470\n",
      "Epoch: 33 \t Validation precision@k20: 0.7287, accuracy@k20: 0.7253\n",
      "Epoch: 33 \t Validation precision@k25: 0.7846, accuracy@k25: 0.7843\n",
      "Epoch: 33 \t Validation precision@k30: 0.8278, accuracy@k30: 0.8278\n",
      "CPU: 12.89\n",
      "RAM %: 62.7\n",
      "Epoch: 34 \t Training Loss: 3.516388\n",
      "Epoch: 34 \t Validation precision@k5: 0.6775, accuracy@k5: 0.3479\n",
      "Epoch: 34 \t Validation precision@k10: 0.6311, accuracy@k10: 0.5330\n",
      "Epoch: 34 \t Validation precision@k15: 0.6698, accuracy@k15: 0.6477\n",
      "Epoch: 34 \t Validation precision@k20: 0.7286, accuracy@k20: 0.7253\n",
      "Epoch: 34 \t Validation precision@k25: 0.7855, accuracy@k25: 0.7852\n",
      "Epoch: 34 \t Validation precision@k30: 0.8287, accuracy@k30: 0.8287\n",
      "CPU: 12.89\n",
      "RAM %: 63.0\n",
      "Epoch: 35 \t Training Loss: 3.513487\n",
      "Epoch: 35 \t Validation precision@k5: 0.6797, accuracy@k5: 0.3492\n",
      "Epoch: 35 \t Validation precision@k10: 0.6313, accuracy@k10: 0.5331\n",
      "Epoch: 35 \t Validation precision@k15: 0.6712, accuracy@k15: 0.6491\n",
      "Epoch: 35 \t Validation precision@k20: 0.7291, accuracy@k20: 0.7257\n",
      "Epoch: 35 \t Validation precision@k25: 0.7859, accuracy@k25: 0.7855\n",
      "Epoch: 35 \t Validation precision@k30: 0.8298, accuracy@k30: 0.8298\n",
      "CPU: 12.89\n",
      "RAM %: 63.0\n",
      "Epoch: 36 \t Training Loss: 3.510759\n",
      "Epoch: 36 \t Validation precision@k5: 0.6813, accuracy@k5: 0.3502\n",
      "Epoch: 36 \t Validation precision@k10: 0.6321, accuracy@k10: 0.5338\n",
      "Epoch: 36 \t Validation precision@k15: 0.6722, accuracy@k15: 0.6501\n",
      "Epoch: 36 \t Validation precision@k20: 0.7297, accuracy@k20: 0.7263\n",
      "Epoch: 36 \t Validation precision@k25: 0.7863, accuracy@k25: 0.7860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation precision@k30: 0.8309, accuracy@k30: 0.8309\n",
      "CPU: 13.10\n",
      "RAM %: 62.9\n",
      "Epoch: 37 \t Training Loss: 3.508199\n",
      "Epoch: 37 \t Validation precision@k5: 0.6816, accuracy@k5: 0.3503\n",
      "Epoch: 37 \t Validation precision@k10: 0.6336, accuracy@k10: 0.5351\n",
      "Epoch: 37 \t Validation precision@k15: 0.6727, accuracy@k15: 0.6505\n",
      "Epoch: 37 \t Validation precision@k20: 0.7301, accuracy@k20: 0.7267\n",
      "Epoch: 37 \t Validation precision@k25: 0.7876, accuracy@k25: 0.7873\n",
      "Epoch: 37 \t Validation precision@k30: 0.8317, accuracy@k30: 0.8317\n",
      "CPU: 13.35\n",
      "RAM %: 62.6\n",
      "Epoch: 38 \t Training Loss: 3.505800\n",
      "Epoch: 38 \t Validation precision@k5: 0.6832, accuracy@k5: 0.3515\n",
      "Epoch: 38 \t Validation precision@k10: 0.6338, accuracy@k10: 0.5352\n",
      "Epoch: 38 \t Validation precision@k15: 0.6740, accuracy@k15: 0.6518\n",
      "Epoch: 38 \t Validation precision@k20: 0.7304, accuracy@k20: 0.7271\n",
      "Epoch: 38 \t Validation precision@k25: 0.7890, accuracy@k25: 0.7887\n",
      "Epoch: 38 \t Validation precision@k30: 0.8326, accuracy@k30: 0.8326\n",
      "CPU: 13.31\n",
      "RAM %: 62.5\n",
      "Epoch: 39 \t Training Loss: 3.503558\n",
      "Epoch: 39 \t Validation precision@k5: 0.6849, accuracy@k5: 0.3524\n",
      "Epoch: 39 \t Validation precision@k10: 0.6344, accuracy@k10: 0.5357\n",
      "Epoch: 39 \t Validation precision@k15: 0.6742, accuracy@k15: 0.6521\n",
      "Epoch: 39 \t Validation precision@k20: 0.7309, accuracy@k20: 0.7276\n",
      "Epoch: 39 \t Validation precision@k25: 0.7889, accuracy@k25: 0.7885\n",
      "Epoch: 39 \t Validation precision@k30: 0.8329, accuracy@k30: 0.8329\n",
      "CPU: 13.34\n",
      "RAM %: 62.5\n",
      "Epoch: 40 \t Training Loss: 3.501465\n",
      "Epoch: 40 \t Validation precision@k5: 0.6850, accuracy@k5: 0.3524\n",
      "Epoch: 40 \t Validation precision@k10: 0.6351, accuracy@k10: 0.5362\n",
      "Epoch: 40 \t Validation precision@k15: 0.6750, accuracy@k15: 0.6528\n",
      "Epoch: 40 \t Validation precision@k20: 0.7318, accuracy@k20: 0.7284\n",
      "Epoch: 40 \t Validation precision@k25: 0.7894, accuracy@k25: 0.7891\n",
      "Epoch: 40 \t Validation precision@k30: 0.8332, accuracy@k30: 0.8332\n",
      "CPU: 13.34\n",
      "RAM %: 62.4\n",
      "Epoch: 41 \t Training Loss: 3.499514\n",
      "Epoch: 41 \t Validation precision@k5: 0.6865, accuracy@k5: 0.3533\n",
      "Epoch: 41 \t Validation precision@k10: 0.6359, accuracy@k10: 0.5369\n",
      "Epoch: 41 \t Validation precision@k15: 0.6758, accuracy@k15: 0.6536\n",
      "Epoch: 41 \t Validation precision@k20: 0.7332, accuracy@k20: 0.7299\n",
      "Epoch: 41 \t Validation precision@k25: 0.7897, accuracy@k25: 0.7894\n",
      "Epoch: 41 \t Validation precision@k30: 0.8332, accuracy@k30: 0.8332\n",
      "CPU: 13.56\n",
      "RAM %: 62.6\n",
      "Epoch: 42 \t Training Loss: 3.497698\n",
      "Epoch: 42 \t Validation precision@k5: 0.6864, accuracy@k5: 0.3534\n",
      "Epoch: 42 \t Validation precision@k10: 0.6364, accuracy@k10: 0.5372\n",
      "Epoch: 42 \t Validation precision@k15: 0.6754, accuracy@k15: 0.6531\n",
      "Epoch: 42 \t Validation precision@k20: 0.7337, accuracy@k20: 0.7304\n",
      "Epoch: 42 \t Validation precision@k25: 0.7901, accuracy@k25: 0.7898\n",
      "Epoch: 42 \t Validation precision@k30: 0.8341, accuracy@k30: 0.8341\n",
      "CPU: 13.55\n",
      "RAM %: 63.0\n",
      "Epoch: 43 \t Training Loss: 3.496011\n",
      "Epoch: 43 \t Validation precision@k5: 0.6873, accuracy@k5: 0.3537\n",
      "Epoch: 43 \t Validation precision@k10: 0.6370, accuracy@k10: 0.5375\n",
      "Epoch: 43 \t Validation precision@k15: 0.6760, accuracy@k15: 0.6537\n",
      "Epoch: 43 \t Validation precision@k20: 0.7338, accuracy@k20: 0.7304\n",
      "Epoch: 43 \t Validation precision@k25: 0.7903, accuracy@k25: 0.7900\n",
      "Epoch: 43 \t Validation precision@k30: 0.8347, accuracy@k30: 0.8347\n",
      "CPU: 13.54\n",
      "RAM %: 63.1\n",
      "Epoch: 44 \t Training Loss: 3.494443\n",
      "Epoch: 44 \t Validation precision@k5: 0.6886, accuracy@k5: 0.3545\n",
      "Epoch: 44 \t Validation precision@k10: 0.6378, accuracy@k10: 0.5381\n",
      "Epoch: 44 \t Validation precision@k15: 0.6764, accuracy@k15: 0.6541\n",
      "Epoch: 44 \t Validation precision@k20: 0.7341, accuracy@k20: 0.7307\n",
      "Epoch: 44 \t Validation precision@k25: 0.7904, accuracy@k25: 0.7901\n",
      "Epoch: 44 \t Validation precision@k30: 0.8350, accuracy@k30: 0.8350\n",
      "CPU: 13.49\n",
      "RAM %: 62.9\n",
      "Epoch: 45 \t Training Loss: 3.492987\n",
      "Epoch: 45 \t Validation precision@k5: 0.6899, accuracy@k5: 0.3551\n",
      "Epoch: 45 \t Validation precision@k10: 0.6386, accuracy@k10: 0.5388\n",
      "Epoch: 45 \t Validation precision@k15: 0.6762, accuracy@k15: 0.6539\n",
      "Epoch: 45 \t Validation precision@k20: 0.7353, accuracy@k20: 0.7320\n",
      "Epoch: 45 \t Validation precision@k25: 0.7903, accuracy@k25: 0.7900\n",
      "Epoch: 45 \t Validation precision@k30: 0.8351, accuracy@k30: 0.8351\n",
      "CPU: 13.60\n",
      "RAM %: 63.1\n",
      "Epoch: 46 \t Training Loss: 3.491637\n",
      "Epoch: 46 \t Validation precision@k5: 0.6905, accuracy@k5: 0.3553\n",
      "Epoch: 46 \t Validation precision@k10: 0.6395, accuracy@k10: 0.5397\n",
      "Epoch: 46 \t Validation precision@k15: 0.6767, accuracy@k15: 0.6544\n",
      "Epoch: 46 \t Validation precision@k20: 0.7353, accuracy@k20: 0.7319\n",
      "Epoch: 46 \t Validation precision@k25: 0.7910, accuracy@k25: 0.7907\n",
      "Epoch: 46 \t Validation precision@k30: 0.8350, accuracy@k30: 0.8350\n",
      "CPU: 13.63\n",
      "RAM %: 63.1\n",
      "Epoch: 47 \t Training Loss: 3.490385\n",
      "Epoch: 47 \t Validation precision@k5: 0.6907, accuracy@k5: 0.3557\n",
      "Epoch: 47 \t Validation precision@k10: 0.6397, accuracy@k10: 0.5399\n",
      "Epoch: 47 \t Validation precision@k15: 0.6773, accuracy@k15: 0.6550\n",
      "Epoch: 47 \t Validation precision@k20: 0.7367, accuracy@k20: 0.7333\n",
      "Epoch: 47 \t Validation precision@k25: 0.7912, accuracy@k25: 0.7909\n",
      "Epoch: 47 \t Validation precision@k30: 0.8347, accuracy@k30: 0.8347\n",
      "CPU: 13.63\n",
      "RAM %: 63.1\n",
      "Epoch: 48 \t Training Loss: 3.489223\n",
      "Epoch: 48 \t Validation precision@k5: 0.6907, accuracy@k5: 0.3556\n",
      "Epoch: 48 \t Validation precision@k10: 0.6402, accuracy@k10: 0.5403\n",
      "Epoch: 48 \t Validation precision@k15: 0.6781, accuracy@k15: 0.6557\n",
      "Epoch: 48 \t Validation precision@k20: 0.7366, accuracy@k20: 0.7333\n",
      "Epoch: 48 \t Validation precision@k25: 0.7914, accuracy@k25: 0.7911\n",
      "Epoch: 48 \t Validation precision@k30: 0.8347, accuracy@k30: 0.8347\n",
      "CPU: 13.73\n",
      "RAM %: 62.8\n",
      "Epoch: 49 \t Training Loss: 3.488146\n",
      "Epoch: 49 \t Validation precision@k5: 0.6907, accuracy@k5: 0.3557\n",
      "Epoch: 49 \t Validation precision@k10: 0.6404, accuracy@k10: 0.5405\n",
      "Epoch: 49 \t Validation precision@k15: 0.6779, accuracy@k15: 0.6554\n",
      "Epoch: 49 \t Validation precision@k20: 0.7370, accuracy@k20: 0.7337\n",
      "Epoch: 49 \t Validation precision@k25: 0.7914, accuracy@k25: 0.7911\n",
      "Epoch: 49 \t Validation precision@k30: 0.8352, accuracy@k30: 0.8352\n",
      "CPU: 13.83\n",
      "RAM %: 62.9\n",
      "Epoch: 50 \t Training Loss: 3.487146\n",
      "Epoch: 50 \t Validation precision@k5: 0.6918, accuracy@k5: 0.3562\n",
      "Epoch: 50 \t Validation precision@k10: 0.6408, accuracy@k10: 0.5408\n",
      "Epoch: 50 \t Validation precision@k15: 0.6780, accuracy@k15: 0.6556\n",
      "Epoch: 50 \t Validation precision@k20: 0.7379, accuracy@k20: 0.7346\n",
      "Epoch: 50 \t Validation precision@k25: 0.7920, accuracy@k25: 0.7917\n",
      "Epoch: 50 \t Validation precision@k30: 0.8354, accuracy@k30: 0.8354\n",
      "CPU: 13.82\n",
      "RAM %: 62.8\n",
      "Epoch: 51 \t Training Loss: 3.486218\n",
      "Epoch: 51 \t Validation precision@k5: 0.6921, accuracy@k5: 0.3565\n",
      "Epoch: 51 \t Validation precision@k10: 0.6416, accuracy@k10: 0.5416\n",
      "Epoch: 51 \t Validation precision@k15: 0.6781, accuracy@k15: 0.6556\n",
      "Epoch: 51 \t Validation precision@k20: 0.7389, accuracy@k20: 0.7355\n",
      "Epoch: 51 \t Validation precision@k25: 0.7922, accuracy@k25: 0.7919\n",
      "Epoch: 51 \t Validation precision@k30: 0.8356, accuracy@k30: 0.8356\n",
      "CPU: 13.81\n",
      "RAM %: 62.8\n",
      "Epoch: 52 \t Training Loss: 3.485357\n",
      "Epoch: 52 \t Validation precision@k5: 0.6924, accuracy@k5: 0.3566\n",
      "Epoch: 52 \t Validation precision@k10: 0.6422, accuracy@k10: 0.5421\n",
      "Epoch: 52 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6566\n",
      "Epoch: 52 \t Validation precision@k20: 0.7388, accuracy@k20: 0.7355\n",
      "Epoch: 52 \t Validation precision@k25: 0.7921, accuracy@k25: 0.7917\n",
      "Epoch: 52 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 13.80\n",
      "RAM %: 62.8\n",
      "Epoch: 53 \t Training Loss: 3.484556\n",
      "Epoch: 53 \t Validation precision@k5: 0.6922, accuracy@k5: 0.3566\n",
      "Epoch: 53 \t Validation precision@k10: 0.6421, accuracy@k10: 0.5421\n",
      "Epoch: 53 \t Validation precision@k15: 0.6788, accuracy@k15: 0.6563\n",
      "Epoch: 53 \t Validation precision@k20: 0.7395, accuracy@k20: 0.7362\n",
      "Epoch: 53 \t Validation precision@k25: 0.7923, accuracy@k25: 0.7920\n",
      "Epoch: 53 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 13.75\n",
      "RAM %: 62.9\n",
      "Epoch: 54 \t Training Loss: 3.483812\n",
      "Epoch: 54 \t Validation precision@k5: 0.6919, accuracy@k5: 0.3565\n",
      "Epoch: 54 \t Validation precision@k10: 0.6420, accuracy@k10: 0.5421\n",
      "Epoch: 54 \t Validation precision@k15: 0.6793, accuracy@k15: 0.6568\n",
      "Epoch: 54 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 \t Validation precision@k25: 0.7924, accuracy@k25: 0.7920\n",
      "Epoch: 54 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 13.75\n",
      "RAM %: 62.8\n",
      "Epoch: 55 \t Training Loss: 3.483120\n",
      "Epoch: 55 \t Validation precision@k5: 0.6923, accuracy@k5: 0.3568\n",
      "Epoch: 55 \t Validation precision@k10: 0.6422, accuracy@k10: 0.5423\n",
      "Epoch: 55 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6561\n",
      "Epoch: 55 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7367\n",
      "Epoch: 55 \t Validation precision@k25: 0.7924, accuracy@k25: 0.7920\n",
      "Epoch: 55 \t Validation precision@k30: 0.8360, accuracy@k30: 0.8360\n",
      "CPU: 13.75\n",
      "RAM %: 62.5\n",
      "Epoch: 56 \t Training Loss: 3.482477\n",
      "Epoch: 56 \t Validation precision@k5: 0.6924, accuracy@k5: 0.3568\n",
      "Epoch: 56 \t Validation precision@k10: 0.6425, accuracy@k10: 0.5426\n",
      "Epoch: 56 \t Validation precision@k15: 0.6784, accuracy@k15: 0.6560\n",
      "Epoch: 56 \t Validation precision@k20: 0.7402, accuracy@k20: 0.7368\n",
      "Epoch: 56 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 56 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 13.70\n",
      "RAM %: 62.6\n",
      "Epoch: 57 \t Training Loss: 3.481877\n",
      "Epoch: 57 \t Validation precision@k5: 0.6921, accuracy@k5: 0.3566\n",
      "Epoch: 57 \t Validation precision@k10: 0.6430, accuracy@k10: 0.5430\n",
      "Epoch: 57 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6561\n",
      "Epoch: 57 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7364\n",
      "Epoch: 57 \t Validation precision@k25: 0.7933, accuracy@k25: 0.7930\n",
      "Epoch: 57 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 13.69\n",
      "RAM %: 62.6\n",
      "Epoch: 58 \t Training Loss: 3.481318\n",
      "Epoch: 58 \t Validation precision@k5: 0.6925, accuracy@k5: 0.3569\n",
      "Epoch: 58 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5429\n",
      "Epoch: 58 \t Validation precision@k15: 0.6783, accuracy@k15: 0.6558\n",
      "Epoch: 58 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7366\n",
      "Epoch: 58 \t Validation precision@k25: 0.7936, accuracy@k25: 0.7933\n",
      "Epoch: 58 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 13.64\n",
      "RAM %: 62.8\n",
      "Epoch: 59 \t Training Loss: 3.480797\n",
      "Epoch: 59 \t Validation precision@k5: 0.6928, accuracy@k5: 0.3571\n",
      "Epoch: 59 \t Validation precision@k10: 0.6432, accuracy@k10: 0.5433\n",
      "Epoch: 59 \t Validation precision@k15: 0.6783, accuracy@k15: 0.6559\n",
      "Epoch: 59 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7364\n",
      "Epoch: 59 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7936\n",
      "Epoch: 59 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 13.89\n",
      "RAM %: 62.9\n",
      "Epoch: 60 \t Training Loss: 3.480311\n",
      "Epoch: 60 \t Validation precision@k5: 0.6930, accuracy@k5: 0.3573\n",
      "Epoch: 60 \t Validation precision@k10: 0.6426, accuracy@k10: 0.5427\n",
      "Epoch: 60 \t Validation precision@k15: 0.6783, accuracy@k15: 0.6558\n",
      "Epoch: 60 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7365\n",
      "Epoch: 60 \t Validation precision@k25: 0.7941, accuracy@k25: 0.7938\n",
      "Epoch: 60 \t Validation precision@k30: 0.8356, accuracy@k30: 0.8356\n",
      "CPU: 13.92\n",
      "RAM %: 62.8\n",
      "Epoch: 61 \t Training Loss: 3.479857\n",
      "Epoch: 61 \t Validation precision@k5: 0.6926, accuracy@k5: 0.3571\n",
      "Epoch: 61 \t Validation precision@k10: 0.6425, accuracy@k10: 0.5427\n",
      "Epoch: 61 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6562\n",
      "Epoch: 61 \t Validation precision@k20: 0.7395, accuracy@k20: 0.7361\n",
      "Epoch: 61 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7936\n",
      "Epoch: 61 \t Validation precision@k30: 0.8354, accuracy@k30: 0.8354\n",
      "CPU: 13.92\n",
      "RAM %: 62.7\n",
      "Epoch: 62 \t Training Loss: 3.479433\n",
      "Epoch: 62 \t Validation precision@k5: 0.6929, accuracy@k5: 0.3572\n",
      "Epoch: 62 \t Validation precision@k10: 0.6433, accuracy@k10: 0.5434\n",
      "Epoch: 62 \t Validation precision@k15: 0.6781, accuracy@k15: 0.6557\n",
      "Epoch: 62 \t Validation precision@k20: 0.7397, accuracy@k20: 0.7364\n",
      "Epoch: 62 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7936\n",
      "Epoch: 62 \t Validation precision@k30: 0.8352, accuracy@k30: 0.8352\n",
      "CPU: 13.95\n",
      "RAM %: 62.9\n",
      "Epoch: 63 \t Training Loss: 3.479037\n",
      "Epoch: 63 \t Validation precision@k5: 0.6934, accuracy@k5: 0.3576\n",
      "Epoch: 63 \t Validation precision@k10: 0.6437, accuracy@k10: 0.5437\n",
      "Epoch: 63 \t Validation precision@k15: 0.6782, accuracy@k15: 0.6557\n",
      "Epoch: 63 \t Validation precision@k20: 0.7397, accuracy@k20: 0.7363\n",
      "Epoch: 63 \t Validation precision@k25: 0.7943, accuracy@k25: 0.7940\n",
      "Epoch: 63 \t Validation precision@k30: 0.8351, accuracy@k30: 0.8351\n",
      "CPU: 13.94\n",
      "RAM %: 63.1\n",
      "Epoch: 64 \t Training Loss: 3.478667\n",
      "Epoch: 64 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3578\n",
      "Epoch: 64 \t Validation precision@k10: 0.6437, accuracy@k10: 0.5437\n",
      "Epoch: 64 \t Validation precision@k15: 0.6783, accuracy@k15: 0.6558\n",
      "Epoch: 64 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7364\n",
      "Epoch: 64 \t Validation precision@k25: 0.7944, accuracy@k25: 0.7941\n",
      "Epoch: 64 \t Validation precision@k30: 0.8355, accuracy@k30: 0.8355\n",
      "CPU: 14.18\n",
      "RAM %: 63.2\n",
      "Epoch: 65 \t Training Loss: 3.478321\n",
      "Epoch: 65 \t Validation precision@k5: 0.6940, accuracy@k5: 0.3580\n",
      "Epoch: 65 \t Validation precision@k10: 0.6433, accuracy@k10: 0.5434\n",
      "Epoch: 65 \t Validation precision@k15: 0.6782, accuracy@k15: 0.6557\n",
      "Epoch: 65 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7367\n",
      "Epoch: 65 \t Validation precision@k25: 0.7947, accuracy@k25: 0.7943\n",
      "Epoch: 65 \t Validation precision@k30: 0.8354, accuracy@k30: 0.8354\n",
      "CPU: 14.28\n",
      "RAM %: 63.1\n",
      "Epoch: 66 \t Training Loss: 3.477997\n",
      "Epoch: 66 \t Validation precision@k5: 0.6941, accuracy@k5: 0.3581\n",
      "Epoch: 66 \t Validation precision@k10: 0.6436, accuracy@k10: 0.5437\n",
      "Epoch: 66 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6562\n",
      "Epoch: 66 \t Validation precision@k20: 0.7399, accuracy@k20: 0.7365\n",
      "Epoch: 66 \t Validation precision@k25: 0.7946, accuracy@k25: 0.7943\n",
      "Epoch: 66 \t Validation precision@k30: 0.8355, accuracy@k30: 0.8355\n",
      "CPU: 14.42\n",
      "RAM %: 63.1\n",
      "Epoch: 67 \t Training Loss: 3.477693\n",
      "Epoch: 67 \t Validation precision@k5: 0.6941, accuracy@k5: 0.3581\n",
      "Epoch: 67 \t Validation precision@k10: 0.6438, accuracy@k10: 0.5438\n",
      "Epoch: 67 \t Validation precision@k15: 0.6787, accuracy@k15: 0.6563\n",
      "Epoch: 67 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7367\n",
      "Epoch: 67 \t Validation precision@k25: 0.7946, accuracy@k25: 0.7943\n",
      "Epoch: 67 \t Validation precision@k30: 0.8355, accuracy@k30: 0.8355\n",
      "CPU: 14.40\n",
      "RAM %: 62.7\n",
      "Epoch: 68 \t Training Loss: 3.477409\n",
      "Epoch: 68 \t Validation precision@k5: 0.6946, accuracy@k5: 0.3585\n",
      "Epoch: 68 \t Validation precision@k10: 0.6438, accuracy@k10: 0.5439\n",
      "Epoch: 68 \t Validation precision@k15: 0.6787, accuracy@k15: 0.6563\n",
      "Epoch: 68 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7367\n",
      "Epoch: 68 \t Validation precision@k25: 0.7946, accuracy@k25: 0.7943\n",
      "Epoch: 68 \t Validation precision@k30: 0.8355, accuracy@k30: 0.8355\n",
      "CPU: 14.40\n",
      "RAM %: 62.7\n",
      "Epoch: 69 \t Training Loss: 3.477143\n",
      "Epoch: 69 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3582\n",
      "Epoch: 69 \t Validation precision@k10: 0.6435, accuracy@k10: 0.5436\n",
      "Epoch: 69 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6562\n",
      "Epoch: 69 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7364\n",
      "Epoch: 69 \t Validation precision@k25: 0.7946, accuracy@k25: 0.7943\n",
      "Epoch: 69 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 14.36\n",
      "RAM %: 62.5\n",
      "Epoch: 70 \t Training Loss: 3.476894\n",
      "Epoch: 70 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3582\n",
      "Epoch: 70 \t Validation precision@k10: 0.6435, accuracy@k10: 0.5437\n",
      "Epoch: 70 \t Validation precision@k15: 0.6784, accuracy@k15: 0.6560\n",
      "Epoch: 70 \t Validation precision@k20: 0.7394, accuracy@k20: 0.7361\n",
      "Epoch: 70 \t Validation precision@k25: 0.7945, accuracy@k25: 0.7941\n",
      "Epoch: 70 \t Validation precision@k30: 0.8360, accuracy@k30: 0.8360\n",
      "CPU: 14.56\n",
      "RAM %: 62.8\n",
      "Epoch: 71 \t Training Loss: 3.476661\n",
      "Epoch: 71 \t Validation precision@k5: 0.6939, accuracy@k5: 0.3583\n",
      "Epoch: 71 \t Validation precision@k10: 0.6433, accuracy@k10: 0.5435\n",
      "Epoch: 71 \t Validation precision@k15: 0.6782, accuracy@k15: 0.6558\n",
      "Epoch: 71 \t Validation precision@k20: 0.7396, accuracy@k20: 0.7363\n",
      "Epoch: 71 \t Validation precision@k25: 0.7944, accuracy@k25: 0.7941\n",
      "Epoch: 71 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 14.84\n",
      "RAM %: 63.2\n",
      "Epoch: 72 \t Training Loss: 3.476442\n",
      "Epoch: 72 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3582\n",
      "Epoch: 72 \t Validation precision@k10: 0.6436, accuracy@k10: 0.5438\n",
      "Epoch: 72 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 \t Validation precision@k20: 0.7396, accuracy@k20: 0.7363\n",
      "Epoch: 72 \t Validation precision@k25: 0.7944, accuracy@k25: 0.7940\n",
      "Epoch: 72 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 14.83\n",
      "RAM %: 63.2\n",
      "Epoch: 73 \t Training Loss: 3.476237\n",
      "Epoch: 73 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3582\n",
      "Epoch: 73 \t Validation precision@k10: 0.6434, accuracy@k10: 0.5436\n",
      "Epoch: 73 \t Validation precision@k15: 0.6788, accuracy@k15: 0.6564\n",
      "Epoch: 73 \t Validation precision@k20: 0.7397, accuracy@k20: 0.7363\n",
      "Epoch: 73 \t Validation precision@k25: 0.7941, accuracy@k25: 0.7937\n",
      "Epoch: 73 \t Validation precision@k30: 0.8359, accuracy@k30: 0.8359\n",
      "CPU: 14.81\n",
      "RAM %: 62.7\n",
      "Epoch: 74 \t Training Loss: 3.476045\n",
      "Epoch: 74 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3583\n",
      "Epoch: 74 \t Validation precision@k10: 0.6435, accuracy@k10: 0.5437\n",
      "Epoch: 74 \t Validation precision@k15: 0.6789, accuracy@k15: 0.6565\n",
      "Epoch: 74 \t Validation precision@k20: 0.7396, accuracy@k20: 0.7362\n",
      "Epoch: 74 \t Validation precision@k25: 0.7941, accuracy@k25: 0.7938\n",
      "Epoch: 74 \t Validation precision@k30: 0.8356, accuracy@k30: 0.8356\n",
      "CPU: 14.80\n",
      "RAM %: 62.6\n",
      "Epoch: 75 \t Training Loss: 3.475865\n",
      "Epoch: 75 \t Validation precision@k5: 0.6935, accuracy@k5: 0.3583\n",
      "Epoch: 75 \t Validation precision@k10: 0.6434, accuracy@k10: 0.5436\n",
      "Epoch: 75 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6566\n",
      "Epoch: 75 \t Validation precision@k20: 0.7397, accuracy@k20: 0.7364\n",
      "Epoch: 75 \t Validation precision@k25: 0.7941, accuracy@k25: 0.7937\n",
      "Epoch: 75 \t Validation precision@k30: 0.8356, accuracy@k30: 0.8356\n",
      "CPU: 14.80\n",
      "RAM %: 62.4\n",
      "Epoch: 76 \t Training Loss: 3.475696\n",
      "Epoch: 76 \t Validation precision@k5: 0.6936, accuracy@k5: 0.3582\n",
      "Epoch: 76 \t Validation precision@k10: 0.6434, accuracy@k10: 0.5437\n",
      "Epoch: 76 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6566\n",
      "Epoch: 76 \t Validation precision@k20: 0.7398, accuracy@k20: 0.7365\n",
      "Epoch: 76 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7936\n",
      "Epoch: 76 \t Validation precision@k30: 0.8356, accuracy@k30: 0.8356\n",
      "CPU: 14.78\n",
      "RAM %: 62.4\n",
      "Epoch: 77 \t Training Loss: 3.475538\n",
      "Epoch: 77 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3583\n",
      "Epoch: 77 \t Validation precision@k10: 0.6434, accuracy@k10: 0.5436\n",
      "Epoch: 77 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6567\n",
      "Epoch: 77 \t Validation precision@k20: 0.7399, accuracy@k20: 0.7366\n",
      "Epoch: 77 \t Validation precision@k25: 0.7937, accuracy@k25: 0.7934\n",
      "Epoch: 77 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 14.88\n",
      "RAM %: 62.3\n",
      "Epoch: 78 \t Training Loss: 3.475390\n",
      "Epoch: 78 \t Validation precision@k5: 0.6936, accuracy@k5: 0.3582\n",
      "Epoch: 78 \t Validation precision@k10: 0.6434, accuracy@k10: 0.5436\n",
      "Epoch: 78 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6569\n",
      "Epoch: 78 \t Validation precision@k20: 0.7397, accuracy@k20: 0.7363\n",
      "Epoch: 78 \t Validation precision@k25: 0.7936, accuracy@k25: 0.7932\n",
      "Epoch: 78 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 14.94\n",
      "RAM %: 62.3\n",
      "Epoch: 79 \t Training Loss: 3.475250\n",
      "Epoch: 79 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3583\n",
      "Epoch: 79 \t Validation precision@k10: 0.6432, accuracy@k10: 0.5434\n",
      "Epoch: 79 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6567\n",
      "Epoch: 79 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7367\n",
      "Epoch: 79 \t Validation precision@k25: 0.7935, accuracy@k25: 0.7932\n",
      "Epoch: 79 \t Validation precision@k30: 0.8359, accuracy@k30: 0.8359\n",
      "CPU: 14.92\n",
      "RAM %: 62.3\n",
      "Epoch: 80 \t Training Loss: 3.475120\n",
      "Epoch: 80 \t Validation precision@k5: 0.6936, accuracy@k5: 0.3581\n",
      "Epoch: 80 \t Validation precision@k10: 0.6433, accuracy@k10: 0.5435\n",
      "Epoch: 80 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 80 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7367\n",
      "Epoch: 80 \t Validation precision@k25: 0.7934, accuracy@k25: 0.7931\n",
      "Epoch: 80 \t Validation precision@k30: 0.8359, accuracy@k30: 0.8359\n",
      "CPU: 14.90\n",
      "RAM %: 62.3\n",
      "Epoch: 81 \t Training Loss: 3.474998\n",
      "Epoch: 81 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3582\n",
      "Epoch: 81 \t Validation precision@k10: 0.6432, accuracy@k10: 0.5434\n",
      "Epoch: 81 \t Validation precision@k15: 0.6793, accuracy@k15: 0.6569\n",
      "Epoch: 81 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7368\n",
      "Epoch: 81 \t Validation precision@k25: 0.7933, accuracy@k25: 0.7929\n",
      "Epoch: 81 \t Validation precision@k30: 0.8359, accuracy@k30: 0.8359\n",
      "CPU: 14.90\n",
      "RAM %: 63.3\n",
      "Epoch: 82 \t Training Loss: 3.474884\n",
      "Epoch: 82 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3582\n",
      "Epoch: 82 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 82 \t Validation precision@k15: 0.6794, accuracy@k15: 0.6570\n",
      "Epoch: 82 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7366\n",
      "Epoch: 82 \t Validation precision@k25: 0.7932, accuracy@k25: 0.7928\n",
      "Epoch: 82 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 15.04\n",
      "RAM %: 63.3\n",
      "Epoch: 83 \t Training Loss: 3.474777\n",
      "Epoch: 83 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3582\n",
      "Epoch: 83 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 83 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 83 \t Validation precision@k20: 0.7404, accuracy@k20: 0.7370\n",
      "Epoch: 83 \t Validation precision@k25: 0.7931, accuracy@k25: 0.7928\n",
      "Epoch: 83 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 14.98\n",
      "RAM %: 63.3\n",
      "Epoch: 84 \t Training Loss: 3.474676\n",
      "Epoch: 84 \t Validation precision@k5: 0.6935, accuracy@k5: 0.3580\n",
      "Epoch: 84 \t Validation precision@k10: 0.6427, accuracy@k10: 0.5429\n",
      "Epoch: 84 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6567\n",
      "Epoch: 84 \t Validation precision@k20: 0.7399, accuracy@k20: 0.7366\n",
      "Epoch: 84 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 84 \t Validation precision@k30: 0.8357, accuracy@k30: 0.8357\n",
      "CPU: 15.08\n",
      "RAM %: 63.5\n",
      "Epoch: 85 \t Training Loss: 3.474582\n",
      "Epoch: 85 \t Validation precision@k5: 0.6935, accuracy@k5: 0.3580\n",
      "Epoch: 85 \t Validation precision@k10: 0.6426, accuracy@k10: 0.5428\n",
      "Epoch: 85 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6567\n",
      "Epoch: 85 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7366\n",
      "Epoch: 85 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 85 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 15.06\n",
      "RAM %: 63.8\n",
      "Epoch: 86 \t Training Loss: 3.474494\n",
      "Epoch: 86 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3581\n",
      "Epoch: 86 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 86 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 86 \t Validation precision@k20: 0.7400, accuracy@k20: 0.7366\n",
      "Epoch: 86 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 86 \t Validation precision@k30: 0.8358, accuracy@k30: 0.8358\n",
      "CPU: 15.27\n",
      "RAM %: 63.6\n",
      "Epoch: 87 \t Training Loss: 3.474412\n",
      "Epoch: 87 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3581\n",
      "Epoch: 87 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 87 \t Validation precision@k15: 0.6793, accuracy@k15: 0.6569\n",
      "Epoch: 87 \t Validation precision@k20: 0.7399, accuracy@k20: 0.7365\n",
      "Epoch: 87 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 87 \t Validation precision@k30: 0.8360, accuracy@k30: 0.8360\n",
      "CPU: 15.29\n",
      "RAM %: 63.5\n",
      "Epoch: 88 \t Training Loss: 3.474335\n",
      "Epoch: 88 \t Validation precision@k5: 0.6937, accuracy@k5: 0.3581\n",
      "Epoch: 88 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 88 \t Validation precision@k15: 0.6794, accuracy@k15: 0.6570\n",
      "Epoch: 88 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7367\n",
      "Epoch: 88 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 88 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 15.34\n",
      "RAM %: 63.3\n",
      "Epoch: 89 \t Training Loss: 3.474263\n",
      "Epoch: 89 \t Validation precision@k5: 0.6939, accuracy@k5: 0.3581\n",
      "Epoch: 89 \t Validation precision@k10: 0.6427, accuracy@k10: 0.5429\n",
      "Epoch: 89 \t Validation precision@k15: 0.6793, accuracy@k15: 0.6569\n",
      "Epoch: 89 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7367\n",
      "Epoch: 89 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 89 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 15.34\n",
      "RAM %: 63.2\n",
      "Epoch: 90 \t Training Loss: 3.474195\n",
      "Epoch: 90 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3581\n",
      "Epoch: 90 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 90 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7368\n",
      "Epoch: 90 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 90 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 15.29\n",
      "RAM %: 62.9\n",
      "Epoch: 91 \t Training Loss: 3.474132\n",
      "Epoch: 91 \t Validation precision@k5: 0.6939, accuracy@k5: 0.3582\n",
      "Epoch: 91 \t Validation precision@k10: 0.6430, accuracy@k10: 0.5431\n",
      "Epoch: 91 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 91 \t Validation precision@k20: 0.7401, accuracy@k20: 0.7368\n",
      "Epoch: 91 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 91 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 15.23\n",
      "RAM %: 62.8\n",
      "Epoch: 92 \t Training Loss: 3.474073\n",
      "Epoch: 92 \t Validation precision@k5: 0.6939, accuracy@k5: 0.3582\n",
      "Epoch: 92 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 92 \t Validation precision@k15: 0.6793, accuracy@k15: 0.6569\n",
      "Epoch: 92 \t Validation precision@k20: 0.7402, accuracy@k20: 0.7368\n",
      "Epoch: 92 \t Validation precision@k25: 0.7925, accuracy@k25: 0.7922\n",
      "Epoch: 92 \t Validation precision@k30: 0.8360, accuracy@k30: 0.8360\n",
      "CPU: 15.21\n",
      "RAM %: 62.7\n",
      "Epoch: 93 \t Training Loss: 3.474019\n",
      "Epoch: 93 \t Validation precision@k5: 0.6938, accuracy@k5: 0.3581\n",
      "Epoch: 93 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 93 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 93 \t Validation precision@k20: 0.7403, accuracy@k20: 0.7369\n",
      "Epoch: 93 \t Validation precision@k25: 0.7925, accuracy@k25: 0.7922\n",
      "Epoch: 93 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 15.41\n",
      "RAM %: 62.7\n",
      "Epoch: 94 \t Training Loss: 3.473967\n",
      "Epoch: 94 \t Validation precision@k5: 0.6933, accuracy@k5: 0.3579\n",
      "Epoch: 94 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 94 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6568\n",
      "Epoch: 94 \t Validation precision@k20: 0.7404, accuracy@k20: 0.7371\n",
      "Epoch: 94 \t Validation precision@k25: 0.7925, accuracy@k25: 0.7922\n",
      "Epoch: 94 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 15.40\n",
      "RAM %: 62.7\n",
      "Epoch: 95 \t Training Loss: 3.473919\n",
      "Epoch: 95 \t Validation precision@k5: 0.6932, accuracy@k5: 0.3578\n",
      "Epoch: 95 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5430\n",
      "Epoch: 95 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6567\n",
      "Epoch: 95 \t Validation precision@k20: 0.7404, accuracy@k20: 0.7370\n",
      "Epoch: 95 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 95 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 15.40\n",
      "RAM %: 62.6\n",
      "Epoch: 96 \t Training Loss: 3.473875\n",
      "Epoch: 96 \t Validation precision@k5: 0.6932, accuracy@k5: 0.3578\n",
      "Epoch: 96 \t Validation precision@k10: 0.6428, accuracy@k10: 0.5429\n",
      "Epoch: 96 \t Validation precision@k15: 0.6790, accuracy@k15: 0.6566\n",
      "Epoch: 96 \t Validation precision@k20: 0.7403, accuracy@k20: 0.7370\n",
      "Epoch: 96 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 96 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 15.38\n",
      "RAM %: 62.6\n",
      "Epoch: 97 \t Training Loss: 3.473833\n",
      "Epoch: 97 \t Validation precision@k5: 0.6931, accuracy@k5: 0.3578\n",
      "Epoch: 97 \t Validation precision@k10: 0.6429, accuracy@k10: 0.5430\n",
      "Epoch: 97 \t Validation precision@k15: 0.6790, accuracy@k15: 0.6566\n",
      "Epoch: 97 \t Validation precision@k20: 0.7403, accuracy@k20: 0.7370\n",
      "Epoch: 97 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 97 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 15.32\n",
      "RAM %: 62.3\n",
      "Epoch: 98 \t Training Loss: 3.473795\n",
      "Epoch: 98 \t Validation precision@k5: 0.6931, accuracy@k5: 0.3578\n",
      "Epoch: 98 \t Validation precision@k10: 0.6429, accuracy@k10: 0.5430\n",
      "Epoch: 98 \t Validation precision@k15: 0.6791, accuracy@k15: 0.6567\n",
      "Epoch: 98 \t Validation precision@k20: 0.7407, accuracy@k20: 0.7373\n",
      "Epoch: 98 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7925\n",
      "Epoch: 98 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 15.30\n",
      "RAM %: 62.2\n",
      "Epoch: 99 \t Training Loss: 3.473759\n",
      "Epoch: 99 \t Validation precision@k5: 0.6931, accuracy@k5: 0.3578\n",
      "Epoch: 99 \t Validation precision@k10: 0.6429, accuracy@k10: 0.5430\n",
      "Epoch: 99 \t Validation precision@k15: 0.6789, accuracy@k15: 0.6564\n",
      "Epoch: 99 \t Validation precision@k20: 0.7407, accuracy@k20: 0.7374\n",
      "Epoch: 99 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 99 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 15.54\n",
      "RAM %: 62.2\n",
      "Epoch: 100 \t Training Loss: 3.473725\n",
      "Epoch: 100 \t Validation precision@k5: 0.6931, accuracy@k5: 0.3578\n",
      "Epoch: 100 \t Validation precision@k10: 0.6426, accuracy@k10: 0.5429\n",
      "Epoch: 100 \t Validation precision@k15: 0.6789, accuracy@k15: 0.6564\n",
      "Epoch: 100 \t Validation precision@k20: 0.7406, accuracy@k20: 0.7373\n",
      "Epoch: 100 \t Validation precision@k25: 0.7928, accuracy@k25: 0.7924\n",
      "Epoch: 100 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 15.54\n",
      "RAM %: 62.2\n",
      "CPU times: user 7min 14s, sys: 1min 17s, total: 8min 32s\n",
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_mlp, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d43a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.7440, accuracy@k5: 0.3867\n",
      "Validation precision@k10: 0.6949, accuracy@k10: 0.5838\n",
      "Validation precision@k15: 0.7231, accuracy@k15: 0.6964\n",
      "Validation precision@k20: 0.7735, accuracy@k20: 0.7691\n",
      "Validation precision@k25: 0.8214, accuracy@k25: 0.8211\n",
      "Validation precision@k30: 0.8596, accuracy@k30: 0.8596\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_mlp, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
