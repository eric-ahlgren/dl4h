{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07324f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1156a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targets = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "prob_targets = pickle.load(open(os.path.join(DATA_PATH,'prob_targets.pkl'), 'rb'))\n",
    "prob_targets_allvisits = pickle.load(open(os.path.join(DATA_PATH,'prob_targets_allvisits.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targets) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef4bc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "206aa1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, prob_targets_allvisits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    num_categories = len(targets[0][0])\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    sum_visits = sum(num_visits)\n",
    "    \n",
    "    x = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.int)\n",
    "    y = torch.zeros((sum_visits - num_patients, num_categories), dtype=torch.float32)\n",
    "    x_masks = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\" \n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             #x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             x_masks[i_patient, j_visit] = 1\n",
    "#     import pdb; pdb.set_trace()\n",
    "    n = 0\n",
    "    for i,patient in enumerate(sequences):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            for k,code in enumerate(visit):\n",
    "                x[n,k] = code\n",
    "                x_masks[n,k] = 1\n",
    "            n+=1\n",
    "    n = 0\n",
    "    for i,patient in enumerate(targets):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            y[n] = torch.tensor(patient[j+1])\n",
    "            n += 1\n",
    "    \n",
    "    \n",
    "    return x, x_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7834769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e400730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21e1a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27668422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d1d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, dim):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros((indices.shape[0], dim), dtype=torch.int)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx.to(torch.int64), multihot.shape[1]).sum(0)\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637c36f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineMLP(\n",
       "  (embedding): Embedding(4903, 128)\n",
       "  (fc): Linear(in_features=128, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineMLP(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        self.padding_idx = 0\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128)\n",
    "        self.fc = nn.Linear(128, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        #import pdb; pdb.set_trace()\n",
    "#         num_codes = self.embedding.weight.shape[0]\n",
    "#         x = indices_to_multihot(x, masks, num_codes)\n",
    "#         x[~masks] = self.padding_idx\n",
    "        x = self.embedding(x)\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        logits = self.fc(x)\n",
    "#         logits = logits.mean(dim=1)\n",
    "        probs = self.softmax(logits)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "baseline_mlp = BaselineMLP(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "375a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(baseline_mlp.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_mlp.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc112d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, y in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            nn = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "            num_labels = y_hat.shape[1]\n",
    "            num_categories = torch.count_nonzero(y, dim=1)\n",
    "            nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(nn):\n",
    "                visit_correct = 0\n",
    "                y_true = nz_cols[nz_rows == i]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "                predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / nn\n",
    "            accuracy_k = total_accuracy / nn\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "#     p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "#     roc_auc = roc_auc_score(y_true, y_score)\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a85ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for x, masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            y_hat = model(x, masks)\n",
    "#             mask_idxs = masks.sum(dim=1) - 1\n",
    "#             y_hat = y_hat[range(len(masks)), mask_idxs]\n",
    "            loss = criterion(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, val_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3097025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1800d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 16.14\n",
      "RAM %: 65.0\n",
      "CPU: 16.20\n",
      "RAM %: 65.0\n",
      "Epoch: 1 \t Training Loss: 3.799349\n",
      "Epoch: 1 \t Validation precision@k5: 0.6240, accuracy@k5: 0.3241\n",
      "Epoch: 1 \t Validation precision@k10: 0.5852, accuracy@k10: 0.4892\n",
      "Epoch: 1 \t Validation precision@k15: 0.6282, accuracy@k15: 0.6010\n",
      "Epoch: 1 \t Validation precision@k20: 0.6881, accuracy@k20: 0.6829\n",
      "Epoch: 1 \t Validation precision@k25: 0.7460, accuracy@k25: 0.7454\n",
      "Epoch: 1 \t Validation precision@k30: 0.7917, accuracy@k30: 0.7917\n",
      "CPU: 16.20\n",
      "RAM %: 65.0\n",
      "Epoch: 2 \t Training Loss: 3.759209\n",
      "Epoch: 2 \t Validation precision@k5: 0.6319, accuracy@k5: 0.3284\n",
      "Epoch: 2 \t Validation precision@k10: 0.5927, accuracy@k10: 0.4956\n",
      "Epoch: 2 \t Validation precision@k15: 0.6349, accuracy@k15: 0.6077\n",
      "Epoch: 2 \t Validation precision@k20: 0.6943, accuracy@k20: 0.6890\n",
      "Epoch: 2 \t Validation precision@k25: 0.7522, accuracy@k25: 0.7516\n",
      "Epoch: 2 \t Validation precision@k30: 0.7968, accuracy@k30: 0.7968\n",
      "CPU: 16.17\n",
      "RAM %: 65.0\n",
      "Epoch: 3 \t Training Loss: 3.734512\n",
      "Epoch: 3 \t Validation precision@k5: 0.6346, accuracy@k5: 0.3296\n",
      "Epoch: 3 \t Validation precision@k10: 0.5970, accuracy@k10: 0.4994\n",
      "Epoch: 3 \t Validation precision@k15: 0.6383, accuracy@k15: 0.6110\n",
      "Epoch: 3 \t Validation precision@k20: 0.6981, accuracy@k20: 0.6927\n",
      "Epoch: 3 \t Validation precision@k25: 0.7546, accuracy@k25: 0.7540\n",
      "Epoch: 3 \t Validation precision@k30: 0.8008, accuracy@k30: 0.8008\n",
      "CPU: 16.15\n",
      "RAM %: 65.0\n",
      "Epoch: 4 \t Training Loss: 3.715664\n",
      "Epoch: 4 \t Validation precision@k5: 0.6378, accuracy@k5: 0.3312\n",
      "Epoch: 4 \t Validation precision@k10: 0.6000, accuracy@k10: 0.5020\n",
      "Epoch: 4 \t Validation precision@k15: 0.6422, accuracy@k15: 0.6147\n",
      "Epoch: 4 \t Validation precision@k20: 0.7012, accuracy@k20: 0.6959\n",
      "Epoch: 4 \t Validation precision@k25: 0.7571, accuracy@k25: 0.7565\n",
      "Epoch: 4 \t Validation precision@k30: 0.8033, accuracy@k30: 0.8033\n",
      "CPU: 16.16\n",
      "RAM %: 65.0\n",
      "Epoch: 5 \t Training Loss: 3.700041\n",
      "Epoch: 5 \t Validation precision@k5: 0.6425, accuracy@k5: 0.3339\n",
      "Epoch: 5 \t Validation precision@k10: 0.6024, accuracy@k10: 0.5041\n",
      "Epoch: 5 \t Validation precision@k15: 0.6460, accuracy@k15: 0.6183\n",
      "Epoch: 5 \t Validation precision@k20: 0.7041, accuracy@k20: 0.6988\n",
      "Epoch: 5 \t Validation precision@k25: 0.7602, accuracy@k25: 0.7596\n",
      "Epoch: 5 \t Validation precision@k30: 0.8050, accuracy@k30: 0.8050\n",
      "CPU: 16.10\n",
      "RAM %: 65.0\n",
      "Epoch: 6 \t Training Loss: 3.686553\n",
      "Epoch: 6 \t Validation precision@k5: 0.6442, accuracy@k5: 0.3348\n",
      "Epoch: 6 \t Validation precision@k10: 0.6055, accuracy@k10: 0.5068\n",
      "Epoch: 6 \t Validation precision@k15: 0.6496, accuracy@k15: 0.6216\n",
      "Epoch: 6 \t Validation precision@k20: 0.7059, accuracy@k20: 0.7006\n",
      "Epoch: 6 \t Validation precision@k25: 0.7610, accuracy@k25: 0.7604\n",
      "Epoch: 6 \t Validation precision@k30: 0.8070, accuracy@k30: 0.8070\n",
      "CPU: 16.19\n",
      "RAM %: 65.0\n",
      "Epoch: 7 \t Training Loss: 3.674617\n",
      "Epoch: 7 \t Validation precision@k5: 0.6467, accuracy@k5: 0.3357\n",
      "Epoch: 7 \t Validation precision@k10: 0.6082, accuracy@k10: 0.5090\n",
      "Epoch: 7 \t Validation precision@k15: 0.6507, accuracy@k15: 0.6227\n",
      "Epoch: 7 \t Validation precision@k20: 0.7087, accuracy@k20: 0.7034\n",
      "Epoch: 7 \t Validation precision@k25: 0.7629, accuracy@k25: 0.7623\n",
      "Epoch: 7 \t Validation precision@k30: 0.8088, accuracy@k30: 0.8088\n",
      "CPU: 16.38\n",
      "RAM %: 65.1\n",
      "Epoch: 8 \t Training Loss: 3.663869\n",
      "Epoch: 8 \t Validation precision@k5: 0.6482, accuracy@k5: 0.3365\n",
      "Epoch: 8 \t Validation precision@k10: 0.6109, accuracy@k10: 0.5113\n",
      "Epoch: 8 \t Validation precision@k15: 0.6523, accuracy@k15: 0.6242\n",
      "Epoch: 8 \t Validation precision@k20: 0.7104, accuracy@k20: 0.7051\n",
      "Epoch: 8 \t Validation precision@k25: 0.7636, accuracy@k25: 0.7630\n",
      "Epoch: 8 \t Validation precision@k30: 0.8100, accuracy@k30: 0.8100\n",
      "CPU: 16.36\n",
      "RAM %: 65.1\n",
      "Epoch: 9 \t Training Loss: 3.654061\n",
      "Epoch: 9 \t Validation precision@k5: 0.6512, accuracy@k5: 0.3375\n",
      "Epoch: 9 \t Validation precision@k10: 0.6118, accuracy@k10: 0.5120\n",
      "Epoch: 9 \t Validation precision@k15: 0.6541, accuracy@k15: 0.6259\n",
      "Epoch: 9 \t Validation precision@k20: 0.7120, accuracy@k20: 0.7066\n",
      "Epoch: 9 \t Validation precision@k25: 0.7654, accuracy@k25: 0.7648\n",
      "Epoch: 9 \t Validation precision@k30: 0.8114, accuracy@k30: 0.8114\n",
      "CPU: 16.30\n",
      "RAM %: 65.1\n",
      "Epoch: 10 \t Training Loss: 3.645010\n",
      "Epoch: 10 \t Validation precision@k5: 0.6539, accuracy@k5: 0.3387\n",
      "Epoch: 10 \t Validation precision@k10: 0.6139, accuracy@k10: 0.5136\n",
      "Epoch: 10 \t Validation precision@k15: 0.6560, accuracy@k15: 0.6278\n",
      "Epoch: 10 \t Validation precision@k20: 0.7125, accuracy@k20: 0.7070\n",
      "Epoch: 10 \t Validation precision@k25: 0.7666, accuracy@k25: 0.7660\n",
      "Epoch: 10 \t Validation precision@k30: 0.8118, accuracy@k30: 0.8118\n",
      "CPU: 16.30\n",
      "RAM %: 65.1\n",
      "Epoch: 11 \t Training Loss: 3.636580\n",
      "Epoch: 11 \t Validation precision@k5: 0.6555, accuracy@k5: 0.3393\n",
      "Epoch: 11 \t Validation precision@k10: 0.6161, accuracy@k10: 0.5153\n",
      "Epoch: 11 \t Validation precision@k15: 0.6575, accuracy@k15: 0.6294\n",
      "Epoch: 11 \t Validation precision@k20: 0.7142, accuracy@k20: 0.7087\n",
      "Epoch: 11 \t Validation precision@k25: 0.7684, accuracy@k25: 0.7678\n",
      "Epoch: 11 \t Validation precision@k30: 0.8128, accuracy@k30: 0.8128\n",
      "CPU: 16.35\n",
      "RAM %: 65.1\n",
      "Epoch: 12 \t Training Loss: 3.628666\n",
      "Epoch: 12 \t Validation precision@k5: 0.6577, accuracy@k5: 0.3400\n",
      "Epoch: 12 \t Validation precision@k10: 0.6180, accuracy@k10: 0.5171\n",
      "Epoch: 12 \t Validation precision@k15: 0.6587, accuracy@k15: 0.6305\n",
      "Epoch: 12 \t Validation precision@k20: 0.7155, accuracy@k20: 0.7101\n",
      "Epoch: 12 \t Validation precision@k25: 0.7697, accuracy@k25: 0.7691\n",
      "Epoch: 12 \t Validation precision@k30: 0.8147, accuracy@k30: 0.8147\n",
      "CPU: 16.32\n",
      "RAM %: 65.1\n",
      "Epoch: 13 \t Training Loss: 3.621184\n",
      "Epoch: 13 \t Validation precision@k5: 0.6584, accuracy@k5: 0.3403\n",
      "Epoch: 13 \t Validation precision@k10: 0.6202, accuracy@k10: 0.5188\n",
      "Epoch: 13 \t Validation precision@k15: 0.6601, accuracy@k15: 0.6318\n",
      "Epoch: 13 \t Validation precision@k20: 0.7162, accuracy@k20: 0.7107\n",
      "Epoch: 13 \t Validation precision@k25: 0.7700, accuracy@k25: 0.7694\n",
      "Epoch: 13 \t Validation precision@k30: 0.8163, accuracy@k30: 0.8163\n",
      "CPU: 16.30\n",
      "RAM %: 65.1\n",
      "Epoch: 14 \t Training Loss: 3.614072\n",
      "Epoch: 14 \t Validation precision@k5: 0.6604, accuracy@k5: 0.3412\n",
      "Epoch: 14 \t Validation precision@k10: 0.6215, accuracy@k10: 0.5199\n",
      "Epoch: 14 \t Validation precision@k15: 0.6618, accuracy@k15: 0.6334\n",
      "Epoch: 14 \t Validation precision@k20: 0.7173, accuracy@k20: 0.7119\n",
      "Epoch: 14 \t Validation precision@k25: 0.7713, accuracy@k25: 0.7707\n",
      "Epoch: 14 \t Validation precision@k30: 0.8173, accuracy@k30: 0.8173\n",
      "CPU: 16.38\n",
      "RAM %: 65.1\n",
      "Epoch: 15 \t Training Loss: 3.607278\n",
      "Epoch: 15 \t Validation precision@k5: 0.6617, accuracy@k5: 0.3421\n",
      "Epoch: 15 \t Validation precision@k10: 0.6229, accuracy@k10: 0.5212\n",
      "Epoch: 15 \t Validation precision@k15: 0.6624, accuracy@k15: 0.6340\n",
      "Epoch: 15 \t Validation precision@k20: 0.7195, accuracy@k20: 0.7140\n",
      "Epoch: 15 \t Validation precision@k25: 0.7724, accuracy@k25: 0.7718\n",
      "Epoch: 15 \t Validation precision@k30: 0.8186, accuracy@k30: 0.8186\n",
      "CPU: 16.36\n",
      "RAM %: 65.1\n",
      "Epoch: 16 \t Training Loss: 3.600767\n",
      "Epoch: 16 \t Validation precision@k5: 0.6639, accuracy@k5: 0.3434\n",
      "Epoch: 16 \t Validation precision@k10: 0.6239, accuracy@k10: 0.5219\n",
      "Epoch: 16 \t Validation precision@k15: 0.6641, accuracy@k15: 0.6355\n",
      "Epoch: 16 \t Validation precision@k20: 0.7200, accuracy@k20: 0.7146\n",
      "Epoch: 16 \t Validation precision@k25: 0.7732, accuracy@k25: 0.7726\n",
      "Epoch: 16 \t Validation precision@k30: 0.8195, accuracy@k30: 0.8195\n",
      "CPU: 16.52\n",
      "RAM %: 65.0\n",
      "Epoch: 17 \t Training Loss: 3.594510\n",
      "Epoch: 17 \t Validation precision@k5: 0.6682, accuracy@k5: 0.3459\n",
      "Epoch: 17 \t Validation precision@k10: 0.6257, accuracy@k10: 0.5234\n",
      "Epoch: 17 \t Validation precision@k15: 0.6655, accuracy@k15: 0.6369\n",
      "Epoch: 17 \t Validation precision@k20: 0.7212, accuracy@k20: 0.7158\n",
      "Epoch: 17 \t Validation precision@k25: 0.7736, accuracy@k25: 0.7730\n",
      "Epoch: 17 \t Validation precision@k30: 0.8202, accuracy@k30: 0.8202\n",
      "CPU: 16.49\n",
      "RAM %: 65.0\n",
      "Epoch: 18 \t Training Loss: 3.588487\n",
      "Epoch: 18 \t Validation precision@k5: 0.6693, accuracy@k5: 0.3461\n",
      "Epoch: 18 \t Validation precision@k10: 0.6281, accuracy@k10: 0.5254\n",
      "Epoch: 18 \t Validation precision@k15: 0.6671, accuracy@k15: 0.6385\n",
      "Epoch: 18 \t Validation precision@k20: 0.7224, accuracy@k20: 0.7169\n",
      "Epoch: 18 \t Validation precision@k25: 0.7746, accuracy@k25: 0.7740\n",
      "Epoch: 18 \t Validation precision@k30: 0.8203, accuracy@k30: 0.8203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 16.72\n",
      "RAM %: 65.0\n",
      "Epoch: 19 \t Training Loss: 3.582683\n",
      "Epoch: 19 \t Validation precision@k5: 0.6704, accuracy@k5: 0.3470\n",
      "Epoch: 19 \t Validation precision@k10: 0.6297, accuracy@k10: 0.5268\n",
      "Epoch: 19 \t Validation precision@k15: 0.6677, accuracy@k15: 0.6391\n",
      "Epoch: 19 \t Validation precision@k20: 0.7241, accuracy@k20: 0.7185\n",
      "Epoch: 19 \t Validation precision@k25: 0.7761, accuracy@k25: 0.7755\n",
      "Epoch: 19 \t Validation precision@k30: 0.8216, accuracy@k30: 0.8216\n",
      "CPU: 16.72\n",
      "RAM %: 65.0\n",
      "Epoch: 20 \t Training Loss: 3.577091\n",
      "Epoch: 20 \t Validation precision@k5: 0.6722, accuracy@k5: 0.3480\n",
      "Epoch: 20 \t Validation precision@k10: 0.6324, accuracy@k10: 0.5292\n",
      "Epoch: 20 \t Validation precision@k15: 0.6701, accuracy@k15: 0.6414\n",
      "Epoch: 20 \t Validation precision@k20: 0.7245, accuracy@k20: 0.7190\n",
      "Epoch: 20 \t Validation precision@k25: 0.7769, accuracy@k25: 0.7763\n",
      "Epoch: 20 \t Validation precision@k30: 0.8236, accuracy@k30: 0.8236\n",
      "CPU: 16.70\n",
      "RAM %: 65.0\n",
      "Epoch: 21 \t Training Loss: 3.571705\n",
      "Epoch: 21 \t Validation precision@k5: 0.6744, accuracy@k5: 0.3493\n",
      "Epoch: 21 \t Validation precision@k10: 0.6344, accuracy@k10: 0.5308\n",
      "Epoch: 21 \t Validation precision@k15: 0.6715, accuracy@k15: 0.6428\n",
      "Epoch: 21 \t Validation precision@k20: 0.7256, accuracy@k20: 0.7200\n",
      "Epoch: 21 \t Validation precision@k25: 0.7782, accuracy@k25: 0.7776\n",
      "Epoch: 21 \t Validation precision@k30: 0.8249, accuracy@k30: 0.8249\n",
      "CPU: 16.67\n",
      "RAM %: 65.0\n",
      "Epoch: 22 \t Training Loss: 3.566521\n",
      "Epoch: 22 \t Validation precision@k5: 0.6751, accuracy@k5: 0.3494\n",
      "Epoch: 22 \t Validation precision@k10: 0.6361, accuracy@k10: 0.5321\n",
      "Epoch: 22 \t Validation precision@k15: 0.6723, accuracy@k15: 0.6435\n",
      "Epoch: 22 \t Validation precision@k20: 0.7267, accuracy@k20: 0.7212\n",
      "Epoch: 22 \t Validation precision@k25: 0.7787, accuracy@k25: 0.7780\n",
      "Epoch: 22 \t Validation precision@k30: 0.8251, accuracy@k30: 0.8251\n",
      "CPU: 16.82\n",
      "RAM %: 65.0\n",
      "Epoch: 23 \t Training Loss: 3.561538\n",
      "Epoch: 23 \t Validation precision@k5: 0.6755, accuracy@k5: 0.3497\n",
      "Epoch: 23 \t Validation precision@k10: 0.6381, accuracy@k10: 0.5336\n",
      "Epoch: 23 \t Validation precision@k15: 0.6741, accuracy@k15: 0.6453\n",
      "Epoch: 23 \t Validation precision@k20: 0.7280, accuracy@k20: 0.7225\n",
      "Epoch: 23 \t Validation precision@k25: 0.7798, accuracy@k25: 0.7792\n",
      "Epoch: 23 \t Validation precision@k30: 0.8259, accuracy@k30: 0.8259\n",
      "CPU: 16.87\n",
      "RAM %: 65.0\n",
      "Epoch: 24 \t Training Loss: 3.556757\n",
      "Epoch: 24 \t Validation precision@k5: 0.6786, accuracy@k5: 0.3517\n",
      "Epoch: 24 \t Validation precision@k10: 0.6401, accuracy@k10: 0.5353\n",
      "Epoch: 24 \t Validation precision@k15: 0.6755, accuracy@k15: 0.6466\n",
      "Epoch: 24 \t Validation precision@k20: 0.7290, accuracy@k20: 0.7234\n",
      "Epoch: 24 \t Validation precision@k25: 0.7816, accuracy@k25: 0.7810\n",
      "Epoch: 24 \t Validation precision@k30: 0.8272, accuracy@k30: 0.8272\n",
      "CPU: 16.84\n",
      "RAM %: 65.1\n",
      "Epoch: 25 \t Training Loss: 3.552178\n",
      "Epoch: 25 \t Validation precision@k5: 0.6791, accuracy@k5: 0.3520\n",
      "Epoch: 25 \t Validation precision@k10: 0.6412, accuracy@k10: 0.5363\n",
      "Epoch: 25 \t Validation precision@k15: 0.6759, accuracy@k15: 0.6469\n",
      "Epoch: 25 \t Validation precision@k20: 0.7299, accuracy@k20: 0.7243\n",
      "Epoch: 25 \t Validation precision@k25: 0.7828, accuracy@k25: 0.7821\n",
      "Epoch: 25 \t Validation precision@k30: 0.8284, accuracy@k30: 0.8284\n",
      "CPU: 16.85\n",
      "RAM %: 65.1\n",
      "Epoch: 26 \t Training Loss: 3.547801\n",
      "Epoch: 26 \t Validation precision@k5: 0.6824, accuracy@k5: 0.3537\n",
      "Epoch: 26 \t Validation precision@k10: 0.6418, accuracy@k10: 0.5367\n",
      "Epoch: 26 \t Validation precision@k15: 0.6773, accuracy@k15: 0.6484\n",
      "Epoch: 26 \t Validation precision@k20: 0.7310, accuracy@k20: 0.7255\n",
      "Epoch: 26 \t Validation precision@k25: 0.7834, accuracy@k25: 0.7827\n",
      "Epoch: 26 \t Validation precision@k30: 0.8289, accuracy@k30: 0.8289\n",
      "CPU: 16.93\n",
      "RAM %: 65.1\n",
      "Epoch: 27 \t Training Loss: 3.543626\n",
      "Epoch: 27 \t Validation precision@k5: 0.6846, accuracy@k5: 0.3549\n",
      "Epoch: 27 \t Validation precision@k10: 0.6433, accuracy@k10: 0.5376\n",
      "Epoch: 27 \t Validation precision@k15: 0.6786, accuracy@k15: 0.6496\n",
      "Epoch: 27 \t Validation precision@k20: 0.7314, accuracy@k20: 0.7259\n",
      "Epoch: 27 \t Validation precision@k25: 0.7841, accuracy@k25: 0.7835\n",
      "Epoch: 27 \t Validation precision@k30: 0.8296, accuracy@k30: 0.8296\n",
      "CPU: 17.13\n",
      "RAM %: 65.1\n",
      "Epoch: 28 \t Training Loss: 3.539653\n",
      "Epoch: 28 \t Validation precision@k5: 0.6858, accuracy@k5: 0.3554\n",
      "Epoch: 28 \t Validation precision@k10: 0.6445, accuracy@k10: 0.5386\n",
      "Epoch: 28 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6501\n",
      "Epoch: 28 \t Validation precision@k20: 0.7317, accuracy@k20: 0.7262\n",
      "Epoch: 28 \t Validation precision@k25: 0.7851, accuracy@k25: 0.7845\n",
      "Epoch: 28 \t Validation precision@k30: 0.8303, accuracy@k30: 0.8303\n",
      "CPU: 17.13\n",
      "RAM %: 65.1\n",
      "Epoch: 29 \t Training Loss: 3.535882\n",
      "Epoch: 29 \t Validation precision@k5: 0.6858, accuracy@k5: 0.3552\n",
      "Epoch: 29 \t Validation precision@k10: 0.6458, accuracy@k10: 0.5397\n",
      "Epoch: 29 \t Validation precision@k15: 0.6795, accuracy@k15: 0.6503\n",
      "Epoch: 29 \t Validation precision@k20: 0.7331, accuracy@k20: 0.7275\n",
      "Epoch: 29 \t Validation precision@k25: 0.7862, accuracy@k25: 0.7856\n",
      "Epoch: 29 \t Validation precision@k30: 0.8302, accuracy@k30: 0.8302\n",
      "CPU: 17.32\n",
      "RAM %: 65.1\n",
      "Epoch: 30 \t Training Loss: 3.532311\n",
      "Epoch: 30 \t Validation precision@k5: 0.6877, accuracy@k5: 0.3564\n",
      "Epoch: 30 \t Validation precision@k10: 0.6472, accuracy@k10: 0.5409\n",
      "Epoch: 30 \t Validation precision@k15: 0.6804, accuracy@k15: 0.6512\n",
      "Epoch: 30 \t Validation precision@k20: 0.7341, accuracy@k20: 0.7285\n",
      "Epoch: 30 \t Validation precision@k25: 0.7877, accuracy@k25: 0.7870\n",
      "Epoch: 30 \t Validation precision@k30: 0.8315, accuracy@k30: 0.8315\n",
      "CPU: 17.29\n",
      "RAM %: 65.1\n",
      "Epoch: 31 \t Training Loss: 3.528938\n",
      "Epoch: 31 \t Validation precision@k5: 0.6884, accuracy@k5: 0.3568\n",
      "Epoch: 31 \t Validation precision@k10: 0.6480, accuracy@k10: 0.5417\n",
      "Epoch: 31 \t Validation precision@k15: 0.6821, accuracy@k15: 0.6527\n",
      "Epoch: 31 \t Validation precision@k20: 0.7340, accuracy@k20: 0.7283\n",
      "Epoch: 31 \t Validation precision@k25: 0.7890, accuracy@k25: 0.7884\n",
      "Epoch: 31 \t Validation precision@k30: 0.8313, accuracy@k30: 0.8313\n",
      "CPU: 17.37\n",
      "RAM %: 65.1\n",
      "Epoch: 32 \t Training Loss: 3.525760\n",
      "Epoch: 32 \t Validation precision@k5: 0.6906, accuracy@k5: 0.3579\n",
      "Epoch: 32 \t Validation precision@k10: 0.6496, accuracy@k10: 0.5430\n",
      "Epoch: 32 \t Validation precision@k15: 0.6830, accuracy@k15: 0.6537\n",
      "Epoch: 32 \t Validation precision@k20: 0.7348, accuracy@k20: 0.7292\n",
      "Epoch: 32 \t Validation precision@k25: 0.7897, accuracy@k25: 0.7890\n",
      "Epoch: 32 \t Validation precision@k30: 0.8323, accuracy@k30: 0.8323\n",
      "CPU: 17.45\n",
      "RAM %: 65.1\n",
      "Epoch: 33 \t Training Loss: 3.522772\n",
      "Epoch: 33 \t Validation precision@k5: 0.6923, accuracy@k5: 0.3586\n",
      "Epoch: 33 \t Validation precision@k10: 0.6509, accuracy@k10: 0.5439\n",
      "Epoch: 33 \t Validation precision@k15: 0.6843, accuracy@k15: 0.6549\n",
      "Epoch: 33 \t Validation precision@k20: 0.7355, accuracy@k20: 0.7299\n",
      "Epoch: 33 \t Validation precision@k25: 0.7903, accuracy@k25: 0.7896\n",
      "Epoch: 33 \t Validation precision@k30: 0.8330, accuracy@k30: 0.8330\n",
      "CPU: 17.42\n",
      "RAM %: 65.2\n",
      "Epoch: 34 \t Training Loss: 3.519970\n",
      "Epoch: 34 \t Validation precision@k5: 0.6931, accuracy@k5: 0.3592\n",
      "Epoch: 34 \t Validation precision@k10: 0.6512, accuracy@k10: 0.5442\n",
      "Epoch: 34 \t Validation precision@k15: 0.6853, accuracy@k15: 0.6557\n",
      "Epoch: 34 \t Validation precision@k20: 0.7358, accuracy@k20: 0.7301\n",
      "Epoch: 34 \t Validation precision@k25: 0.7903, accuracy@k25: 0.7897\n",
      "Epoch: 34 \t Validation precision@k30: 0.8332, accuracy@k30: 0.8332\n",
      "CPU: 17.46\n",
      "RAM %: 65.2\n",
      "Epoch: 35 \t Training Loss: 3.517349\n",
      "Epoch: 35 \t Validation precision@k5: 0.6927, accuracy@k5: 0.3588\n",
      "Epoch: 35 \t Validation precision@k10: 0.6519, accuracy@k10: 0.5448\n",
      "Epoch: 35 \t Validation precision@k15: 0.6862, accuracy@k15: 0.6565\n",
      "Epoch: 35 \t Validation precision@k20: 0.7365, accuracy@k20: 0.7308\n",
      "Epoch: 35 \t Validation precision@k25: 0.7913, accuracy@k25: 0.7906\n",
      "Epoch: 35 \t Validation precision@k30: 0.8335, accuracy@k30: 0.8335\n",
      "CPU: 17.50\n",
      "RAM %: 65.2\n",
      "Epoch: 36 \t Training Loss: 3.514901\n",
      "Epoch: 36 \t Validation precision@k5: 0.6946, accuracy@k5: 0.3597\n",
      "Epoch: 36 \t Validation precision@k10: 0.6525, accuracy@k10: 0.5452\n",
      "Epoch: 36 \t Validation precision@k15: 0.6871, accuracy@k15: 0.6574\n",
      "Epoch: 36 \t Validation precision@k20: 0.7378, accuracy@k20: 0.7321\n",
      "Epoch: 36 \t Validation precision@k25: 0.7924, accuracy@k25: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation precision@k30: 0.8343, accuracy@k30: 0.8343\n",
      "CPU: 17.54\n",
      "RAM %: 65.2\n",
      "Epoch: 37 \t Training Loss: 3.512620\n",
      "Epoch: 37 \t Validation precision@k5: 0.6956, accuracy@k5: 0.3604\n",
      "Epoch: 37 \t Validation precision@k10: 0.6535, accuracy@k10: 0.5461\n",
      "Epoch: 37 \t Validation precision@k15: 0.6872, accuracy@k15: 0.6574\n",
      "Epoch: 37 \t Validation precision@k20: 0.7382, accuracy@k20: 0.7324\n",
      "Epoch: 37 \t Validation precision@k25: 0.7926, accuracy@k25: 0.7920\n",
      "Epoch: 37 \t Validation precision@k30: 0.8346, accuracy@k30: 0.8346\n",
      "CPU: 17.51\n",
      "RAM %: 65.2\n",
      "Epoch: 38 \t Training Loss: 3.510499\n",
      "Epoch: 38 \t Validation precision@k5: 0.6965, accuracy@k5: 0.3610\n",
      "Epoch: 38 \t Validation precision@k10: 0.6541, accuracy@k10: 0.5466\n",
      "Epoch: 38 \t Validation precision@k15: 0.6880, accuracy@k15: 0.6582\n",
      "Epoch: 38 \t Validation precision@k20: 0.7379, accuracy@k20: 0.7322\n",
      "Epoch: 38 \t Validation precision@k25: 0.7933, accuracy@k25: 0.7926\n",
      "Epoch: 38 \t Validation precision@k30: 0.8351, accuracy@k30: 0.8351\n",
      "CPU: 17.51\n",
      "RAM %: 65.2\n",
      "Epoch: 39 \t Training Loss: 3.508530\n",
      "Epoch: 39 \t Validation precision@k5: 0.6964, accuracy@k5: 0.3609\n",
      "Epoch: 39 \t Validation precision@k10: 0.6550, accuracy@k10: 0.5474\n",
      "Epoch: 39 \t Validation precision@k15: 0.6886, accuracy@k15: 0.6589\n",
      "Epoch: 39 \t Validation precision@k20: 0.7391, accuracy@k20: 0.7333\n",
      "Epoch: 39 \t Validation precision@k25: 0.7933, accuracy@k25: 0.7926\n",
      "Epoch: 39 \t Validation precision@k30: 0.8359, accuracy@k30: 0.8359\n",
      "CPU: 17.48\n",
      "RAM %: 65.2\n",
      "Epoch: 40 \t Training Loss: 3.506703\n",
      "Epoch: 40 \t Validation precision@k5: 0.6968, accuracy@k5: 0.3607\n",
      "Epoch: 40 \t Validation precision@k10: 0.6552, accuracy@k10: 0.5476\n",
      "Epoch: 40 \t Validation precision@k15: 0.6895, accuracy@k15: 0.6597\n",
      "Epoch: 40 \t Validation precision@k20: 0.7404, accuracy@k20: 0.7347\n",
      "Epoch: 40 \t Validation precision@k25: 0.7935, accuracy@k25: 0.7928\n",
      "Epoch: 40 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 17.45\n",
      "RAM %: 65.2\n",
      "Epoch: 41 \t Training Loss: 3.505011\n",
      "Epoch: 41 \t Validation precision@k5: 0.6977, accuracy@k5: 0.3614\n",
      "Epoch: 41 \t Validation precision@k10: 0.6570, accuracy@k10: 0.5492\n",
      "Epoch: 41 \t Validation precision@k15: 0.6896, accuracy@k15: 0.6596\n",
      "Epoch: 41 \t Validation precision@k20: 0.7408, accuracy@k20: 0.7351\n",
      "Epoch: 41 \t Validation precision@k25: 0.7936, accuracy@k25: 0.7929\n",
      "Epoch: 41 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 17.39\n",
      "RAM %: 65.1\n",
      "Epoch: 42 \t Training Loss: 3.503445\n",
      "Epoch: 42 \t Validation precision@k5: 0.6989, accuracy@k5: 0.3621\n",
      "Epoch: 42 \t Validation precision@k10: 0.6578, accuracy@k10: 0.5498\n",
      "Epoch: 42 \t Validation precision@k15: 0.6905, accuracy@k15: 0.6605\n",
      "Epoch: 42 \t Validation precision@k20: 0.7408, accuracy@k20: 0.7350\n",
      "Epoch: 42 \t Validation precision@k25: 0.7935, accuracy@k25: 0.7928\n",
      "Epoch: 42 \t Validation precision@k30: 0.8366, accuracy@k30: 0.8366\n",
      "CPU: 17.47\n",
      "RAM %: 65.1\n",
      "Epoch: 43 \t Training Loss: 3.501996\n",
      "Epoch: 43 \t Validation precision@k5: 0.6988, accuracy@k5: 0.3620\n",
      "Epoch: 43 \t Validation precision@k10: 0.6580, accuracy@k10: 0.5499\n",
      "Epoch: 43 \t Validation precision@k15: 0.6906, accuracy@k15: 0.6606\n",
      "Epoch: 43 \t Validation precision@k20: 0.7410, accuracy@k20: 0.7353\n",
      "Epoch: 43 \t Validation precision@k25: 0.7940, accuracy@k25: 0.7933\n",
      "Epoch: 43 \t Validation precision@k30: 0.8363, accuracy@k30: 0.8363\n",
      "CPU: 17.54\n",
      "RAM %: 65.2\n",
      "Epoch: 44 \t Training Loss: 3.500657\n",
      "Epoch: 44 \t Validation precision@k5: 0.6984, accuracy@k5: 0.3622\n",
      "Epoch: 44 \t Validation precision@k10: 0.6590, accuracy@k10: 0.5506\n",
      "Epoch: 44 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6616\n",
      "Epoch: 44 \t Validation precision@k20: 0.7415, accuracy@k20: 0.7358\n",
      "Epoch: 44 \t Validation precision@k25: 0.7947, accuracy@k25: 0.7940\n",
      "Epoch: 44 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 17.51\n",
      "RAM %: 65.2\n",
      "Epoch: 45 \t Training Loss: 3.499419\n",
      "Epoch: 45 \t Validation precision@k5: 0.6996, accuracy@k5: 0.3627\n",
      "Epoch: 45 \t Validation precision@k10: 0.6594, accuracy@k10: 0.5510\n",
      "Epoch: 45 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6612\n",
      "Epoch: 45 \t Validation precision@k20: 0.7416, accuracy@k20: 0.7358\n",
      "Epoch: 45 \t Validation precision@k25: 0.7952, accuracy@k25: 0.7945\n",
      "Epoch: 45 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 17.45\n",
      "RAM %: 65.2\n",
      "Epoch: 46 \t Training Loss: 3.498275\n",
      "Epoch: 46 \t Validation precision@k5: 0.7007, accuracy@k5: 0.3633\n",
      "Epoch: 46 \t Validation precision@k10: 0.6592, accuracy@k10: 0.5507\n",
      "Epoch: 46 \t Validation precision@k15: 0.6910, accuracy@k15: 0.6609\n",
      "Epoch: 46 \t Validation precision@k20: 0.7423, accuracy@k20: 0.7366\n",
      "Epoch: 46 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7950\n",
      "Epoch: 46 \t Validation precision@k30: 0.8379, accuracy@k30: 0.8379\n",
      "CPU: 17.42\n",
      "RAM %: 65.2\n",
      "Epoch: 47 \t Training Loss: 3.497217\n",
      "Epoch: 47 \t Validation precision@k5: 0.7015, accuracy@k5: 0.3638\n",
      "Epoch: 47 \t Validation precision@k10: 0.6588, accuracy@k10: 0.5504\n",
      "Epoch: 47 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6610\n",
      "Epoch: 47 \t Validation precision@k20: 0.7424, accuracy@k20: 0.7367\n",
      "Epoch: 47 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7949\n",
      "Epoch: 47 \t Validation precision@k30: 0.8378, accuracy@k30: 0.8378\n",
      "CPU: 17.42\n",
      "RAM %: 65.2\n",
      "Epoch: 48 \t Training Loss: 3.496239\n",
      "Epoch: 48 \t Validation precision@k5: 0.7018, accuracy@k5: 0.3639\n",
      "Epoch: 48 \t Validation precision@k10: 0.6592, accuracy@k10: 0.5506\n",
      "Epoch: 48 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6610\n",
      "Epoch: 48 \t Validation precision@k20: 0.7429, accuracy@k20: 0.7371\n",
      "Epoch: 48 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7951\n",
      "Epoch: 48 \t Validation precision@k30: 0.8377, accuracy@k30: 0.8377\n",
      "CPU: 17.35\n",
      "RAM %: 65.2\n",
      "Epoch: 49 \t Training Loss: 3.495335\n",
      "Epoch: 49 \t Validation precision@k5: 0.7024, accuracy@k5: 0.3645\n",
      "Epoch: 49 \t Validation precision@k10: 0.6600, accuracy@k10: 0.5514\n",
      "Epoch: 49 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6613\n",
      "Epoch: 49 \t Validation precision@k20: 0.7432, accuracy@k20: 0.7374\n",
      "Epoch: 49 \t Validation precision@k25: 0.7960, accuracy@k25: 0.7953\n",
      "Epoch: 49 \t Validation precision@k30: 0.8376, accuracy@k30: 0.8376\n",
      "CPU: 17.32\n",
      "RAM %: 65.2\n",
      "Epoch: 50 \t Training Loss: 3.494497\n",
      "Epoch: 50 \t Validation precision@k5: 0.7029, accuracy@k5: 0.3646\n",
      "Epoch: 50 \t Validation precision@k10: 0.6607, accuracy@k10: 0.5520\n",
      "Epoch: 50 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6615\n",
      "Epoch: 50 \t Validation precision@k20: 0.7433, accuracy@k20: 0.7375\n",
      "Epoch: 50 \t Validation precision@k25: 0.7954, accuracy@k25: 0.7947\n",
      "Epoch: 50 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 17.32\n",
      "RAM %: 65.3\n",
      "Epoch: 51 \t Training Loss: 3.493721\n",
      "Epoch: 51 \t Validation precision@k5: 0.7024, accuracy@k5: 0.3644\n",
      "Epoch: 51 \t Validation precision@k10: 0.6608, accuracy@k10: 0.5521\n",
      "Epoch: 51 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6614\n",
      "Epoch: 51 \t Validation precision@k20: 0.7433, accuracy@k20: 0.7376\n",
      "Epoch: 51 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7949\n",
      "Epoch: 51 \t Validation precision@k30: 0.8373, accuracy@k30: 0.8373\n",
      "CPU: 17.29\n",
      "RAM %: 65.3\n",
      "Epoch: 52 \t Training Loss: 3.493002\n",
      "Epoch: 52 \t Validation precision@k5: 0.7036, accuracy@k5: 0.3653\n",
      "Epoch: 52 \t Validation precision@k10: 0.6608, accuracy@k10: 0.5522\n",
      "Epoch: 52 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6613\n",
      "Epoch: 52 \t Validation precision@k20: 0.7440, accuracy@k20: 0.7382\n",
      "Epoch: 52 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7952\n",
      "Epoch: 52 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 17.26\n",
      "RAM %: 65.3\n",
      "Epoch: 53 \t Training Loss: 3.492335\n",
      "Epoch: 53 \t Validation precision@k5: 0.7037, accuracy@k5: 0.3654\n",
      "Epoch: 53 \t Validation precision@k10: 0.6607, accuracy@k10: 0.5521\n",
      "Epoch: 53 \t Validation precision@k15: 0.6920, accuracy@k15: 0.6618\n",
      "Epoch: 53 \t Validation precision@k20: 0.7440, accuracy@k20: 0.7382\n",
      "Epoch: 53 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7951\n",
      "Epoch: 53 \t Validation precision@k30: 0.8373, accuracy@k30: 0.8373\n",
      "CPU: 17.20\n",
      "RAM %: 65.3\n",
      "Epoch: 54 \t Training Loss: 3.491716\n",
      "Epoch: 54 \t Validation precision@k5: 0.7038, accuracy@k5: 0.3655\n",
      "Epoch: 54 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5531\n",
      "Epoch: 54 \t Validation precision@k15: 0.6918, accuracy@k15: 0.6616\n",
      "Epoch: 54 \t Validation precision@k20: 0.7445, accuracy@k20: 0.7387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7949\n",
      "Epoch: 54 \t Validation precision@k30: 0.8374, accuracy@k30: 0.8374\n",
      "CPU: 17.13\n",
      "RAM %: 65.3\n",
      "Epoch: 55 \t Training Loss: 3.491141\n",
      "Epoch: 55 \t Validation precision@k5: 0.7037, accuracy@k5: 0.3654\n",
      "Epoch: 55 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5527\n",
      "Epoch: 55 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6612\n",
      "Epoch: 55 \t Validation precision@k20: 0.7446, accuracy@k20: 0.7388\n",
      "Epoch: 55 \t Validation precision@k25: 0.7960, accuracy@k25: 0.7953\n",
      "Epoch: 55 \t Validation precision@k30: 0.8374, accuracy@k30: 0.8374\n",
      "CPU: 17.29\n",
      "RAM %: 65.3\n",
      "Epoch: 56 \t Training Loss: 3.490605\n",
      "Epoch: 56 \t Validation precision@k5: 0.7037, accuracy@k5: 0.3654\n",
      "Epoch: 56 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5529\n",
      "Epoch: 56 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6609\n",
      "Epoch: 56 \t Validation precision@k20: 0.7446, accuracy@k20: 0.7388\n",
      "Epoch: 56 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7955\n",
      "Epoch: 56 \t Validation precision@k30: 0.8377, accuracy@k30: 0.8377\n",
      "CPU: 17.25\n",
      "RAM %: 65.3\n",
      "Epoch: 57 \t Training Loss: 3.490108\n",
      "Epoch: 57 \t Validation precision@k5: 0.7039, accuracy@k5: 0.3655\n",
      "Epoch: 57 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5529\n",
      "Epoch: 57 \t Validation precision@k15: 0.6905, accuracy@k15: 0.6603\n",
      "Epoch: 57 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7400\n",
      "Epoch: 57 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7954\n",
      "Epoch: 57 \t Validation precision@k30: 0.8382, accuracy@k30: 0.8382\n",
      "CPU: 17.25\n",
      "RAM %: 65.3\n",
      "Epoch: 58 \t Training Loss: 3.489644\n",
      "Epoch: 58 \t Validation precision@k5: 0.7047, accuracy@k5: 0.3659\n",
      "Epoch: 58 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5528\n",
      "Epoch: 58 \t Validation precision@k15: 0.6901, accuracy@k15: 0.6600\n",
      "Epoch: 58 \t Validation precision@k20: 0.7460, accuracy@k20: 0.7401\n",
      "Epoch: 58 \t Validation precision@k25: 0.7963, accuracy@k25: 0.7956\n",
      "Epoch: 58 \t Validation precision@k30: 0.8381, accuracy@k30: 0.8381\n",
      "CPU: 17.22\n",
      "RAM %: 65.3\n",
      "Epoch: 59 \t Training Loss: 3.489211\n",
      "Epoch: 59 \t Validation precision@k5: 0.7047, accuracy@k5: 0.3659\n",
      "Epoch: 59 \t Validation precision@k10: 0.6610, accuracy@k10: 0.5523\n",
      "Epoch: 59 \t Validation precision@k15: 0.6902, accuracy@k15: 0.6600\n",
      "Epoch: 59 \t Validation precision@k20: 0.7463, accuracy@k20: 0.7405\n",
      "Epoch: 59 \t Validation precision@k25: 0.7966, accuracy@k25: 0.7959\n",
      "Epoch: 59 \t Validation precision@k30: 0.8381, accuracy@k30: 0.8381\n",
      "CPU: 17.27\n",
      "RAM %: 65.3\n",
      "Epoch: 60 \t Training Loss: 3.488808\n",
      "Epoch: 60 \t Validation precision@k5: 0.7046, accuracy@k5: 0.3658\n",
      "Epoch: 60 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5527\n",
      "Epoch: 60 \t Validation precision@k15: 0.6904, accuracy@k15: 0.6602\n",
      "Epoch: 60 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7405\n",
      "Epoch: 60 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7955\n",
      "Epoch: 60 \t Validation precision@k30: 0.8378, accuracy@k30: 0.8378\n",
      "CPU: 17.38\n",
      "RAM %: 65.3\n",
      "Epoch: 61 \t Training Loss: 3.488432\n",
      "Epoch: 61 \t Validation precision@k5: 0.7045, accuracy@k5: 0.3658\n",
      "Epoch: 61 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5528\n",
      "Epoch: 61 \t Validation precision@k15: 0.6907, accuracy@k15: 0.6604\n",
      "Epoch: 61 \t Validation precision@k20: 0.7463, accuracy@k20: 0.7405\n",
      "Epoch: 61 \t Validation precision@k25: 0.7960, accuracy@k25: 0.7953\n",
      "Epoch: 61 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 17.39\n",
      "RAM %: 65.3\n",
      "Epoch: 62 \t Training Loss: 3.488081\n",
      "Epoch: 62 \t Validation precision@k5: 0.7039, accuracy@k5: 0.3655\n",
      "Epoch: 62 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5527\n",
      "Epoch: 62 \t Validation precision@k15: 0.6908, accuracy@k15: 0.6605\n",
      "Epoch: 62 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7405\n",
      "Epoch: 62 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7952\n",
      "Epoch: 62 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 17.32\n",
      "RAM %: 65.3\n",
      "Epoch: 63 \t Training Loss: 3.487752\n",
      "Epoch: 63 \t Validation precision@k5: 0.7033, accuracy@k5: 0.3651\n",
      "Epoch: 63 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5528\n",
      "Epoch: 63 \t Validation precision@k15: 0.6908, accuracy@k15: 0.6607\n",
      "Epoch: 63 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7408\n",
      "Epoch: 63 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7954\n",
      "Epoch: 63 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 17.29\n",
      "RAM %: 65.3\n",
      "Epoch: 64 \t Training Loss: 3.487445\n",
      "Epoch: 64 \t Validation precision@k5: 0.7029, accuracy@k5: 0.3649\n",
      "Epoch: 64 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5526\n",
      "Epoch: 64 \t Validation precision@k15: 0.6908, accuracy@k15: 0.6606\n",
      "Epoch: 64 \t Validation precision@k20: 0.7465, accuracy@k20: 0.7407\n",
      "Epoch: 64 \t Validation precision@k25: 0.7960, accuracy@k25: 0.7953\n",
      "Epoch: 64 \t Validation precision@k30: 0.8373, accuracy@k30: 0.8373\n",
      "CPU: 17.22\n",
      "RAM %: 65.3\n",
      "Epoch: 65 \t Training Loss: 3.487158\n",
      "Epoch: 65 \t Validation precision@k5: 0.7030, accuracy@k5: 0.3648\n",
      "Epoch: 65 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5528\n",
      "Epoch: 65 \t Validation precision@k15: 0.6906, accuracy@k15: 0.6604\n",
      "Epoch: 65 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7407\n",
      "Epoch: 65 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7951\n",
      "Epoch: 65 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 17.19\n",
      "RAM %: 65.3\n",
      "Epoch: 66 \t Training Loss: 3.486889\n",
      "Epoch: 66 \t Validation precision@k5: 0.7027, accuracy@k5: 0.3647\n",
      "Epoch: 66 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5526\n",
      "Epoch: 66 \t Validation precision@k15: 0.6909, accuracy@k15: 0.6607\n",
      "Epoch: 66 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7406\n",
      "Epoch: 66 \t Validation precision@k25: 0.7959, accuracy@k25: 0.7952\n",
      "Epoch: 66 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 17.27\n",
      "RAM %: 65.3\n",
      "Epoch: 67 \t Training Loss: 3.486637\n",
      "Epoch: 67 \t Validation precision@k5: 0.7026, accuracy@k5: 0.3647\n",
      "Epoch: 67 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5525\n",
      "Epoch: 67 \t Validation precision@k15: 0.6909, accuracy@k15: 0.6607\n",
      "Epoch: 67 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7406\n",
      "Epoch: 67 \t Validation precision@k25: 0.7959, accuracy@k25: 0.7953\n",
      "Epoch: 67 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 17.27\n",
      "RAM %: 65.3\n",
      "Epoch: 68 \t Training Loss: 3.486401\n",
      "Epoch: 68 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3645\n",
      "Epoch: 68 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5525\n",
      "Epoch: 68 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6609\n",
      "Epoch: 68 \t Validation precision@k20: 0.7462, accuracy@k20: 0.7403\n",
      "Epoch: 68 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7954\n",
      "Epoch: 68 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.39\n",
      "RAM %: 65.3\n",
      "Epoch: 69 \t Training Loss: 3.486180\n",
      "Epoch: 69 \t Validation precision@k5: 0.7021, accuracy@k5: 0.3645\n",
      "Epoch: 69 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5528\n",
      "Epoch: 69 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6609\n",
      "Epoch: 69 \t Validation precision@k20: 0.7460, accuracy@k20: 0.7402\n",
      "Epoch: 69 \t Validation precision@k25: 0.7959, accuracy@k25: 0.7953\n",
      "Epoch: 69 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.40\n",
      "RAM %: 65.3\n",
      "Epoch: 70 \t Training Loss: 3.485973\n",
      "Epoch: 70 \t Validation precision@k5: 0.7021, accuracy@k5: 0.3645\n",
      "Epoch: 70 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5529\n",
      "Epoch: 70 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 70 \t Validation precision@k20: 0.7461, accuracy@k20: 0.7403\n",
      "Epoch: 70 \t Validation precision@k25: 0.7957, accuracy@k25: 0.7950\n",
      "Epoch: 70 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 17.40\n",
      "RAM %: 65.3\n",
      "Epoch: 71 \t Training Loss: 3.485779\n",
      "Epoch: 71 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3645\n",
      "Epoch: 71 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5526\n",
      "Epoch: 71 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 71 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7406\n",
      "Epoch: 71 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7949\n",
      "Epoch: 71 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 17.33\n",
      "RAM %: 65.3\n",
      "Epoch: 72 \t Training Loss: 3.485597\n",
      "Epoch: 72 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3647\n",
      "Epoch: 72 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5526\n",
      "Epoch: 72 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7406\n",
      "Epoch: 72 \t Validation precision@k25: 0.7954, accuracy@k25: 0.7948\n",
      "Epoch: 72 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 17.41\n",
      "RAM %: 65.3\n",
      "Epoch: 73 \t Training Loss: 3.485427\n",
      "Epoch: 73 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3647\n",
      "Epoch: 73 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5527\n",
      "Epoch: 73 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6612\n",
      "Epoch: 73 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7407\n",
      "Epoch: 73 \t Validation precision@k25: 0.7955, accuracy@k25: 0.7948\n",
      "Epoch: 73 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 17.46\n",
      "RAM %: 65.3\n",
      "Epoch: 74 \t Training Loss: 3.485266\n",
      "Epoch: 74 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3647\n",
      "Epoch: 74 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5525\n",
      "Epoch: 74 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6610\n",
      "Epoch: 74 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7407\n",
      "Epoch: 74 \t Validation precision@k25: 0.7955, accuracy@k25: 0.7948\n",
      "Epoch: 74 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.39\n",
      "RAM %: 65.3\n",
      "Epoch: 75 \t Training Loss: 3.485116\n",
      "Epoch: 75 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3647\n",
      "Epoch: 75 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5525\n",
      "Epoch: 75 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6613\n",
      "Epoch: 75 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7407\n",
      "Epoch: 75 \t Validation precision@k25: 0.7954, accuracy@k25: 0.7948\n",
      "Epoch: 75 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.47\n",
      "RAM %: 65.3\n",
      "Epoch: 76 \t Training Loss: 3.484976\n",
      "Epoch: 76 \t Validation precision@k5: 0.7023, accuracy@k5: 0.3648\n",
      "Epoch: 76 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5524\n",
      "Epoch: 76 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6614\n",
      "Epoch: 76 \t Validation precision@k20: 0.7467, accuracy@k20: 0.7409\n",
      "Epoch: 76 \t Validation precision@k25: 0.7953, accuracy@k25: 0.7946\n",
      "Epoch: 76 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.40\n",
      "RAM %: 65.3\n",
      "Epoch: 77 \t Training Loss: 3.484843\n",
      "Epoch: 77 \t Validation precision@k5: 0.7026, accuracy@k5: 0.3648\n",
      "Epoch: 77 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5526\n",
      "Epoch: 77 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6615\n",
      "Epoch: 77 \t Validation precision@k20: 0.7461, accuracy@k20: 0.7402\n",
      "Epoch: 77 \t Validation precision@k25: 0.7952, accuracy@k25: 0.7946\n",
      "Epoch: 77 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.33\n",
      "RAM %: 65.3\n",
      "Epoch: 78 \t Training Loss: 3.484720\n",
      "Epoch: 78 \t Validation precision@k5: 0.7026, accuracy@k5: 0.3648\n",
      "Epoch: 78 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5526\n",
      "Epoch: 78 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6615\n",
      "Epoch: 78 \t Validation precision@k20: 0.7460, accuracy@k20: 0.7401\n",
      "Epoch: 78 \t Validation precision@k25: 0.7954, accuracy@k25: 0.7947\n",
      "Epoch: 78 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.49\n",
      "RAM %: 65.4\n",
      "Epoch: 79 \t Training Loss: 3.484603\n",
      "Epoch: 79 \t Validation precision@k5: 0.7026, accuracy@k5: 0.3649\n",
      "Epoch: 79 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5525\n",
      "Epoch: 79 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6614\n",
      "Epoch: 79 \t Validation precision@k20: 0.7459, accuracy@k20: 0.7401\n",
      "Epoch: 79 \t Validation precision@k25: 0.7955, accuracy@k25: 0.7948\n",
      "Epoch: 79 \t Validation precision@k30: 0.8366, accuracy@k30: 0.8366\n",
      "CPU: 17.49\n",
      "RAM %: 65.4\n",
      "Epoch: 80 \t Training Loss: 3.484494\n",
      "Epoch: 80 \t Validation precision@k5: 0.7025, accuracy@k5: 0.3648\n",
      "Epoch: 80 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5524\n",
      "Epoch: 80 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6610\n",
      "Epoch: 80 \t Validation precision@k20: 0.7456, accuracy@k20: 0.7397\n",
      "Epoch: 80 \t Validation precision@k25: 0.7955, accuracy@k25: 0.7949\n",
      "Epoch: 80 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.57\n",
      "RAM %: 65.4\n",
      "Epoch: 81 \t Training Loss: 3.484392\n",
      "Epoch: 81 \t Validation precision@k5: 0.7025, accuracy@k5: 0.3648\n",
      "Epoch: 81 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5525\n",
      "Epoch: 81 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6610\n",
      "Epoch: 81 \t Validation precision@k20: 0.7456, accuracy@k20: 0.7398\n",
      "Epoch: 81 \t Validation precision@k25: 0.7957, accuracy@k25: 0.7951\n",
      "Epoch: 81 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.54\n",
      "RAM %: 65.4\n",
      "Epoch: 82 \t Training Loss: 3.484296\n",
      "Epoch: 82 \t Validation precision@k5: 0.7024, accuracy@k5: 0.3648\n",
      "Epoch: 82 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5526\n",
      "Epoch: 82 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 82 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7399\n",
      "Epoch: 82 \t Validation precision@k25: 0.7957, accuracy@k25: 0.7950\n",
      "Epoch: 82 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.50\n",
      "RAM %: 65.4\n",
      "Epoch: 83 \t Training Loss: 3.484206\n",
      "Epoch: 83 \t Validation precision@k5: 0.7024, accuracy@k5: 0.3648\n",
      "Epoch: 83 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5526\n",
      "Epoch: 83 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 83 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7400\n",
      "Epoch: 83 \t Validation precision@k25: 0.7955, accuracy@k25: 0.7949\n",
      "Epoch: 83 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.44\n",
      "RAM %: 65.4\n",
      "Epoch: 84 \t Training Loss: 3.484122\n",
      "Epoch: 84 \t Validation precision@k5: 0.7023, accuracy@k5: 0.3647\n",
      "Epoch: 84 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5525\n",
      "Epoch: 84 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6611\n",
      "Epoch: 84 \t Validation precision@k20: 0.7459, accuracy@k20: 0.7400\n",
      "Epoch: 84 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7949\n",
      "Epoch: 84 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.44\n",
      "RAM %: 65.4\n",
      "Epoch: 85 \t Training Loss: 3.484043\n",
      "Epoch: 85 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3646\n",
      "Epoch: 85 \t Validation precision@k10: 0.6615, accuracy@k10: 0.5525\n",
      "Epoch: 85 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6611\n",
      "Epoch: 85 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7399\n",
      "Epoch: 85 \t Validation precision@k25: 0.7956, accuracy@k25: 0.7950\n",
      "Epoch: 85 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.38\n",
      "RAM %: 65.4\n",
      "Epoch: 86 \t Training Loss: 3.483969\n",
      "Epoch: 86 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3646\n",
      "Epoch: 86 \t Validation precision@k10: 0.6613, accuracy@k10: 0.5523\n",
      "Epoch: 86 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 86 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7398\n",
      "Epoch: 86 \t Validation precision@k25: 0.7958, accuracy@k25: 0.7951\n",
      "Epoch: 86 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.35\n",
      "RAM %: 65.4\n",
      "Epoch: 87 \t Training Loss: 3.483900\n",
      "Epoch: 87 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3646\n",
      "Epoch: 87 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5524\n",
      "Epoch: 87 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 87 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7399\n",
      "Epoch: 87 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7955\n",
      "Epoch: 87 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.35\n",
      "RAM %: 65.4\n",
      "Epoch: 88 \t Training Loss: 3.483835\n",
      "Epoch: 88 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3646\n",
      "Epoch: 88 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5526\n",
      "Epoch: 88 \t Validation precision@k15: 0.6913, accuracy@k15: 0.6611\n",
      "Epoch: 88 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7400\n",
      "Epoch: 88 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7954\n",
      "Epoch: 88 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.32\n",
      "RAM %: 65.5\n",
      "Epoch: 89 \t Training Loss: 3.483774\n",
      "Epoch: 89 \t Validation precision@k5: 0.7018, accuracy@k5: 0.3646\n",
      "Epoch: 89 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5526\n",
      "Epoch: 89 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6610\n",
      "Epoch: 89 \t Validation precision@k20: 0.7459, accuracy@k20: 0.7400\n",
      "Epoch: 89 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7954\n",
      "Epoch: 89 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 17.51\n",
      "RAM %: 65.5\n",
      "Epoch: 90 \t Training Loss: 3.483718\n",
      "Epoch: 90 \t Validation precision@k5: 0.7018, accuracy@k5: 0.3646\n",
      "Epoch: 90 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6610\n",
      "Epoch: 90 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7399\n",
      "Epoch: 90 \t Validation precision@k25: 0.7961, accuracy@k25: 0.7955\n",
      "Epoch: 90 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.51\n",
      "RAM %: 65.5\n",
      "Epoch: 91 \t Training Loss: 3.483665\n",
      "Epoch: 91 \t Validation precision@k5: 0.7021, accuracy@k5: 0.3647\n",
      "Epoch: 91 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 91 \t Validation precision@k15: 0.6912, accuracy@k15: 0.6610\n",
      "Epoch: 91 \t Validation precision@k20: 0.7456, accuracy@k20: 0.7398\n",
      "Epoch: 91 \t Validation precision@k25: 0.7963, accuracy@k25: 0.7956\n",
      "Epoch: 91 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.48\n",
      "RAM %: 65.5\n",
      "Epoch: 92 \t Training Loss: 3.483615\n",
      "Epoch: 92 \t Validation precision@k5: 0.7022, accuracy@k5: 0.3648\n",
      "Epoch: 92 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5526\n",
      "Epoch: 92 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6612\n",
      "Epoch: 92 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7398\n",
      "Epoch: 92 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7957\n",
      "Epoch: 92 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.49\n",
      "RAM %: 65.5\n",
      "Epoch: 93 \t Training Loss: 3.483568\n",
      "Epoch: 93 \t Validation precision@k5: 0.7021, accuracy@k5: 0.3647\n",
      "Epoch: 93 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5526\n",
      "Epoch: 93 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6613\n",
      "Epoch: 93 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7399\n",
      "Epoch: 93 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7957\n",
      "Epoch: 93 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.46\n",
      "RAM %: 65.5\n",
      "Epoch: 94 \t Training Loss: 3.483525\n",
      "Epoch: 94 \t Validation precision@k5: 0.7021, accuracy@k5: 0.3647\n",
      "Epoch: 94 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5527\n",
      "Epoch: 94 \t Validation precision@k15: 0.6914, accuracy@k15: 0.6612\n",
      "Epoch: 94 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7400\n",
      "Epoch: 94 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7958\n",
      "Epoch: 94 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.65\n",
      "RAM %: 65.5\n",
      "Epoch: 95 \t Training Loss: 3.483484\n",
      "Epoch: 95 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 95 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5527\n",
      "Epoch: 95 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6613\n",
      "Epoch: 95 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7399\n",
      "Epoch: 95 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7957\n",
      "Epoch: 95 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.58\n",
      "RAM %: 65.5\n",
      "Epoch: 96 \t Training Loss: 3.483447\n",
      "Epoch: 96 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 96 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 96 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6614\n",
      "Epoch: 96 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7399\n",
      "Epoch: 96 \t Validation precision@k25: 0.7963, accuracy@k25: 0.7957\n",
      "Epoch: 96 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.58\n",
      "RAM %: 65.5\n",
      "Epoch: 97 \t Training Loss: 3.483411\n",
      "Epoch: 97 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 97 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 97 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6614\n",
      "Epoch: 97 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7399\n",
      "Epoch: 97 \t Validation precision@k25: 0.7963, accuracy@k25: 0.7957\n",
      "Epoch: 97 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 17.77\n",
      "RAM %: 65.5\n",
      "Epoch: 98 \t Training Loss: 3.483378\n",
      "Epoch: 98 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 98 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 98 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6614\n",
      "Epoch: 98 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7398\n",
      "Epoch: 98 \t Validation precision@k25: 0.7962, accuracy@k25: 0.7955\n",
      "Epoch: 98 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.96\n",
      "RAM %: 65.5\n",
      "Epoch: 99 \t Training Loss: 3.483348\n",
      "Epoch: 99 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 99 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 99 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6613\n",
      "Epoch: 99 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7398\n",
      "Epoch: 99 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7958\n",
      "Epoch: 99 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 17.96\n",
      "RAM %: 65.5\n",
      "Epoch: 100 \t Training Loss: 3.483319\n",
      "Epoch: 100 \t Validation precision@k5: 0.7019, accuracy@k5: 0.3647\n",
      "Epoch: 100 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5526\n",
      "Epoch: 100 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6614\n",
      "Epoch: 100 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7398\n",
      "Epoch: 100 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7958\n",
      "Epoch: 100 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 18.11\n",
      "RAM %: 65.5\n",
      "CPU times: user 7min 34s, sys: 1min 31s, total: 9min 6s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_mlp, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d43a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.7461, accuracy@k5: 0.3857\n",
      "Validation precision@k10: 0.6910, accuracy@k10: 0.5794\n",
      "Validation precision@k15: 0.7183, accuracy@k15: 0.6919\n",
      "Validation precision@k20: 0.7696, accuracy@k20: 0.7653\n",
      "Validation precision@k25: 0.8185, accuracy@k25: 0.8183\n",
      "Validation precision@k30: 0.8573, accuracy@k30: 0.8573\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_mlp, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
