{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07324f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# # set seed\n",
    "# seed = 24\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "# os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1156a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targets = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "prob_targets = pickle.load(open(os.path.join(DATA_PATH,'prob_targets.pkl'), 'rb'))\n",
    "prob_targets_allvisits = pickle.load(open(os.path.join(DATA_PATH,'prob_targets_allvisits.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targets) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadc956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.load(os.path.join(DATA_PATH, 'embedding_matrix.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4bc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206aa1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, prob_targets_allvisits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    num_categories = len(targets[0][0])\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    sum_visits = sum(num_visits)\n",
    "    \n",
    "    x = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.int)\n",
    "    y = torch.zeros((sum_visits - num_patients, num_categories), dtype=torch.float32)\n",
    "    x_masks = torch.zeros((sum_visits - num_patients, max_num_codes), dtype=torch.bool)\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\" \n",
    "#             x[i_patient, j_visit] = torch.Tensor(visit)\n",
    "#             #x_masks[i_patient, j_visit] = torch.Tensor(np.ones(num_codes, dtype=int))\n",
    "#             x_masks[i_patient, j_visit] = 1\n",
    "#     import pdb; pdb.set_trace()\n",
    "    n = 0\n",
    "    for i,patient in enumerate(sequences):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            for k,code in enumerate(visit):\n",
    "                x[n,k] = code\n",
    "                x_masks[n,k] = 1\n",
    "            n+=1\n",
    "    n = 0\n",
    "    for i,patient in enumerate(targets):\n",
    "        for j,visit in enumerate(patient):\n",
    "            if j == len(patient) - 1:\n",
    "                break\n",
    "            y[n] = torch.tensor(patient[j+1])\n",
    "            n += 1\n",
    "    \n",
    "    \n",
    "    return x, x_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7834769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e400730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e1a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27668422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Mask select the embeddings for true visits (not padding visits) and then sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = x * masks.unsqueeze(-1)\n",
    "    x = torch.sum(x, dim = -2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4d1d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, dim):\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros((indices.shape[0], dim), dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx.to(torch.int64), multihot.shape[1]).sum(0)\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637c36f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('embedding.weight', Parameter containing:\n",
      "tensor([[1.2068, 0.8850, 0.4353,  ..., 0.0000, 1.4104, 0.0000],\n",
      "        [0.9513, 0.9758, 0.6133,  ..., 0.0000, 0.9793, 1.8127],\n",
      "        [1.8947, 0.1143, 1.5448,  ..., 0.0000, 0.7021, 0.2866],\n",
      "        ...,\n",
      "        [2.5348, 1.3164, 0.0000,  ..., 0.6268, 0.6893, 0.0000],\n",
      "        [0.1616, 1.5199, 0.0000,  ..., 0.0000, 0.0000, 0.9218],\n",
      "        [0.0000, 0.0988, 0.2644,  ..., 2.6060, 0.8468, 0.0000]],\n",
      "       requires_grad=True))\n",
      "('embedding.weight', Parameter containing:\n",
      "tensor([[1.2068, 0.8850, 0.4353,  ..., 0.0000, 1.4104, 0.0000],\n",
      "        [0.9513, 0.9758, 0.6133,  ..., 0.0000, 0.9793, 1.8127],\n",
      "        [1.8947, 0.1143, 1.5448,  ..., 0.0000, 0.7021, 0.2866],\n",
      "        ...,\n",
      "        [2.5348, 1.3164, 0.0000,  ..., 0.6268, 0.6893, 0.0000],\n",
      "        [0.1616, 1.5199, 0.0000,  ..., 0.0000, 0.0000, 0.9218],\n",
      "        [0.0000, 0.0988, 0.2644,  ..., 2.6060, 0.8468, 0.0000]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnhancedMLP(\n",
       "  (embedding): Linear(in_features=4903, out_features=300, bias=True)\n",
       "  (fc): Linear(in_features=300, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories, embedding_matrix):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "#         self.padding_idx = 0\n",
    "        \n",
    "        #self.embedding = nn.Embedding(num_codes, embedding_dim=128, padding_idx=0)\n",
    "        self.embedding = nn.Linear(4903, 300)\n",
    "        self.embedding.weight.data = embedding_matrix\n",
    "        self.fc = nn.Linear(300, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        import pdb; pdb.set_trace()\n",
    "#         num_codes = self.embedding.weight.shape[0]\n",
    "#         x = indices_to_multihot(x, masks, num_codes)\n",
    "#         x[~masks] = self.padding_idx\n",
    "#         x[masks] += 1\n",
    "        x = indices_to_multihot(x, masks, 4903)\n",
    "        x = self.embedding(x)\n",
    "        x = torch.tanh(x)\n",
    "#         x = x.sum(dim=1)\n",
    "        #x = sum_embeddings_with_mask(x, masks)\n",
    "        logits = self.fc(x)\n",
    "#         logits = logits.mean(dim=1)\n",
    "        probs = self.softmax(logits)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "enhanced_mlp = EnhancedMLP(num_codes = len(codes), num_categories=len(sub_categories), embedding_matrix=embedding_matrix)\n",
    "for param in enhanced_mlp.named_parameters():\n",
    "    if param[0] == \"embedding.weight\":\n",
    "        print(param)\n",
    "        param[1].requires_grad = False\n",
    "        print(param)\n",
    "enhanced_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375a279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(baseline_mlp.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(enhanced_mlp.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc112d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, y in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             num_labels = y_hat.shape[1]\n",
    "#             num_categories = torch.count_nonzero(y, dim=1)\n",
    "            nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "#             predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "                y_true = nz_cols[nz_rows == i]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "#                 predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a85ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for x, masks, y in train_loader:\n",
    "\n",
    "            y_hat = model(x, masks)\n",
    "#             mask_idxs = masks.sum(dim=1) - 1\n",
    "#             y_hat = y_hat[range(len(masks)), mask_idxs]\n",
    "            loss = criterion(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3097025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1800d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "%time train(enhanced_mlp, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(enhanced_mlp, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02c32d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
