{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc0918",
   "metadata": {},
   "source": [
    "### Set local paths for input data and output models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ac642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8be68",
   "metadata": {},
   "source": [
    "### Load input data and learned embedding matrix $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "prob_targs = pickle.load(open(os.path.join(DATA_PATH, 'prob_targets_allvisits.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targs) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a00175",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.load(os.path.join(DATA_PATH, 'embedding_matrix.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83ed50",
   "metadata": {},
   "source": [
    "### Define and load custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(seqs, targs)\n",
    "dataset = CustomDataset(seqs, prob_targs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c065fdd5",
   "metadata": {},
   "source": [
    "### Define collate and data loader functions, split dataset to [0.75,0.15,0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        x_masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        y: a tensor of shape (# patiens, max # categories) of type torch.long\n",
    "        y_masks: a tensor of shape (# patiens, max # categories) of type torch.bool\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    max_num_categories = len(targets[0][0])\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.float)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "    \n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "\n",
    "    for i_patient, patient in enumerate(targets):\n",
    "        last_visit = patient[-1]\n",
    "        y[i_patient] = torch.FloatTensor(last_visit)\n",
    "        y_masks[i_patient] = torch.BoolTensor(np.ones(max_num_categories))\n",
    "    \n",
    "    return x, x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    '''\n",
    "    Arguments:\n",
    "        train_dataset: train dataset of type `CustomDataset`\n",
    "        test_dataset: test dataset of type `CustomDataset`\n",
    "        val_dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, test_loader, val_loader: train, test and validation dataloaders\n",
    "    '''\n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa132a",
   "metadata": {},
   "source": [
    "### Define helper functions:\n",
    "- `get_last_visit`\n",
    "- `indices_to_multihot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "    \"\"\"\n",
    "    idx_vector = masks.any(dim=2).sum(1) - 1\n",
    "    p_idx = torch.arange(0,len(hidden_states), dtype=torch.int64)\n",
    "    last_hidden_state = hidden_states[p_idx,idx_vector]\n",
    "    return last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, dim):\n",
    "    mh_size = tuple(indices.shape[:-1]) + (dim,)\n",
    "    multihot = torch.zeros(mh_size, dtype=torch.float)\n",
    "    if len(mh_size) == 3:\n",
    "        for i, patient in enumerate(indices):\n",
    "            for j, visit in enumerate(patient):\n",
    "                if masks[i,j].sum() == 0:\n",
    "                    break\n",
    "                y_idx = visit[masks[i,j]].unique()\n",
    "                multihot[i,j] = F.one_hot(y_idx.to(torch.int64), multihot.shape[-1]).sum(0)\n",
    "    else:\n",
    "        for idx, row in enumerate(indices):\n",
    "            y_idx = row[masks[idx]].unique()\n",
    "            multihot[idx] = F.one_hot(y_idx, dim).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ac90d",
   "metadata": {},
   "source": [
    "### Define EnhancedRNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "            num_categories: total number of higher level categories to predict\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Linear(num_codes, 300)\n",
    "        self.embedding.weight.data = embedding_matrix\n",
    "        self.rnn = nn.GRU(300, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_categories)\n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            logits: logits of shape (batch_size, # categories)\n",
    "        \"\"\"\n",
    "        x = indices_to_multihot(x, masks, 4903)\n",
    "        x = self.embedding(x)\n",
    "        x[~masks.any(dim=2)] = -1e9  # Set masked visits to -1e9\n",
    "        x = torch.tanh(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        logits = self.fc(true_h_n)        \n",
    "        return logits\n",
    "\n",
    "# load the model here\n",
    "enhanced_rnn = EnhancedRNN(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "enhanced_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca6bf6",
   "metadata": {},
   "source": [
    "### Define loss and optimization, and train and evalutaion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(baseline_rnn.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(enhanced_rnn.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the EnhancedRNN model\n",
    "        test_loader: validation dataloader\n",
    "        k: value for top k predictions\n",
    "        n: num of records to evaluate in the batch, value -1 evaulates all records\n",
    "        \n",
    "    Outputs:\n",
    "        precision_k: visit-level precison@k\n",
    "        accuracy_k: code-level accuracy@k\n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, masks, y, y_masks in test_loader:\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "            #y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "            nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "                #y_true = y[i, y_masks[i]]\n",
    "                y_true = nz_cols[nz_rows == i]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the EnhancedRNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"    \n",
    "    max_cpu, max_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, y, y_masks in train_loader:\n",
    "            y_hat = model(x, x_masks)                  \n",
    "            loss = criterion(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        cpu, ram = print_cpu_usage()\n",
    "        max_cpu = cpu if cpu > max_cpu else max_cpu\n",
    "        max_ram = ram if ram > max_ram else max_ram\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, val_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()\n",
    "    print(f\"Max CPU usage: {max_cpu:.3f}\\tMax RAM % usage: {max_ram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f5ca8",
   "metadata": {},
   "source": [
    "### Define helper function to get cpu/RAM usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8e354",
   "metadata": {},
   "source": [
    "### Set num epochs and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "%time train(enhanced_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d476a",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(enhanced_rnn, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d1a82",
   "metadata": {},
   "source": [
    "### Save pre-trained model to `CHECKPOINT_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enhanced_rnn, os.path.join(CHECKPOINT_PATH, \"EnhancedRNN_100.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fef2df5",
   "metadata": {},
   "source": [
    "### Load pre-trained model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988118b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_rnn = torch.load(os.path.join(CHECKPOINT_PATH, \"EnhancedRNN_100.pth\"))\n",
    "\n",
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(enhanced_rnn, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8095e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
