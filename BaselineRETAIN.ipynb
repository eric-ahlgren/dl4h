{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data/\"\n",
    "CHECKPOINT_PATH = \"models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "prob_targs = pickle.load(open(os.path.join(DATA_PATH, 'prob_targets_allvisits.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targs) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CustomDataset(seqs, targs)\n",
    "dataset = CustomDataset(seqs, prob_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d83bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        x_masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time.\n",
    "        rev_masks: same as x_masks but in reversed time.\n",
    "        y: a tensor of shape (# patients, max # diagnosis categories) of type torch.float\n",
    "        y_masks: a tensor of shape (# patients, max # diagnosis categories) of type torch.bool\n",
    "    \"\"\"\n",
    "#     sequences, targets = zip(*data)\n",
    "\n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "#     batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    \n",
    "#     max_num_visits = max(num_visits)\n",
    "#     max_num_codes = max(num_codes)\n",
    "#     max_num_categories = max(batch_num_categories)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "#     rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient[:-1]):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 x[i_patient, j_visit, k_code] = code\n",
    "#                 x_masks[i_patient, j_visit, k_code] = 1\n",
    "#                 if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "#                     rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                     rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                     rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for visit in patient[-1:]:\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, k_code] = code\n",
    "#                 y_masks[i_patient, k_code] = 1\n",
    "    \n",
    "#     return x, x_masks, rev_x, rev_x_masks, y, y_masks\n",
    "\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "#     max_num_categories = max(batch_num_categories)\n",
    "    max_num_categories = len(targets[0][0])\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.float)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "\n",
    "#     for i_patient, patient in enumerate(targets):\n",
    "# #         import pdb; pdb.set_trace()\n",
    "#         last_visit = patient[-1]\n",
    "#         y[i_patient,:len(last_visit)] = torch.LongTensor(last_visit)\n",
    "#         y_masks[i_patient,:len(last_visit)] = torch.BoolTensor(np.ones(len(last_visit)))\n",
    "    for i_patient, patient in enumerate(targets):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        last_visit = patient[-1]\n",
    "        y[i_patient] = torch.FloatTensor(last_visit)\n",
    "        y_masks[i_patient] = torch.BoolTensor(np.ones(max_num_categories))\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    '''\n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        test dataset: test dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, test_loader, val_loader: train, test and validation dataloaders\n",
    "    '''\n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    \"\"\"\n",
    "    x[~masks] = 0\n",
    "    return x.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86967a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, # visits, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, # visits, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "    \"\"\"\n",
    "    rev_masks = rev_masks[:,:].any(dim=2)\n",
    "    rev_v_masked = rev_v.clone()\n",
    "    rev_v_masked[~rev_masks] = 0\n",
    "    weights = alpha * beta\n",
    "    c = torch.sum(weights * rev_v_masked, dim=1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9eecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, # visits, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, # visits, 1)\n",
    "        \"\"\"\n",
    "        m = nn.Softmax(dim=1)\n",
    "        return m(self.a_att(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b55ec0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, # visits, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, # visits, hidden_dim)\n",
    "        \"\"\"\n",
    "        return torch.tanh(self.b_att(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineRETAIN(\n",
       "  (embedding): Embedding(4903, 180)\n",
       "  (rnn_a): GRU(180, 128, batch_first=True)\n",
       "  (rnn_b): GRU(180, 128, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=180, out_features=184, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineRETAIN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "            num_categories: total number of diagnosis categories to predict\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=180)\n",
    "        self.rnn_a = nn.GRU(180, hidden_size=128, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(180, hidden_size=128, batch_first=True)\n",
    "        self.att_a = AlphaAttention(128)\n",
    "        self.att_b = AlphaAttention(128)\n",
    "        self.fc = nn.Linear(180, num_categories)\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "            rev_x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            rev_masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            logits: logits of shape (batch_size, # categories)\n",
    "        \"\"\"\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        logits = self.fc(c)\n",
    "        return logits\n",
    "\n",
    "# load the model here\n",
    "baseline_retain = BaselineRETAIN(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_retain.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the BaselineRETAIN model\n",
    "        train_loader: training dataloder\n",
    "        test_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "    \"\"\"\n",
    "#     max_cpu, max_ram = print_cpu_usage()\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for x, x_masks, rev_x, rev_x_masks, y, y_masks in train_loader:\n",
    "#             y_hat = model(x, x_masks, rev_x, rev_x_masks)\n",
    "#             y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "#             loss = criterion(y_hat, y_mh)\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#         train_loss = train_loss / len(train_loader)\n",
    "#         cpu, ram = print_cpu_usage()\n",
    "#         max_cpu = cpu if cpu > max_cpu else max_cpu\n",
    "#         max_ram = ram if ram > max_ram else max_ram\n",
    "#         print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "#         for k in range(5, 31, 5):\n",
    "#             precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "#             print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "#     final_cpu, final_ram = print_cpu_usage()\n",
    "#     print(f\"Max CPU usage: {max_cpu:.3f}\\tMax RAM % usage: {max_ram}\")\n",
    "    \n",
    "    max_cpu, max_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in train_loader:\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks)                  \n",
    "#             y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "            loss = criterion(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        cpu, ram = print_cpu_usage()\n",
    "        max_cpu = cpu if cpu > max_cpu else max_cpu\n",
    "        max_ram = ram if ram > max_ram else max_ram\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, val_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')\n",
    "    final_cpu, final_ram = print_cpu_usage()\n",
    "    print(f\"Max CPU usage: {max_cpu:.3f}\\tMax RAM % usage: {max_ram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34dfac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the BaselineRETAIN model\n",
    "        test_loader: validation dataloader\n",
    "        k: value for top k predictions\n",
    "        n: num of records to evaluate in the batch, value -1 evaulates all records\n",
    "        \n",
    "    Outputs:\n",
    "        precision_k: visit-level precison@k\n",
    "        accuracy_k: code-level accuracy@k\n",
    "    \"\"\"\n",
    "#     y_pred = torch.LongTensor()\n",
    "#     y_true = torch.LongTensor()\n",
    "#     all_precision = []\n",
    "#     all_accuracy = []\n",
    "    \n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for x, x_masks, rev_x, rev_x_masks, y, y_masks in test_loader:\n",
    "#             n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "#             y_hat = model(x, x_masks, rev_x, rev_x_masks)\n",
    "#             y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "#             k_correct = 0\n",
    "#             total_precision = 0\n",
    "#             total_accuracy = 0\n",
    "#             for i in range(n_eval):\n",
    "#                 visit_correct = 0\n",
    "#                 y_true = y[i, y_masks[i]]\n",
    "#                 _, y_pred = torch.topk(y_hat[i], k)\n",
    "\n",
    "#                 for v in y_true:\n",
    "#                     if v in y_pred:\n",
    "#                         visit_correct += 1\n",
    "                        \n",
    "#                 visit_precision = visit_correct / min(k, len(y_true))\n",
    "#                 visit_accuracy = visit_correct / len(y_true)\n",
    "#                 k_correct += visit_correct\n",
    "#                 total_precision += visit_precision\n",
    "#                 total_accuracy += visit_accuracy\n",
    "\n",
    "#             precision_k = total_precision / n_eval\n",
    "#             accuracy_k = total_accuracy / n_eval\n",
    "#             all_precision.append(precision_k)\n",
    "#             all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#     total_precision_k = np.mean(all_precision)\n",
    "#     total_accuracy_k = np.mean(all_accuracy)\n",
    "#     return total_precision_k, total_accuracy_k\n",
    "\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in test_loader:\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "            #y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "            nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            k_correct = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "                #y_true = y[i, y_masks[i]]\n",
    "                y_true = nz_cols[nz_rows == i]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 11.51\n",
      "RAM %: 60.9\n",
      "CPU: 11.58\n",
      "RAM %: 62.0\n",
      "Epoch: 1 \t Training Loss: 4.580161\n",
      "Epoch: 1 \t Validation precision@k5: 0.5718, accuracy@k5: 0.2898\n",
      "Epoch: 1 \t Validation precision@k10: 0.5313, accuracy@k10: 0.4477\n",
      "Epoch: 1 \t Validation precision@k15: 0.5735, accuracy@k15: 0.5547\n",
      "Epoch: 1 \t Validation precision@k20: 0.6381, accuracy@k20: 0.6355\n",
      "Epoch: 1 \t Validation precision@k25: 0.7035, accuracy@k25: 0.7033\n",
      "Epoch: 1 \t Validation precision@k30: 0.7538, accuracy@k30: 0.7538\n",
      "CPU: 11.73\n",
      "RAM %: 62.0\n",
      "Epoch: 2 \t Training Loss: 3.888857\n",
      "Epoch: 2 \t Validation precision@k5: 0.6273, accuracy@k5: 0.3330\n",
      "Epoch: 2 \t Validation precision@k10: 0.5911, accuracy@k10: 0.5038\n",
      "Epoch: 2 \t Validation precision@k15: 0.6263, accuracy@k15: 0.6071\n",
      "Epoch: 2 \t Validation precision@k20: 0.6881, accuracy@k20: 0.6853\n",
      "Epoch: 2 \t Validation precision@k25: 0.7479, accuracy@k25: 0.7477\n",
      "Epoch: 2 \t Validation precision@k30: 0.7899, accuracy@k30: 0.7899\n",
      "CPU: 11.96\n",
      "RAM %: 62.0\n",
      "Epoch: 3 \t Training Loss: 3.709499\n",
      "Epoch: 3 \t Validation precision@k5: 0.6430, accuracy@k5: 0.3427\n",
      "Epoch: 3 \t Validation precision@k10: 0.6029, accuracy@k10: 0.5145\n",
      "Epoch: 3 \t Validation precision@k15: 0.6413, accuracy@k15: 0.6219\n",
      "Epoch: 3 \t Validation precision@k20: 0.7011, accuracy@k20: 0.6984\n",
      "Epoch: 3 \t Validation precision@k25: 0.7538, accuracy@k25: 0.7536\n",
      "Epoch: 3 \t Validation precision@k30: 0.7994, accuracy@k30: 0.7994\n",
      "CPU: 12.19\n",
      "RAM %: 62.1\n",
      "Epoch: 4 \t Training Loss: 3.631244\n",
      "Epoch: 4 \t Validation precision@k5: 0.6508, accuracy@k5: 0.3475\n",
      "Epoch: 4 \t Validation precision@k10: 0.6043, accuracy@k10: 0.5160\n",
      "Epoch: 4 \t Validation precision@k15: 0.6438, accuracy@k15: 0.6244\n",
      "Epoch: 4 \t Validation precision@k20: 0.7024, accuracy@k20: 0.6996\n",
      "Epoch: 4 \t Validation precision@k25: 0.7602, accuracy@k25: 0.7601\n",
      "Epoch: 4 \t Validation precision@k30: 0.8039, accuracy@k30: 0.8039\n",
      "CPU: 12.37\n",
      "RAM %: 62.1\n",
      "Epoch: 5 \t Training Loss: 3.582899\n",
      "Epoch: 5 \t Validation precision@k5: 0.6546, accuracy@k5: 0.3509\n",
      "Epoch: 5 \t Validation precision@k10: 0.6092, accuracy@k10: 0.5208\n",
      "Epoch: 5 \t Validation precision@k15: 0.6459, accuracy@k15: 0.6263\n",
      "Epoch: 5 \t Validation precision@k20: 0.7037, accuracy@k20: 0.7010\n",
      "Epoch: 5 \t Validation precision@k25: 0.7613, accuracy@k25: 0.7611\n",
      "Epoch: 5 \t Validation precision@k30: 0.8063, accuracy@k30: 0.8063\n",
      "CPU: 12.66\n",
      "RAM %: 62.1\n",
      "Epoch: 6 \t Training Loss: 3.554377\n",
      "Epoch: 6 \t Validation precision@k5: 0.6562, accuracy@k5: 0.3512\n",
      "Epoch: 6 \t Validation precision@k10: 0.6115, accuracy@k10: 0.5221\n",
      "Epoch: 6 \t Validation precision@k15: 0.6477, accuracy@k15: 0.6281\n",
      "Epoch: 6 \t Validation precision@k20: 0.7087, accuracy@k20: 0.7059\n",
      "Epoch: 6 \t Validation precision@k25: 0.7648, accuracy@k25: 0.7647\n",
      "Epoch: 6 \t Validation precision@k30: 0.8089, accuracy@k30: 0.8089\n",
      "CPU: 12.91\n",
      "RAM %: 62.1\n",
      "Epoch: 7 \t Training Loss: 3.530827\n",
      "Epoch: 7 \t Validation precision@k5: 0.6526, accuracy@k5: 0.3499\n",
      "Epoch: 7 \t Validation precision@k10: 0.6111, accuracy@k10: 0.5225\n",
      "Epoch: 7 \t Validation precision@k15: 0.6493, accuracy@k15: 0.6298\n",
      "Epoch: 7 \t Validation precision@k20: 0.7062, accuracy@k20: 0.7035\n",
      "Epoch: 7 \t Validation precision@k25: 0.7638, accuracy@k25: 0.7637\n",
      "Epoch: 7 \t Validation precision@k30: 0.8112, accuracy@k30: 0.8112\n",
      "CPU: 12.94\n",
      "RAM %: 62.1\n",
      "Epoch: 8 \t Training Loss: 3.518504\n",
      "Epoch: 8 \t Validation precision@k5: 0.6592, accuracy@k5: 0.3528\n",
      "Epoch: 8 \t Validation precision@k10: 0.6129, accuracy@k10: 0.5241\n",
      "Epoch: 8 \t Validation precision@k15: 0.6474, accuracy@k15: 0.6278\n",
      "Epoch: 8 \t Validation precision@k20: 0.7074, accuracy@k20: 0.7046\n",
      "Epoch: 8 \t Validation precision@k25: 0.7680, accuracy@k25: 0.7679\n",
      "Epoch: 8 \t Validation precision@k30: 0.8108, accuracy@k30: 0.8108\n",
      "CPU: 13.04\n",
      "RAM %: 62.6\n",
      "Epoch: 9 \t Training Loss: 3.504211\n",
      "Epoch: 9 \t Validation precision@k5: 0.6523, accuracy@k5: 0.3495\n",
      "Epoch: 9 \t Validation precision@k10: 0.6129, accuracy@k10: 0.5239\n",
      "Epoch: 9 \t Validation precision@k15: 0.6496, accuracy@k15: 0.6300\n",
      "Epoch: 9 \t Validation precision@k20: 0.7094, accuracy@k20: 0.7066\n",
      "Epoch: 9 \t Validation precision@k25: 0.7678, accuracy@k25: 0.7677\n",
      "Epoch: 9 \t Validation precision@k30: 0.8139, accuracy@k30: 0.8139\n",
      "CPU: 13.03\n",
      "RAM %: 62.9\n",
      "Epoch: 10 \t Training Loss: 3.492608\n",
      "Epoch: 10 \t Validation precision@k5: 0.6599, accuracy@k5: 0.3545\n",
      "Epoch: 10 \t Validation precision@k10: 0.6148, accuracy@k10: 0.5256\n",
      "Epoch: 10 \t Validation precision@k15: 0.6509, accuracy@k15: 0.6313\n",
      "Epoch: 10 \t Validation precision@k20: 0.7114, accuracy@k20: 0.7086\n",
      "Epoch: 10 \t Validation precision@k25: 0.7659, accuracy@k25: 0.7657\n",
      "Epoch: 10 \t Validation precision@k30: 0.8141, accuracy@k30: 0.8141\n",
      "CPU: 13.49\n",
      "RAM %: 62.8\n",
      "Epoch: 11 \t Training Loss: 3.485289\n",
      "Epoch: 11 \t Validation precision@k5: 0.6595, accuracy@k5: 0.3536\n",
      "Epoch: 11 \t Validation precision@k10: 0.6140, accuracy@k10: 0.5247\n",
      "Epoch: 11 \t Validation precision@k15: 0.6492, accuracy@k15: 0.6298\n",
      "Epoch: 11 \t Validation precision@k20: 0.7090, accuracy@k20: 0.7062\n",
      "Epoch: 11 \t Validation precision@k25: 0.7681, accuracy@k25: 0.7680\n",
      "Epoch: 11 \t Validation precision@k30: 0.8128, accuracy@k30: 0.8128\n",
      "CPU: 13.77\n",
      "RAM %: 62.8\n",
      "Epoch: 12 \t Training Loss: 3.479945\n",
      "Epoch: 12 \t Validation precision@k5: 0.6654, accuracy@k5: 0.3565\n",
      "Epoch: 12 \t Validation precision@k10: 0.6167, accuracy@k10: 0.5271\n",
      "Epoch: 12 \t Validation precision@k15: 0.6543, accuracy@k15: 0.6346\n",
      "Epoch: 12 \t Validation precision@k20: 0.7099, accuracy@k20: 0.7071\n",
      "Epoch: 12 \t Validation precision@k25: 0.7664, accuracy@k25: 0.7663\n",
      "Epoch: 12 \t Validation precision@k30: 0.8128, accuracy@k30: 0.8128\n",
      "CPU: 13.86\n",
      "RAM %: 62.8\n",
      "Epoch: 13 \t Training Loss: 3.474454\n",
      "Epoch: 13 \t Validation precision@k5: 0.6689, accuracy@k5: 0.3591\n",
      "Epoch: 13 \t Validation precision@k10: 0.6208, accuracy@k10: 0.5307\n",
      "Epoch: 13 \t Validation precision@k15: 0.6557, accuracy@k15: 0.6359\n",
      "Epoch: 13 \t Validation precision@k20: 0.7121, accuracy@k20: 0.7093\n",
      "Epoch: 13 \t Validation precision@k25: 0.7720, accuracy@k25: 0.7719\n",
      "Epoch: 13 \t Validation precision@k30: 0.8142, accuracy@k30: 0.8142\n",
      "CPU: 14.02\n",
      "RAM %: 62.8\n",
      "Epoch: 14 \t Training Loss: 3.470588\n",
      "Epoch: 14 \t Validation precision@k5: 0.6663, accuracy@k5: 0.3571\n",
      "Epoch: 14 \t Validation precision@k10: 0.6210, accuracy@k10: 0.5314\n",
      "Epoch: 14 \t Validation precision@k15: 0.6558, accuracy@k15: 0.6362\n",
      "Epoch: 14 \t Validation precision@k20: 0.7134, accuracy@k20: 0.7106\n",
      "Epoch: 14 \t Validation precision@k25: 0.7704, accuracy@k25: 0.7702\n",
      "Epoch: 14 \t Validation precision@k30: 0.8166, accuracy@k30: 0.8166\n",
      "CPU: 14.21\n",
      "RAM %: 62.8\n",
      "Epoch: 15 \t Training Loss: 3.465852\n",
      "Epoch: 15 \t Validation precision@k5: 0.6686, accuracy@k5: 0.3576\n",
      "Epoch: 15 \t Validation precision@k10: 0.6245, accuracy@k10: 0.5342\n",
      "Epoch: 15 \t Validation precision@k15: 0.6578, accuracy@k15: 0.6382\n",
      "Epoch: 15 \t Validation precision@k20: 0.7169, accuracy@k20: 0.7141\n",
      "Epoch: 15 \t Validation precision@k25: 0.7726, accuracy@k25: 0.7724\n",
      "Epoch: 15 \t Validation precision@k30: 0.8164, accuracy@k30: 0.8164\n",
      "CPU: 14.29\n",
      "RAM %: 62.8\n",
      "Epoch: 16 \t Training Loss: 3.460449\n",
      "Epoch: 16 \t Validation precision@k5: 0.6704, accuracy@k5: 0.3580\n",
      "Epoch: 16 \t Validation precision@k10: 0.6266, accuracy@k10: 0.5357\n",
      "Epoch: 16 \t Validation precision@k15: 0.6605, accuracy@k15: 0.6408\n",
      "Epoch: 16 \t Validation precision@k20: 0.7174, accuracy@k20: 0.7146\n",
      "Epoch: 16 \t Validation precision@k25: 0.7748, accuracy@k25: 0.7746\n",
      "Epoch: 16 \t Validation precision@k30: 0.8185, accuracy@k30: 0.8185\n",
      "CPU: 14.45\n",
      "RAM %: 62.8\n",
      "Epoch: 17 \t Training Loss: 3.457882\n",
      "Epoch: 17 \t Validation precision@k5: 0.6742, accuracy@k5: 0.3608\n",
      "Epoch: 17 \t Validation precision@k10: 0.6286, accuracy@k10: 0.5382\n",
      "Epoch: 17 \t Validation precision@k15: 0.6613, accuracy@k15: 0.6416\n",
      "Epoch: 17 \t Validation precision@k20: 0.7195, accuracy@k20: 0.7166\n",
      "Epoch: 17 \t Validation precision@k25: 0.7742, accuracy@k25: 0.7741\n",
      "Epoch: 17 \t Validation precision@k30: 0.8179, accuracy@k30: 0.8179\n",
      "CPU: 14.76\n",
      "RAM %: 62.8\n",
      "Epoch: 18 \t Training Loss: 3.454274\n",
      "Epoch: 18 \t Validation precision@k5: 0.6754, accuracy@k5: 0.3617\n",
      "Epoch: 18 \t Validation precision@k10: 0.6293, accuracy@k10: 0.5383\n",
      "Epoch: 18 \t Validation precision@k15: 0.6623, accuracy@k15: 0.6424\n",
      "Epoch: 18 \t Validation precision@k20: 0.7202, accuracy@k20: 0.7174\n",
      "Epoch: 18 \t Validation precision@k25: 0.7759, accuracy@k25: 0.7757\n",
      "Epoch: 18 \t Validation precision@k30: 0.8191, accuracy@k30: 0.8191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 14.93\n",
      "RAM %: 62.8\n",
      "Epoch: 19 \t Training Loss: 3.450114\n",
      "Epoch: 19 \t Validation precision@k5: 0.6773, accuracy@k5: 0.3631\n",
      "Epoch: 19 \t Validation precision@k10: 0.6312, accuracy@k10: 0.5397\n",
      "Epoch: 19 \t Validation precision@k15: 0.6668, accuracy@k15: 0.6468\n",
      "Epoch: 19 \t Validation precision@k20: 0.7213, accuracy@k20: 0.7185\n",
      "Epoch: 19 \t Validation precision@k25: 0.7775, accuracy@k25: 0.7773\n",
      "Epoch: 19 \t Validation precision@k30: 0.8208, accuracy@k30: 0.8208\n",
      "CPU: 15.08\n",
      "RAM %: 62.8\n",
      "Epoch: 20 \t Training Loss: 3.448546\n",
      "Epoch: 20 \t Validation precision@k5: 0.6807, accuracy@k5: 0.3641\n",
      "Epoch: 20 \t Validation precision@k10: 0.6323, accuracy@k10: 0.5405\n",
      "Epoch: 20 \t Validation precision@k15: 0.6669, accuracy@k15: 0.6470\n",
      "Epoch: 20 \t Validation precision@k20: 0.7225, accuracy@k20: 0.7197\n",
      "Epoch: 20 \t Validation precision@k25: 0.7789, accuracy@k25: 0.7787\n",
      "Epoch: 20 \t Validation precision@k30: 0.8212, accuracy@k30: 0.8212\n",
      "CPU: 15.38\n",
      "RAM %: 62.8\n",
      "Epoch: 21 \t Training Loss: 3.445582\n",
      "Epoch: 21 \t Validation precision@k5: 0.6799, accuracy@k5: 0.3653\n",
      "Epoch: 21 \t Validation precision@k10: 0.6326, accuracy@k10: 0.5405\n",
      "Epoch: 21 \t Validation precision@k15: 0.6679, accuracy@k15: 0.6481\n",
      "Epoch: 21 \t Validation precision@k20: 0.7242, accuracy@k20: 0.7213\n",
      "Epoch: 21 \t Validation precision@k25: 0.7823, accuracy@k25: 0.7821\n",
      "Epoch: 21 \t Validation precision@k30: 0.8233, accuracy@k30: 0.8233\n",
      "CPU: 15.53\n",
      "RAM %: 62.8\n",
      "Epoch: 22 \t Training Loss: 3.443301\n",
      "Epoch: 22 \t Validation precision@k5: 0.6846, accuracy@k5: 0.3688\n",
      "Epoch: 22 \t Validation precision@k10: 0.6329, accuracy@k10: 0.5406\n",
      "Epoch: 22 \t Validation precision@k15: 0.6682, accuracy@k15: 0.6485\n",
      "Epoch: 22 \t Validation precision@k20: 0.7259, accuracy@k20: 0.7230\n",
      "Epoch: 22 \t Validation precision@k25: 0.7806, accuracy@k25: 0.7804\n",
      "Epoch: 22 \t Validation precision@k30: 0.8244, accuracy@k30: 0.8244\n",
      "CPU: 15.80\n",
      "RAM %: 62.8\n",
      "Epoch: 23 \t Training Loss: 3.441833\n",
      "Epoch: 23 \t Validation precision@k5: 0.6747, accuracy@k5: 0.3634\n",
      "Epoch: 23 \t Validation precision@k10: 0.6338, accuracy@k10: 0.5416\n",
      "Epoch: 23 \t Validation precision@k15: 0.6687, accuracy@k15: 0.6487\n",
      "Epoch: 23 \t Validation precision@k20: 0.7256, accuracy@k20: 0.7228\n",
      "Epoch: 23 \t Validation precision@k25: 0.7814, accuracy@k25: 0.7812\n",
      "Epoch: 23 \t Validation precision@k30: 0.8256, accuracy@k30: 0.8256\n",
      "CPU: 16.09\n",
      "RAM %: 62.8\n",
      "Epoch: 24 \t Training Loss: 3.436662\n",
      "Epoch: 24 \t Validation precision@k5: 0.6897, accuracy@k5: 0.3706\n",
      "Epoch: 24 \t Validation precision@k10: 0.6394, accuracy@k10: 0.5457\n",
      "Epoch: 24 \t Validation precision@k15: 0.6725, accuracy@k15: 0.6525\n",
      "Epoch: 24 \t Validation precision@k20: 0.7304, accuracy@k20: 0.7275\n",
      "Epoch: 24 \t Validation precision@k25: 0.7824, accuracy@k25: 0.7823\n",
      "Epoch: 24 \t Validation precision@k30: 0.8265, accuracy@k30: 0.8265\n",
      "CPU: 16.41\n",
      "RAM %: 62.8\n",
      "Epoch: 25 \t Training Loss: 3.434281\n",
      "Epoch: 25 \t Validation precision@k5: 0.6868, accuracy@k5: 0.3686\n",
      "Epoch: 25 \t Validation precision@k10: 0.6386, accuracy@k10: 0.5448\n",
      "Epoch: 25 \t Validation precision@k15: 0.6729, accuracy@k15: 0.6529\n",
      "Epoch: 25 \t Validation precision@k20: 0.7290, accuracy@k20: 0.7262\n",
      "Epoch: 25 \t Validation precision@k25: 0.7842, accuracy@k25: 0.7840\n",
      "Epoch: 25 \t Validation precision@k30: 0.8266, accuracy@k30: 0.8266\n",
      "CPU: 16.61\n",
      "RAM %: 62.8\n",
      "Epoch: 26 \t Training Loss: 3.432262\n",
      "Epoch: 26 \t Validation precision@k5: 0.6880, accuracy@k5: 0.3689\n",
      "Epoch: 26 \t Validation precision@k10: 0.6403, accuracy@k10: 0.5463\n",
      "Epoch: 26 \t Validation precision@k15: 0.6728, accuracy@k15: 0.6528\n",
      "Epoch: 26 \t Validation precision@k20: 0.7299, accuracy@k20: 0.7271\n",
      "Epoch: 26 \t Validation precision@k25: 0.7861, accuracy@k25: 0.7859\n",
      "Epoch: 26 \t Validation precision@k30: 0.8284, accuracy@k30: 0.8284\n",
      "CPU: 17.01\n",
      "RAM %: 62.8\n",
      "Epoch: 27 \t Training Loss: 3.429533\n",
      "Epoch: 27 \t Validation precision@k5: 0.6889, accuracy@k5: 0.3701\n",
      "Epoch: 27 \t Validation precision@k10: 0.6418, accuracy@k10: 0.5476\n",
      "Epoch: 27 \t Validation precision@k15: 0.6792, accuracy@k15: 0.6589\n",
      "Epoch: 27 \t Validation precision@k20: 0.7338, accuracy@k20: 0.7309\n",
      "Epoch: 27 \t Validation precision@k25: 0.7877, accuracy@k25: 0.7875\n",
      "Epoch: 27 \t Validation precision@k30: 0.8283, accuracy@k30: 0.8283\n",
      "CPU: 17.21\n",
      "RAM %: 62.8\n",
      "Epoch: 28 \t Training Loss: 3.427554\n",
      "Epoch: 28 \t Validation precision@k5: 0.6970, accuracy@k5: 0.3728\n",
      "Epoch: 28 \t Validation precision@k10: 0.6442, accuracy@k10: 0.5496\n",
      "Epoch: 28 \t Validation precision@k15: 0.6784, accuracy@k15: 0.6581\n",
      "Epoch: 28 \t Validation precision@k20: 0.7363, accuracy@k20: 0.7333\n",
      "Epoch: 28 \t Validation precision@k25: 0.7878, accuracy@k25: 0.7877\n",
      "Epoch: 28 \t Validation precision@k30: 0.8297, accuracy@k30: 0.8297\n",
      "CPU: 17.15\n",
      "RAM %: 62.8\n",
      "Epoch: 29 \t Training Loss: 3.424517\n",
      "Epoch: 29 \t Validation precision@k5: 0.7039, accuracy@k5: 0.3770\n",
      "Epoch: 29 \t Validation precision@k10: 0.6458, accuracy@k10: 0.5503\n",
      "Epoch: 29 \t Validation precision@k15: 0.6802, accuracy@k15: 0.6599\n",
      "Epoch: 29 \t Validation precision@k20: 0.7342, accuracy@k20: 0.7313\n",
      "Epoch: 29 \t Validation precision@k25: 0.7889, accuracy@k25: 0.7888\n",
      "Epoch: 29 \t Validation precision@k30: 0.8313, accuracy@k30: 0.8313\n",
      "CPU: 17.25\n",
      "RAM %: 62.8\n",
      "Epoch: 30 \t Training Loss: 3.422735\n",
      "Epoch: 30 \t Validation precision@k5: 0.6935, accuracy@k5: 0.3716\n",
      "Epoch: 30 \t Validation precision@k10: 0.6469, accuracy@k10: 0.5520\n",
      "Epoch: 30 \t Validation precision@k15: 0.6808, accuracy@k15: 0.6605\n",
      "Epoch: 30 \t Validation precision@k20: 0.7382, accuracy@k20: 0.7352\n",
      "Epoch: 30 \t Validation precision@k25: 0.7893, accuracy@k25: 0.7891\n",
      "Epoch: 30 \t Validation precision@k30: 0.8303, accuracy@k30: 0.8303\n",
      "CPU: 17.37\n",
      "RAM %: 62.8\n",
      "Epoch: 31 \t Training Loss: 3.419630\n",
      "Epoch: 31 \t Validation precision@k5: 0.6978, accuracy@k5: 0.3743\n",
      "Epoch: 31 \t Validation precision@k10: 0.6474, accuracy@k10: 0.5522\n",
      "Epoch: 31 \t Validation precision@k15: 0.6807, accuracy@k15: 0.6603\n",
      "Epoch: 31 \t Validation precision@k20: 0.7392, accuracy@k20: 0.7362\n",
      "Epoch: 31 \t Validation precision@k25: 0.7915, accuracy@k25: 0.7914\n",
      "Epoch: 31 \t Validation precision@k30: 0.8321, accuracy@k30: 0.8321\n",
      "CPU: 17.42\n",
      "RAM %: 62.8\n",
      "Epoch: 32 \t Training Loss: 3.417010\n",
      "Epoch: 32 \t Validation precision@k5: 0.7025, accuracy@k5: 0.3764\n",
      "Epoch: 32 \t Validation precision@k10: 0.6513, accuracy@k10: 0.5554\n",
      "Epoch: 32 \t Validation precision@k15: 0.6838, accuracy@k15: 0.6633\n",
      "Epoch: 32 \t Validation precision@k20: 0.7372, accuracy@k20: 0.7342\n",
      "Epoch: 32 \t Validation precision@k25: 0.7913, accuracy@k25: 0.7911\n",
      "Epoch: 32 \t Validation precision@k30: 0.8328, accuracy@k30: 0.8328\n",
      "CPU: 17.69\n",
      "RAM %: 62.8\n",
      "Epoch: 33 \t Training Loss: 3.415994\n",
      "Epoch: 33 \t Validation precision@k5: 0.6974, accuracy@k5: 0.3729\n",
      "Epoch: 33 \t Validation precision@k10: 0.6516, accuracy@k10: 0.5554\n",
      "Epoch: 33 \t Validation precision@k15: 0.6847, accuracy@k15: 0.6641\n",
      "Epoch: 33 \t Validation precision@k20: 0.7391, accuracy@k20: 0.7361\n",
      "Epoch: 33 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7937\n",
      "Epoch: 33 \t Validation precision@k30: 0.8326, accuracy@k30: 0.8326\n",
      "CPU: 17.96\n",
      "RAM %: 62.7\n",
      "Epoch: 34 \t Training Loss: 3.412883\n",
      "Epoch: 34 \t Validation precision@k5: 0.7010, accuracy@k5: 0.3750\n",
      "Epoch: 34 \t Validation precision@k10: 0.6517, accuracy@k10: 0.5558\n",
      "Epoch: 34 \t Validation precision@k15: 0.6845, accuracy@k15: 0.6639\n",
      "Epoch: 34 \t Validation precision@k20: 0.7408, accuracy@k20: 0.7378\n",
      "Epoch: 34 \t Validation precision@k25: 0.7927, accuracy@k25: 0.7926\n",
      "Epoch: 34 \t Validation precision@k30: 0.8337, accuracy@k30: 0.8337\n",
      "CPU: 17.96\n",
      "RAM %: 62.8\n",
      "Epoch: 35 \t Training Loss: 3.411801\n",
      "Epoch: 35 \t Validation precision@k5: 0.6939, accuracy@k5: 0.3706\n",
      "Epoch: 35 \t Validation precision@k10: 0.6539, accuracy@k10: 0.5571\n",
      "Epoch: 35 \t Validation precision@k15: 0.6839, accuracy@k15: 0.6632\n",
      "Epoch: 35 \t Validation precision@k20: 0.7404, accuracy@k20: 0.7374\n",
      "Epoch: 35 \t Validation precision@k25: 0.7935, accuracy@k25: 0.7934\n",
      "Epoch: 35 \t Validation precision@k30: 0.8345, accuracy@k30: 0.8345\n",
      "CPU: 18.26\n",
      "RAM %: 62.8\n",
      "Epoch: 36 \t Training Loss: 3.409263\n",
      "Epoch: 36 \t Validation precision@k5: 0.6999, accuracy@k5: 0.3743\n",
      "Epoch: 36 \t Validation precision@k10: 0.6559, accuracy@k10: 0.5596\n",
      "Epoch: 36 \t Validation precision@k15: 0.6848, accuracy@k15: 0.6640\n",
      "Epoch: 36 \t Validation precision@k20: 0.7426, accuracy@k20: 0.7395\n",
      "Epoch: 36 \t Validation precision@k25: 0.7934, accuracy@k25: 0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 \t Validation precision@k30: 0.8331, accuracy@k30: 0.8331\n",
      "CPU: 18.23\n",
      "RAM %: 62.8\n",
      "Epoch: 37 \t Training Loss: 3.408657\n",
      "Epoch: 37 \t Validation precision@k5: 0.6978, accuracy@k5: 0.3728\n",
      "Epoch: 37 \t Validation precision@k10: 0.6573, accuracy@k10: 0.5598\n",
      "Epoch: 37 \t Validation precision@k15: 0.6840, accuracy@k15: 0.6632\n",
      "Epoch: 37 \t Validation precision@k20: 0.7419, accuracy@k20: 0.7389\n",
      "Epoch: 37 \t Validation precision@k25: 0.7959, accuracy@k25: 0.7957\n",
      "Epoch: 37 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 18.45\n",
      "RAM %: 62.8\n",
      "Epoch: 38 \t Training Loss: 3.406201\n",
      "Epoch: 38 \t Validation precision@k5: 0.7033, accuracy@k5: 0.3756\n",
      "Epoch: 38 \t Validation precision@k10: 0.6595, accuracy@k10: 0.5624\n",
      "Epoch: 38 \t Validation precision@k15: 0.6899, accuracy@k15: 0.6688\n",
      "Epoch: 38 \t Validation precision@k20: 0.7431, accuracy@k20: 0.7401\n",
      "Epoch: 38 \t Validation precision@k25: 0.7964, accuracy@k25: 0.7962\n",
      "Epoch: 38 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 18.63\n",
      "RAM %: 62.8\n",
      "Epoch: 39 \t Training Loss: 3.405234\n",
      "Epoch: 39 \t Validation precision@k5: 0.6976, accuracy@k5: 0.3731\n",
      "Epoch: 39 \t Validation precision@k10: 0.6547, accuracy@k10: 0.5585\n",
      "Epoch: 39 \t Validation precision@k15: 0.6874, accuracy@k15: 0.6664\n",
      "Epoch: 39 \t Validation precision@k20: 0.7409, accuracy@k20: 0.7379\n",
      "Epoch: 39 \t Validation precision@k25: 0.7939, accuracy@k25: 0.7938\n",
      "Epoch: 39 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 18.67\n",
      "RAM %: 62.8\n",
      "Epoch: 40 \t Training Loss: 3.403125\n",
      "Epoch: 40 \t Validation precision@k5: 0.7045, accuracy@k5: 0.3762\n",
      "Epoch: 40 \t Validation precision@k10: 0.6617, accuracy@k10: 0.5641\n",
      "Epoch: 40 \t Validation precision@k15: 0.6888, accuracy@k15: 0.6678\n",
      "Epoch: 40 \t Validation precision@k20: 0.7429, accuracy@k20: 0.7399\n",
      "Epoch: 40 \t Validation precision@k25: 0.7967, accuracy@k25: 0.7965\n",
      "Epoch: 40 \t Validation precision@k30: 0.8381, accuracy@k30: 0.8381\n",
      "CPU: 18.99\n",
      "RAM %: 62.8\n",
      "Epoch: 41 \t Training Loss: 3.401331\n",
      "Epoch: 41 \t Validation precision@k5: 0.7064, accuracy@k5: 0.3784\n",
      "Epoch: 41 \t Validation precision@k10: 0.6625, accuracy@k10: 0.5646\n",
      "Epoch: 41 \t Validation precision@k15: 0.6893, accuracy@k15: 0.6680\n",
      "Epoch: 41 \t Validation precision@k20: 0.7434, accuracy@k20: 0.7403\n",
      "Epoch: 41 \t Validation precision@k25: 0.7978, accuracy@k25: 0.7976\n",
      "Epoch: 41 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 19.24\n",
      "RAM %: 62.8\n",
      "Epoch: 42 \t Training Loss: 3.399338\n",
      "Epoch: 42 \t Validation precision@k5: 0.7069, accuracy@k5: 0.3777\n",
      "Epoch: 42 \t Validation precision@k10: 0.6616, accuracy@k10: 0.5643\n",
      "Epoch: 42 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6706\n",
      "Epoch: 42 \t Validation precision@k20: 0.7447, accuracy@k20: 0.7417\n",
      "Epoch: 42 \t Validation precision@k25: 0.7965, accuracy@k25: 0.7964\n",
      "Epoch: 42 \t Validation precision@k30: 0.8354, accuracy@k30: 0.8354\n",
      "CPU: 19.49\n",
      "RAM %: 62.8\n",
      "Epoch: 43 \t Training Loss: 3.399720\n",
      "Epoch: 43 \t Validation precision@k5: 0.7102, accuracy@k5: 0.3789\n",
      "Epoch: 43 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5638\n",
      "Epoch: 43 \t Validation precision@k15: 0.6880, accuracy@k15: 0.6671\n",
      "Epoch: 43 \t Validation precision@k20: 0.7431, accuracy@k20: 0.7401\n",
      "Epoch: 43 \t Validation precision@k25: 0.7965, accuracy@k25: 0.7963\n",
      "Epoch: 43 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 19.77\n",
      "RAM %: 62.8\n",
      "Epoch: 44 \t Training Loss: 3.399234\n",
      "Epoch: 44 \t Validation precision@k5: 0.7027, accuracy@k5: 0.3754\n",
      "Epoch: 44 \t Validation precision@k10: 0.6594, accuracy@k10: 0.5618\n",
      "Epoch: 44 \t Validation precision@k15: 0.6881, accuracy@k15: 0.6669\n",
      "Epoch: 44 \t Validation precision@k20: 0.7413, accuracy@k20: 0.7382\n",
      "Epoch: 44 \t Validation precision@k25: 0.7971, accuracy@k25: 0.7970\n",
      "Epoch: 44 \t Validation precision@k30: 0.8361, accuracy@k30: 0.8361\n",
      "CPU: 19.90\n",
      "RAM %: 62.8\n",
      "Epoch: 45 \t Training Loss: 3.397487\n",
      "Epoch: 45 \t Validation precision@k5: 0.7081, accuracy@k5: 0.3786\n",
      "Epoch: 45 \t Validation precision@k10: 0.6611, accuracy@k10: 0.5630\n",
      "Epoch: 45 \t Validation precision@k15: 0.6901, accuracy@k15: 0.6688\n",
      "Epoch: 45 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7426\n",
      "Epoch: 45 \t Validation precision@k25: 0.7989, accuracy@k25: 0.7987\n",
      "Epoch: 45 \t Validation precision@k30: 0.8377, accuracy@k30: 0.8377\n",
      "CPU: 19.99\n",
      "RAM %: 62.8\n",
      "Epoch: 46 \t Training Loss: 3.396936\n",
      "Epoch: 46 \t Validation precision@k5: 0.7058, accuracy@k5: 0.3769\n",
      "Epoch: 46 \t Validation precision@k10: 0.6620, accuracy@k10: 0.5644\n",
      "Epoch: 46 \t Validation precision@k15: 0.6893, accuracy@k15: 0.6681\n",
      "Epoch: 46 \t Validation precision@k20: 0.7436, accuracy@k20: 0.7405\n",
      "Epoch: 46 \t Validation precision@k25: 0.7990, accuracy@k25: 0.7989\n",
      "Epoch: 46 \t Validation precision@k30: 0.8379, accuracy@k30: 0.8379\n",
      "CPU: 20.12\n",
      "RAM %: 62.8\n",
      "Epoch: 47 \t Training Loss: 3.395455\n",
      "Epoch: 47 \t Validation precision@k5: 0.7042, accuracy@k5: 0.3759\n",
      "Epoch: 47 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5640\n",
      "Epoch: 47 \t Validation precision@k15: 0.6898, accuracy@k15: 0.6687\n",
      "Epoch: 47 \t Validation precision@k20: 0.7469, accuracy@k20: 0.7438\n",
      "Epoch: 47 \t Validation precision@k25: 0.7995, accuracy@k25: 0.7993\n",
      "Epoch: 47 \t Validation precision@k30: 0.8379, accuracy@k30: 0.8379\n",
      "CPU: 20.31\n",
      "RAM %: 62.8\n",
      "Epoch: 48 \t Training Loss: 3.395526\n",
      "Epoch: 48 \t Validation precision@k5: 0.7058, accuracy@k5: 0.3770\n",
      "Epoch: 48 \t Validation precision@k10: 0.6637, accuracy@k10: 0.5652\n",
      "Epoch: 48 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6696\n",
      "Epoch: 48 \t Validation precision@k20: 0.7437, accuracy@k20: 0.7406\n",
      "Epoch: 48 \t Validation precision@k25: 0.7984, accuracy@k25: 0.7983\n",
      "Epoch: 48 \t Validation precision@k30: 0.8380, accuracy@k30: 0.8380\n",
      "CPU: 20.55\n",
      "RAM %: 62.8\n",
      "Epoch: 49 \t Training Loss: 3.393873\n",
      "Epoch: 49 \t Validation precision@k5: 0.7001, accuracy@k5: 0.3737\n",
      "Epoch: 49 \t Validation precision@k10: 0.6639, accuracy@k10: 0.5658\n",
      "Epoch: 49 \t Validation precision@k15: 0.6924, accuracy@k15: 0.6710\n",
      "Epoch: 49 \t Validation precision@k20: 0.7469, accuracy@k20: 0.7437\n",
      "Epoch: 49 \t Validation precision@k25: 0.7991, accuracy@k25: 0.7989\n",
      "Epoch: 49 \t Validation precision@k30: 0.8380, accuracy@k30: 0.8380\n",
      "CPU: 20.52\n",
      "RAM %: 62.8\n",
      "Epoch: 50 \t Training Loss: 3.393763\n",
      "Epoch: 50 \t Validation precision@k5: 0.7050, accuracy@k5: 0.3772\n",
      "Epoch: 50 \t Validation precision@k10: 0.6623, accuracy@k10: 0.5644\n",
      "Epoch: 50 \t Validation precision@k15: 0.6916, accuracy@k15: 0.6703\n",
      "Epoch: 50 \t Validation precision@k20: 0.7452, accuracy@k20: 0.7421\n",
      "Epoch: 50 \t Validation precision@k25: 0.7987, accuracy@k25: 0.7986\n",
      "Epoch: 50 \t Validation precision@k30: 0.8387, accuracy@k30: 0.8387\n",
      "CPU: 20.61\n",
      "RAM %: 62.8\n",
      "Epoch: 51 \t Training Loss: 3.391547\n",
      "Epoch: 51 \t Validation precision@k5: 0.7111, accuracy@k5: 0.3794\n",
      "Epoch: 51 \t Validation precision@k10: 0.6688, accuracy@k10: 0.5694\n",
      "Epoch: 51 \t Validation precision@k15: 0.6919, accuracy@k15: 0.6706\n",
      "Epoch: 51 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7426\n",
      "Epoch: 51 \t Validation precision@k25: 0.7973, accuracy@k25: 0.7972\n",
      "Epoch: 51 \t Validation precision@k30: 0.8370, accuracy@k30: 0.8370\n",
      "CPU: 21.15\n",
      "RAM %: 62.8\n",
      "Epoch: 52 \t Training Loss: 3.392240\n",
      "Epoch: 52 \t Validation precision@k5: 0.7133, accuracy@k5: 0.3810\n",
      "Epoch: 52 \t Validation precision@k10: 0.6663, accuracy@k10: 0.5671\n",
      "Epoch: 52 \t Validation precision@k15: 0.6896, accuracy@k15: 0.6682\n",
      "Epoch: 52 \t Validation precision@k20: 0.7436, accuracy@k20: 0.7406\n",
      "Epoch: 52 \t Validation precision@k25: 0.7987, accuracy@k25: 0.7985\n",
      "Epoch: 52 \t Validation precision@k30: 0.8382, accuracy@k30: 0.8382\n",
      "CPU: 21.16\n",
      "RAM %: 62.8\n",
      "Epoch: 53 \t Training Loss: 3.391068\n",
      "Epoch: 53 \t Validation precision@k5: 0.7032, accuracy@k5: 0.3752\n",
      "Epoch: 53 \t Validation precision@k10: 0.6639, accuracy@k10: 0.5650\n",
      "Epoch: 53 \t Validation precision@k15: 0.6910, accuracy@k15: 0.6697\n",
      "Epoch: 53 \t Validation precision@k20: 0.7467, accuracy@k20: 0.7436\n",
      "Epoch: 53 \t Validation precision@k25: 0.7973, accuracy@k25: 0.7971\n",
      "Epoch: 53 \t Validation precision@k30: 0.8388, accuracy@k30: 0.8388\n",
      "CPU: 21.28\n",
      "RAM %: 62.7\n",
      "Epoch: 54 \t Training Loss: 3.390249\n",
      "Epoch: 54 \t Validation precision@k5: 0.7071, accuracy@k5: 0.3778\n",
      "Epoch: 54 \t Validation precision@k10: 0.6635, accuracy@k10: 0.5648\n",
      "Epoch: 54 \t Validation precision@k15: 0.6918, accuracy@k15: 0.6703\n",
      "Epoch: 54 \t Validation precision@k20: 0.7460, accuracy@k20: 0.7429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 \t Validation precision@k25: 0.7997, accuracy@k25: 0.7995\n",
      "Epoch: 54 \t Validation precision@k30: 0.8389, accuracy@k30: 0.8389\n",
      "CPU: 21.63\n",
      "RAM %: 62.7\n",
      "Epoch: 55 \t Training Loss: 3.389363\n",
      "Epoch: 55 \t Validation precision@k5: 0.7104, accuracy@k5: 0.3794\n",
      "Epoch: 55 \t Validation precision@k10: 0.6669, accuracy@k10: 0.5679\n",
      "Epoch: 55 \t Validation precision@k15: 0.6907, accuracy@k15: 0.6693\n",
      "Epoch: 55 \t Validation precision@k20: 0.7468, accuracy@k20: 0.7437\n",
      "Epoch: 55 \t Validation precision@k25: 0.7990, accuracy@k25: 0.7988\n",
      "Epoch: 55 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 21.74\n",
      "RAM %: 62.7\n",
      "Epoch: 56 \t Training Loss: 3.388649\n",
      "Epoch: 56 \t Validation precision@k5: 0.7061, accuracy@k5: 0.3775\n",
      "Epoch: 56 \t Validation precision@k10: 0.6622, accuracy@k10: 0.5639\n",
      "Epoch: 56 \t Validation precision@k15: 0.6906, accuracy@k15: 0.6692\n",
      "Epoch: 56 \t Validation precision@k20: 0.7452, accuracy@k20: 0.7421\n",
      "Epoch: 56 \t Validation precision@k25: 0.7991, accuracy@k25: 0.7989\n",
      "Epoch: 56 \t Validation precision@k30: 0.8380, accuracy@k30: 0.8380\n",
      "CPU: 21.66\n",
      "RAM %: 62.8\n",
      "Epoch: 57 \t Training Loss: 3.389397\n",
      "Epoch: 57 \t Validation precision@k5: 0.7113, accuracy@k5: 0.3800\n",
      "Epoch: 57 \t Validation precision@k10: 0.6641, accuracy@k10: 0.5656\n",
      "Epoch: 57 \t Validation precision@k15: 0.6918, accuracy@k15: 0.6704\n",
      "Epoch: 57 \t Validation precision@k20: 0.7446, accuracy@k20: 0.7415\n",
      "Epoch: 57 \t Validation precision@k25: 0.8005, accuracy@k25: 0.8003\n",
      "Epoch: 57 \t Validation precision@k30: 0.8376, accuracy@k30: 0.8376\n",
      "CPU: 21.63\n",
      "RAM %: 62.8\n",
      "Epoch: 58 \t Training Loss: 3.388854\n",
      "Epoch: 58 \t Validation precision@k5: 0.7128, accuracy@k5: 0.3804\n",
      "Epoch: 58 \t Validation precision@k10: 0.6669, accuracy@k10: 0.5679\n",
      "Epoch: 58 \t Validation precision@k15: 0.6903, accuracy@k15: 0.6687\n",
      "Epoch: 58 \t Validation precision@k20: 0.7443, accuracy@k20: 0.7411\n",
      "Epoch: 58 \t Validation precision@k25: 0.7995, accuracy@k25: 0.7993\n",
      "Epoch: 58 \t Validation precision@k30: 0.8382, accuracy@k30: 0.8382\n",
      "CPU: 21.65\n",
      "RAM %: 62.7\n",
      "Epoch: 59 \t Training Loss: 3.388091\n",
      "Epoch: 59 \t Validation precision@k5: 0.7095, accuracy@k5: 0.3794\n",
      "Epoch: 59 \t Validation precision@k10: 0.6661, accuracy@k10: 0.5671\n",
      "Epoch: 59 \t Validation precision@k15: 0.6944, accuracy@k15: 0.6726\n",
      "Epoch: 59 \t Validation precision@k20: 0.7469, accuracy@k20: 0.7438\n",
      "Epoch: 59 \t Validation precision@k25: 0.7994, accuracy@k25: 0.7993\n",
      "Epoch: 59 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 21.76\n",
      "RAM %: 62.7\n",
      "Epoch: 60 \t Training Loss: 3.387894\n",
      "Epoch: 60 \t Validation precision@k5: 0.7085, accuracy@k5: 0.3783\n",
      "Epoch: 60 \t Validation precision@k10: 0.6653, accuracy@k10: 0.5666\n",
      "Epoch: 60 \t Validation precision@k15: 0.6924, accuracy@k15: 0.6707\n",
      "Epoch: 60 \t Validation precision@k20: 0.7445, accuracy@k20: 0.7414\n",
      "Epoch: 60 \t Validation precision@k25: 0.7977, accuracy@k25: 0.7975\n",
      "Epoch: 60 \t Validation precision@k30: 0.8378, accuracy@k30: 0.8378\n",
      "CPU: 22.01\n",
      "RAM %: 62.7\n",
      "Epoch: 61 \t Training Loss: 3.387428\n",
      "Epoch: 61 \t Validation precision@k5: 0.7157, accuracy@k5: 0.3822\n",
      "Epoch: 61 \t Validation precision@k10: 0.6669, accuracy@k10: 0.5684\n",
      "Epoch: 61 \t Validation precision@k15: 0.6898, accuracy@k15: 0.6685\n",
      "Epoch: 61 \t Validation precision@k20: 0.7437, accuracy@k20: 0.7406\n",
      "Epoch: 61 \t Validation precision@k25: 0.7969, accuracy@k25: 0.7967\n",
      "Epoch: 61 \t Validation precision@k30: 0.8373, accuracy@k30: 0.8373\n",
      "CPU: 22.08\n",
      "RAM %: 62.7\n",
      "Epoch: 62 \t Training Loss: 3.387330\n",
      "Epoch: 62 \t Validation precision@k5: 0.7105, accuracy@k5: 0.3796\n",
      "Epoch: 62 \t Validation precision@k10: 0.6630, accuracy@k10: 0.5649\n",
      "Epoch: 62 \t Validation precision@k15: 0.6899, accuracy@k15: 0.6684\n",
      "Epoch: 62 \t Validation precision@k20: 0.7450, accuracy@k20: 0.7419\n",
      "Epoch: 62 \t Validation precision@k25: 0.7983, accuracy@k25: 0.7982\n",
      "Epoch: 62 \t Validation precision@k30: 0.8366, accuracy@k30: 0.8366\n",
      "CPU: 22.20\n",
      "RAM %: 62.7\n",
      "Epoch: 63 \t Training Loss: 3.386489\n",
      "Epoch: 63 \t Validation precision@k5: 0.7109, accuracy@k5: 0.3799\n",
      "Epoch: 63 \t Validation precision@k10: 0.6682, accuracy@k10: 0.5695\n",
      "Epoch: 63 \t Validation precision@k15: 0.6935, accuracy@k15: 0.6720\n",
      "Epoch: 63 \t Validation precision@k20: 0.7464, accuracy@k20: 0.7432\n",
      "Epoch: 63 \t Validation precision@k25: 0.7970, accuracy@k25: 0.7968\n",
      "Epoch: 63 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 22.08\n",
      "RAM %: 62.7\n",
      "Epoch: 64 \t Training Loss: 3.385392\n",
      "Epoch: 64 \t Validation precision@k5: 0.7109, accuracy@k5: 0.3803\n",
      "Epoch: 64 \t Validation precision@k10: 0.6611, accuracy@k10: 0.5631\n",
      "Epoch: 64 \t Validation precision@k15: 0.6897, accuracy@k15: 0.6682\n",
      "Epoch: 64 \t Validation precision@k20: 0.7420, accuracy@k20: 0.7390\n",
      "Epoch: 64 \t Validation precision@k25: 0.7963, accuracy@k25: 0.7961\n",
      "Epoch: 64 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 22.33\n",
      "RAM %: 62.8\n",
      "Epoch: 65 \t Training Loss: 3.385725\n",
      "Epoch: 65 \t Validation precision@k5: 0.7086, accuracy@k5: 0.3783\n",
      "Epoch: 65 \t Validation precision@k10: 0.6640, accuracy@k10: 0.5652\n",
      "Epoch: 65 \t Validation precision@k15: 0.6953, accuracy@k15: 0.6736\n",
      "Epoch: 65 \t Validation precision@k20: 0.7454, accuracy@k20: 0.7423\n",
      "Epoch: 65 \t Validation precision@k25: 0.7987, accuracy@k25: 0.7985\n",
      "Epoch: 65 \t Validation precision@k30: 0.8382, accuracy@k30: 0.8382\n",
      "CPU: 22.34\n",
      "RAM %: 62.8\n",
      "Epoch: 66 \t Training Loss: 3.385457\n",
      "Epoch: 66 \t Validation precision@k5: 0.7124, accuracy@k5: 0.3804\n",
      "Epoch: 66 \t Validation precision@k10: 0.6658, accuracy@k10: 0.5668\n",
      "Epoch: 66 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6702\n",
      "Epoch: 66 \t Validation precision@k20: 0.7454, accuracy@k20: 0.7423\n",
      "Epoch: 66 \t Validation precision@k25: 0.7980, accuracy@k25: 0.7978\n",
      "Epoch: 66 \t Validation precision@k30: 0.8376, accuracy@k30: 0.8376\n",
      "CPU: 22.41\n",
      "RAM %: 62.8\n",
      "Epoch: 67 \t Training Loss: 3.385333\n",
      "Epoch: 67 \t Validation precision@k5: 0.7090, accuracy@k5: 0.3779\n",
      "Epoch: 67 \t Validation precision@k10: 0.6640, accuracy@k10: 0.5659\n",
      "Epoch: 67 \t Validation precision@k15: 0.6922, accuracy@k15: 0.6705\n",
      "Epoch: 67 \t Validation precision@k20: 0.7452, accuracy@k20: 0.7420\n",
      "Epoch: 67 \t Validation precision@k25: 0.8002, accuracy@k25: 0.8000\n",
      "Epoch: 67 \t Validation precision@k30: 0.8387, accuracy@k30: 0.8387\n",
      "CPU: 23.02\n",
      "RAM %: 62.7\n",
      "Epoch: 68 \t Training Loss: 3.385281\n",
      "Epoch: 68 \t Validation precision@k5: 0.7078, accuracy@k5: 0.3795\n",
      "Epoch: 68 \t Validation precision@k10: 0.6655, accuracy@k10: 0.5669\n",
      "Epoch: 68 \t Validation precision@k15: 0.6923, accuracy@k15: 0.6706\n",
      "Epoch: 68 \t Validation precision@k20: 0.7474, accuracy@k20: 0.7443\n",
      "Epoch: 68 \t Validation precision@k25: 0.7978, accuracy@k25: 0.7976\n",
      "Epoch: 68 \t Validation precision@k30: 0.8371, accuracy@k30: 0.8371\n",
      "CPU: 23.26\n",
      "RAM %: 62.8\n",
      "Epoch: 69 \t Training Loss: 3.384658\n",
      "Epoch: 69 \t Validation precision@k5: 0.7114, accuracy@k5: 0.3793\n",
      "Epoch: 69 \t Validation precision@k10: 0.6645, accuracy@k10: 0.5657\n",
      "Epoch: 69 \t Validation precision@k15: 0.6932, accuracy@k15: 0.6715\n",
      "Epoch: 69 \t Validation precision@k20: 0.7467, accuracy@k20: 0.7436\n",
      "Epoch: 69 \t Validation precision@k25: 0.7980, accuracy@k25: 0.7978\n",
      "Epoch: 69 \t Validation precision@k30: 0.8377, accuracy@k30: 0.8377\n",
      "CPU: 23.86\n",
      "RAM %: 62.8\n",
      "Epoch: 70 \t Training Loss: 3.384333\n",
      "Epoch: 70 \t Validation precision@k5: 0.7083, accuracy@k5: 0.3788\n",
      "Epoch: 70 \t Validation precision@k10: 0.6691, accuracy@k10: 0.5701\n",
      "Epoch: 70 \t Validation precision@k15: 0.6915, accuracy@k15: 0.6700\n",
      "Epoch: 70 \t Validation precision@k20: 0.7484, accuracy@k20: 0.7453\n",
      "Epoch: 70 \t Validation precision@k25: 0.7974, accuracy@k25: 0.7972\n",
      "Epoch: 70 \t Validation precision@k30: 0.8375, accuracy@k30: 0.8375\n",
      "CPU: 23.95\n",
      "RAM %: 62.8\n",
      "Epoch: 71 \t Training Loss: 3.384385\n",
      "Epoch: 71 \t Validation precision@k5: 0.7119, accuracy@k5: 0.3803\n",
      "Epoch: 71 \t Validation precision@k10: 0.6650, accuracy@k10: 0.5666\n",
      "Epoch: 71 \t Validation precision@k15: 0.6934, accuracy@k15: 0.6719\n",
      "Epoch: 71 \t Validation precision@k20: 0.7474, accuracy@k20: 0.7442\n",
      "Epoch: 71 \t Validation precision@k25: 0.7988, accuracy@k25: 0.7987\n",
      "Epoch: 71 \t Validation precision@k30: 0.8390, accuracy@k30: 0.8390\n",
      "CPU: 23.89\n",
      "RAM %: 62.8\n",
      "Epoch: 72 \t Training Loss: 3.383004\n",
      "Epoch: 72 \t Validation precision@k5: 0.7007, accuracy@k5: 0.3749\n",
      "Epoch: 72 \t Validation precision@k10: 0.6678, accuracy@k10: 0.5684\n",
      "Epoch: 72 \t Validation precision@k15: 0.6944, accuracy@k15: 0.6726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 \t Validation precision@k20: 0.7468, accuracy@k20: 0.7436\n",
      "Epoch: 72 \t Validation precision@k25: 0.7977, accuracy@k25: 0.7976\n",
      "Epoch: 72 \t Validation precision@k30: 0.8365, accuracy@k30: 0.8365\n",
      "CPU: 24.15\n",
      "RAM %: 62.8\n",
      "Epoch: 73 \t Training Loss: 3.384805\n",
      "Epoch: 73 \t Validation precision@k5: 0.7091, accuracy@k5: 0.3794\n",
      "Epoch: 73 \t Validation precision@k10: 0.6641, accuracy@k10: 0.5655\n",
      "Epoch: 73 \t Validation precision@k15: 0.6936, accuracy@k15: 0.6719\n",
      "Epoch: 73 \t Validation precision@k20: 0.7474, accuracy@k20: 0.7442\n",
      "Epoch: 73 \t Validation precision@k25: 0.7970, accuracy@k25: 0.7969\n",
      "Epoch: 73 \t Validation precision@k30: 0.8353, accuracy@k30: 0.8353\n",
      "CPU: 24.27\n",
      "RAM %: 62.8\n",
      "Epoch: 74 \t Training Loss: 3.383311\n",
      "Epoch: 74 \t Validation precision@k5: 0.7097, accuracy@k5: 0.3789\n",
      "Epoch: 74 \t Validation precision@k10: 0.6631, accuracy@k10: 0.5646\n",
      "Epoch: 74 \t Validation precision@k15: 0.6928, accuracy@k15: 0.6711\n",
      "Epoch: 74 \t Validation precision@k20: 0.7456, accuracy@k20: 0.7425\n",
      "Epoch: 74 \t Validation precision@k25: 0.7977, accuracy@k25: 0.7976\n",
      "Epoch: 74 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 24.21\n",
      "RAM %: 62.8\n",
      "Epoch: 75 \t Training Loss: 3.383160\n",
      "Epoch: 75 \t Validation precision@k5: 0.7083, accuracy@k5: 0.3789\n",
      "Epoch: 75 \t Validation precision@k10: 0.6671, accuracy@k10: 0.5686\n",
      "Epoch: 75 \t Validation precision@k15: 0.6917, accuracy@k15: 0.6702\n",
      "Epoch: 75 \t Validation precision@k20: 0.7479, accuracy@k20: 0.7447\n",
      "Epoch: 75 \t Validation precision@k25: 0.7974, accuracy@k25: 0.7972\n",
      "Epoch: 75 \t Validation precision@k30: 0.8355, accuracy@k30: 0.8355\n",
      "CPU: 24.54\n",
      "RAM %: 62.8\n",
      "Epoch: 76 \t Training Loss: 3.382864\n",
      "Epoch: 76 \t Validation precision@k5: 0.7133, accuracy@k5: 0.3821\n",
      "Epoch: 76 \t Validation precision@k10: 0.6707, accuracy@k10: 0.5717\n",
      "Epoch: 76 \t Validation precision@k15: 0.6942, accuracy@k15: 0.6726\n",
      "Epoch: 76 \t Validation precision@k20: 0.7486, accuracy@k20: 0.7454\n",
      "Epoch: 76 \t Validation precision@k25: 0.7973, accuracy@k25: 0.7972\n",
      "Epoch: 76 \t Validation precision@k30: 0.8374, accuracy@k30: 0.8374\n",
      "CPU: 24.76\n",
      "RAM %: 62.8\n",
      "Epoch: 77 \t Training Loss: 3.381796\n",
      "Epoch: 77 \t Validation precision@k5: 0.7107, accuracy@k5: 0.3804\n",
      "Epoch: 77 \t Validation precision@k10: 0.6663, accuracy@k10: 0.5674\n",
      "Epoch: 77 \t Validation precision@k15: 0.6911, accuracy@k15: 0.6694\n",
      "Epoch: 77 \t Validation precision@k20: 0.7459, accuracy@k20: 0.7428\n",
      "Epoch: 77 \t Validation precision@k25: 0.7989, accuracy@k25: 0.7987\n",
      "Epoch: 77 \t Validation precision@k30: 0.8372, accuracy@k30: 0.8372\n",
      "CPU: 24.87\n",
      "RAM %: 62.8\n",
      "Epoch: 78 \t Training Loss: 3.382672\n",
      "Epoch: 78 \t Validation precision@k5: 0.7029, accuracy@k5: 0.3755\n",
      "Epoch: 78 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5639\n",
      "Epoch: 78 \t Validation precision@k15: 0.6937, accuracy@k15: 0.6722\n",
      "Epoch: 78 \t Validation precision@k20: 0.7458, accuracy@k20: 0.7427\n",
      "Epoch: 78 \t Validation precision@k25: 0.7983, accuracy@k25: 0.7981\n",
      "Epoch: 78 \t Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n",
      "CPU: 24.87\n",
      "RAM %: 62.7\n",
      "Epoch: 79 \t Training Loss: 3.383424\n",
      "Epoch: 79 \t Validation precision@k5: 0.7098, accuracy@k5: 0.3799\n",
      "Epoch: 79 \t Validation precision@k10: 0.6619, accuracy@k10: 0.5637\n",
      "Epoch: 79 \t Validation precision@k15: 0.6876, accuracy@k15: 0.6661\n",
      "Epoch: 79 \t Validation precision@k20: 0.7451, accuracy@k20: 0.7419\n",
      "Epoch: 79 \t Validation precision@k25: 0.7993, accuracy@k25: 0.7992\n",
      "Epoch: 79 \t Validation precision@k30: 0.8383, accuracy@k30: 0.8383\n",
      "CPU: 24.84\n",
      "RAM %: 62.7\n",
      "Epoch: 80 \t Training Loss: 3.382904\n",
      "Epoch: 80 \t Validation precision@k5: 0.7085, accuracy@k5: 0.3788\n",
      "Epoch: 80 \t Validation precision@k10: 0.6640, accuracy@k10: 0.5658\n",
      "Epoch: 80 \t Validation precision@k15: 0.6922, accuracy@k15: 0.6708\n",
      "Epoch: 80 \t Validation precision@k20: 0.7468, accuracy@k20: 0.7437\n",
      "Epoch: 80 \t Validation precision@k25: 0.7979, accuracy@k25: 0.7977\n",
      "Epoch: 80 \t Validation precision@k30: 0.8376, accuracy@k30: 0.8376\n",
      "CPU: 25.13\n",
      "RAM %: 62.7\n",
      "Epoch: 81 \t Training Loss: 3.382736\n",
      "Epoch: 81 \t Validation precision@k5: 0.7132, accuracy@k5: 0.3816\n",
      "Epoch: 81 \t Validation precision@k10: 0.6618, accuracy@k10: 0.5632\n",
      "Epoch: 81 \t Validation precision@k15: 0.6927, accuracy@k15: 0.6710\n",
      "Epoch: 81 \t Validation precision@k20: 0.7444, accuracy@k20: 0.7412\n",
      "Epoch: 81 \t Validation precision@k25: 0.7975, accuracy@k25: 0.7973\n",
      "Epoch: 81 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 25.27\n",
      "RAM %: 62.7\n",
      "Epoch: 82 \t Training Loss: 3.381645\n",
      "Epoch: 82 \t Validation precision@k5: 0.7100, accuracy@k5: 0.3796\n",
      "Epoch: 82 \t Validation precision@k10: 0.6666, accuracy@k10: 0.5679\n",
      "Epoch: 82 \t Validation precision@k15: 0.6944, accuracy@k15: 0.6728\n",
      "Epoch: 82 \t Validation precision@k20: 0.7488, accuracy@k20: 0.7456\n",
      "Epoch: 82 \t Validation precision@k25: 0.7985, accuracy@k25: 0.7984\n",
      "Epoch: 82 \t Validation precision@k30: 0.8381, accuracy@k30: 0.8381\n",
      "CPU: 25.30\n",
      "RAM %: 62.8\n",
      "Epoch: 83 \t Training Loss: 3.382735\n",
      "Epoch: 83 \t Validation precision@k5: 0.7064, accuracy@k5: 0.3780\n",
      "Epoch: 83 \t Validation precision@k10: 0.6673, accuracy@k10: 0.5687\n",
      "Epoch: 83 \t Validation precision@k15: 0.6962, accuracy@k15: 0.6744\n",
      "Epoch: 83 \t Validation precision@k20: 0.7468, accuracy@k20: 0.7436\n",
      "Epoch: 83 \t Validation precision@k25: 0.7957, accuracy@k25: 0.7955\n",
      "Epoch: 83 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 25.33\n",
      "RAM %: 62.8\n",
      "Epoch: 84 \t Training Loss: 3.381971\n",
      "Epoch: 84 \t Validation precision@k5: 0.7098, accuracy@k5: 0.3783\n",
      "Epoch: 84 \t Validation precision@k10: 0.6651, accuracy@k10: 0.5664\n",
      "Epoch: 84 \t Validation precision@k15: 0.6941, accuracy@k15: 0.6723\n",
      "Epoch: 84 \t Validation precision@k20: 0.7476, accuracy@k20: 0.7444\n",
      "Epoch: 84 \t Validation precision@k25: 0.7978, accuracy@k25: 0.7976\n",
      "Epoch: 84 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 25.87\n",
      "RAM %: 62.8\n",
      "Epoch: 85 \t Training Loss: 3.383400\n",
      "Epoch: 85 \t Validation precision@k5: 0.7062, accuracy@k5: 0.3775\n",
      "Epoch: 85 \t Validation precision@k10: 0.6640, accuracy@k10: 0.5653\n",
      "Epoch: 85 \t Validation precision@k15: 0.6953, accuracy@k15: 0.6736\n",
      "Epoch: 85 \t Validation precision@k20: 0.7476, accuracy@k20: 0.7444\n",
      "Epoch: 85 \t Validation precision@k25: 0.7981, accuracy@k25: 0.7980\n",
      "Epoch: 85 \t Validation precision@k30: 0.8365, accuracy@k30: 0.8365\n",
      "CPU: 25.78\n",
      "RAM %: 62.8\n",
      "Epoch: 86 \t Training Loss: 3.382481\n",
      "Epoch: 86 \t Validation precision@k5: 0.7138, accuracy@k5: 0.3809\n",
      "Epoch: 86 \t Validation precision@k10: 0.6629, accuracy@k10: 0.5645\n",
      "Epoch: 86 \t Validation precision@k15: 0.6908, accuracy@k15: 0.6692\n",
      "Epoch: 86 \t Validation precision@k20: 0.7474, accuracy@k20: 0.7442\n",
      "Epoch: 86 \t Validation precision@k25: 0.7975, accuracy@k25: 0.7973\n",
      "Epoch: 86 \t Validation precision@k30: 0.8385, accuracy@k30: 0.8385\n",
      "CPU: 25.96\n",
      "RAM %: 62.7\n",
      "Epoch: 87 \t Training Loss: 3.381486\n",
      "Epoch: 87 \t Validation precision@k5: 0.7146, accuracy@k5: 0.3819\n",
      "Epoch: 87 \t Validation precision@k10: 0.6651, accuracy@k10: 0.5662\n",
      "Epoch: 87 \t Validation precision@k15: 0.6923, accuracy@k15: 0.6708\n",
      "Epoch: 87 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7425\n",
      "Epoch: 87 \t Validation precision@k25: 0.7996, accuracy@k25: 0.7994\n",
      "Epoch: 87 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 25.91\n",
      "RAM %: 62.7\n",
      "Epoch: 88 \t Training Loss: 3.382697\n",
      "Epoch: 88 \t Validation precision@k5: 0.7085, accuracy@k5: 0.3790\n",
      "Epoch: 88 \t Validation precision@k10: 0.6652, accuracy@k10: 0.5666\n",
      "Epoch: 88 \t Validation precision@k15: 0.6941, accuracy@k15: 0.6725\n",
      "Epoch: 88 \t Validation precision@k20: 0.7477, accuracy@k20: 0.7446\n",
      "Epoch: 88 \t Validation precision@k25: 0.7973, accuracy@k25: 0.7971\n",
      "Epoch: 88 \t Validation precision@k30: 0.8366, accuracy@k30: 0.8366\n",
      "CPU: 26.39\n",
      "RAM %: 62.8\n",
      "Epoch: 89 \t Training Loss: 3.381197\n",
      "Epoch: 89 \t Validation precision@k5: 0.7101, accuracy@k5: 0.3797\n",
      "Epoch: 89 \t Validation precision@k10: 0.6658, accuracy@k10: 0.5669\n",
      "Epoch: 89 \t Validation precision@k15: 0.6932, accuracy@k15: 0.6715\n",
      "Epoch: 89 \t Validation precision@k20: 0.7466, accuracy@k20: 0.7434\n",
      "Epoch: 89 \t Validation precision@k25: 0.7947, accuracy@k25: 0.7945\n",
      "Epoch: 89 \t Validation precision@k30: 0.8377, accuracy@k30: 0.8377\n",
      "CPU: 26.70\n",
      "RAM %: 62.8\n",
      "Epoch: 90 \t Training Loss: 3.381186\n",
      "Epoch: 90 \t Validation precision@k5: 0.7128, accuracy@k5: 0.3812\n",
      "Epoch: 90 \t Validation precision@k10: 0.6672, accuracy@k10: 0.5680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 \t Validation precision@k15: 0.6941, accuracy@k15: 0.6724\n",
      "Epoch: 90 \t Validation precision@k20: 0.7461, accuracy@k20: 0.7429\n",
      "Epoch: 90 \t Validation precision@k25: 0.7988, accuracy@k25: 0.7986\n",
      "Epoch: 90 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 26.82\n",
      "RAM %: 62.8\n",
      "Epoch: 91 \t Training Loss: 3.381402\n",
      "Epoch: 91 \t Validation precision@k5: 0.7114, accuracy@k5: 0.3806\n",
      "Epoch: 91 \t Validation precision@k10: 0.6644, accuracy@k10: 0.5655\n",
      "Epoch: 91 \t Validation precision@k15: 0.6954, accuracy@k15: 0.6738\n",
      "Epoch: 91 \t Validation precision@k20: 0.7451, accuracy@k20: 0.7420\n",
      "Epoch: 91 \t Validation precision@k25: 0.7975, accuracy@k25: 0.7974\n",
      "Epoch: 91 \t Validation precision@k30: 0.8364, accuracy@k30: 0.8364\n",
      "CPU: 26.79\n",
      "RAM %: 62.7\n",
      "Epoch: 92 \t Training Loss: 3.382241\n",
      "Epoch: 92 \t Validation precision@k5: 0.7140, accuracy@k5: 0.3814\n",
      "Epoch: 92 \t Validation precision@k10: 0.6646, accuracy@k10: 0.5661\n",
      "Epoch: 92 \t Validation precision@k15: 0.6929, accuracy@k15: 0.6712\n",
      "Epoch: 92 \t Validation precision@k20: 0.7443, accuracy@k20: 0.7412\n",
      "Epoch: 92 \t Validation precision@k25: 0.7975, accuracy@k25: 0.7974\n",
      "Epoch: 92 \t Validation precision@k30: 0.8360, accuracy@k30: 0.8360\n",
      "CPU: 26.69\n",
      "RAM %: 62.7\n",
      "Epoch: 93 \t Training Loss: 3.382532\n",
      "Epoch: 93 \t Validation precision@k5: 0.7108, accuracy@k5: 0.3802\n",
      "Epoch: 93 \t Validation precision@k10: 0.6630, accuracy@k10: 0.5643\n",
      "Epoch: 93 \t Validation precision@k15: 0.6933, accuracy@k15: 0.6717\n",
      "Epoch: 93 \t Validation precision@k20: 0.7450, accuracy@k20: 0.7419\n",
      "Epoch: 93 \t Validation precision@k25: 0.7969, accuracy@k25: 0.7967\n",
      "Epoch: 93 \t Validation precision@k30: 0.8380, accuracy@k30: 0.8380\n",
      "CPU: 26.70\n",
      "RAM %: 62.8\n",
      "Epoch: 94 \t Training Loss: 3.382161\n",
      "Epoch: 94 \t Validation precision@k5: 0.7076, accuracy@k5: 0.3771\n",
      "Epoch: 94 \t Validation precision@k10: 0.6659, accuracy@k10: 0.5667\n",
      "Epoch: 94 \t Validation precision@k15: 0.6934, accuracy@k15: 0.6718\n",
      "Epoch: 94 \t Validation precision@k20: 0.7476, accuracy@k20: 0.7445\n",
      "Epoch: 94 \t Validation precision@k25: 0.7979, accuracy@k25: 0.7977\n",
      "Epoch: 94 \t Validation precision@k30: 0.8368, accuracy@k30: 0.8368\n",
      "CPU: 26.64\n",
      "RAM %: 62.8\n",
      "Epoch: 95 \t Training Loss: 3.382053\n",
      "Epoch: 95 \t Validation precision@k5: 0.7097, accuracy@k5: 0.3801\n",
      "Epoch: 95 \t Validation precision@k10: 0.6645, accuracy@k10: 0.5655\n",
      "Epoch: 95 \t Validation precision@k15: 0.6935, accuracy@k15: 0.6718\n",
      "Epoch: 95 \t Validation precision@k20: 0.7447, accuracy@k20: 0.7415\n",
      "Epoch: 95 \t Validation precision@k25: 0.7973, accuracy@k25: 0.7971\n",
      "Epoch: 95 \t Validation precision@k30: 0.8366, accuracy@k30: 0.8366\n",
      "CPU: 26.64\n",
      "RAM %: 62.8\n",
      "Epoch: 96 \t Training Loss: 3.380739\n",
      "Epoch: 96 \t Validation precision@k5: 0.7053, accuracy@k5: 0.3764\n",
      "Epoch: 96 \t Validation precision@k10: 0.6610, accuracy@k10: 0.5637\n",
      "Epoch: 96 \t Validation precision@k15: 0.6888, accuracy@k15: 0.6672\n",
      "Epoch: 96 \t Validation precision@k20: 0.7431, accuracy@k20: 0.7400\n",
      "Epoch: 96 \t Validation precision@k25: 0.7986, accuracy@k25: 0.7984\n",
      "Epoch: 96 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 26.73\n",
      "RAM %: 62.7\n",
      "Epoch: 97 \t Training Loss: 3.381116\n",
      "Epoch: 97 \t Validation precision@k5: 0.7118, accuracy@k5: 0.3804\n",
      "Epoch: 97 \t Validation precision@k10: 0.6626, accuracy@k10: 0.5641\n",
      "Epoch: 97 \t Validation precision@k15: 0.6943, accuracy@k15: 0.6727\n",
      "Epoch: 97 \t Validation precision@k20: 0.7457, accuracy@k20: 0.7426\n",
      "Epoch: 97 \t Validation precision@k25: 0.7970, accuracy@k25: 0.7969\n",
      "Epoch: 97 \t Validation precision@k30: 0.8367, accuracy@k30: 0.8367\n",
      "CPU: 26.81\n",
      "RAM %: 62.7\n",
      "Epoch: 98 \t Training Loss: 3.380931\n",
      "Epoch: 98 \t Validation precision@k5: 0.7178, accuracy@k5: 0.3838\n",
      "Epoch: 98 \t Validation precision@k10: 0.6691, accuracy@k10: 0.5701\n",
      "Epoch: 98 \t Validation precision@k15: 0.6939, accuracy@k15: 0.6723\n",
      "Epoch: 98 \t Validation precision@k20: 0.7473, accuracy@k20: 0.7441\n",
      "Epoch: 98 \t Validation precision@k25: 0.7974, accuracy@k25: 0.7972\n",
      "Epoch: 98 \t Validation precision@k30: 0.8362, accuracy@k30: 0.8362\n",
      "CPU: 26.72\n",
      "RAM %: 62.8\n",
      "Epoch: 99 \t Training Loss: 3.381719\n",
      "Epoch: 99 \t Validation precision@k5: 0.7126, accuracy@k5: 0.3807\n",
      "Epoch: 99 \t Validation precision@k10: 0.6625, accuracy@k10: 0.5642\n",
      "Epoch: 99 \t Validation precision@k15: 0.6926, accuracy@k15: 0.6710\n",
      "Epoch: 99 \t Validation precision@k20: 0.7451, accuracy@k20: 0.7420\n",
      "Epoch: 99 \t Validation precision@k25: 0.7959, accuracy@k25: 0.7957\n",
      "Epoch: 99 \t Validation precision@k30: 0.8378, accuracy@k30: 0.8378\n",
      "CPU: 26.65\n",
      "RAM %: 62.8\n",
      "Epoch: 100 \t Training Loss: 3.381064\n",
      "Epoch: 100 \t Validation precision@k5: 0.7090, accuracy@k5: 0.3785\n",
      "Epoch: 100 \t Validation precision@k10: 0.6660, accuracy@k10: 0.5673\n",
      "Epoch: 100 \t Validation precision@k15: 0.6918, accuracy@k15: 0.6701\n",
      "Epoch: 100 \t Validation precision@k20: 0.7453, accuracy@k20: 0.7421\n",
      "Epoch: 100 \t Validation precision@k25: 0.7988, accuracy@k25: 0.7986\n",
      "Epoch: 100 \t Validation precision@k30: 0.8373, accuracy@k30: 0.8373\n",
      "CPU: 26.53\n",
      "RAM %: 62.8\n",
      "Max CPU usage: 26.825\tMax RAM % usage: 62.9\n",
      "CPU times: user 27min 33s, sys: 5min 2s, total: 32min 36s\n",
      "Wall time: 18min 53s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_retain, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.6537, accuracy@k5: 0.3494\n",
      "Validation precision@k10: 0.6338, accuracy@k10: 0.5404\n",
      "Validation precision@k15: 0.6770, accuracy@k15: 0.6569\n",
      "Validation precision@k20: 0.7373, accuracy@k20: 0.7340\n",
      "Validation precision@k25: 0.7941, accuracy@k25: 0.7939\n",
      "Validation precision@k30: 0.8369, accuracy@k30: 0.8369\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_retain, test_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(baseline_retain, os.path.join(CHECKPOINT_PATH, \"BaselineRETAIN_100.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
