{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7006db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"data/\"\n",
    "GRAM_DATA_PATH = \"../Project/code/processed_data/gram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef1516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'vids.pkl'), 'rb'))\n",
    "targs = pickle.load(open(os.path.join(DATA_PATH,'targets.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'seqs.pkl'), 'rb'))\n",
    "diags = pickle.load(open(os.path.join(DATA_PATH,'diags.pkl'), 'rb'))\n",
    "codes = pickle.load(open(os.path.join(DATA_PATH,'icd9.pkl'), 'rb'))\n",
    "categories = pickle.load(open(os.path.join(DATA_PATH,'categories.pkl'), 'rb'))\n",
    "sub_categories = pickle.load(open(os.path.join(DATA_PATH,'subcategories.pkl'), 'rb'))\n",
    "assert len(pids) == len(vids) == len(targs) == len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608c4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, targets):\n",
    "\n",
    "        self.x = seqs\n",
    "        self.y = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return(len(self.x))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return (self.x[index], self.y[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2d4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(seqs, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d83bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "    \"\"\"\n",
    "    sequences, targets = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(targets, dtype=torch.float)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "    batch_num_categories = [len(visit) for patient in targets for visit in patient]\n",
    "    global sub_categories\n",
    "# #     import pdb; pdb.set_trace()\n",
    "    num_categories = len(sub_categories)\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    max_num_categories = max(batch_num_categories)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    y = torch.zeros((num_patients, max_num_categories), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_x_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    y_masks = torch.zeros((num_patients, max_num_categories), dtype=torch.bool)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i_patient, patient in enumerate(sequences):   \n",
    "        for j_visit, visit in enumerate(patient[:-1]):\n",
    "            for k_code, code in enumerate(visit):\n",
    "                x[i_patient, j_visit, k_code] = code\n",
    "                x_masks[i_patient, j_visit, k_code] = 1\n",
    "                if j_visit == len(patient) - 2 and k_code == len(visit) - 1:\n",
    "                    rev_visit = x_masks[i_patient].any(dim=1)\n",
    "                    rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "                    rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient[1:]):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "                \n",
    "    for i_patient, patient in enumerate(targets):   \n",
    "        for visit in patient[-1:]:\n",
    "            for k_code, code in enumerate(visit):\n",
    "                y[i_patient, k_code] = code\n",
    "                y_masks[i_patient, k_code] = 1\n",
    "\n",
    "#     for i_patient, patient in enumerate(sequences):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 x[i_patient, j_visit, k_code] = code\n",
    "#                 x_masks[i_patient, j_visit, k_code] = 1\n",
    "#                 if j_visit == len(patient) - 1 and k_code == len(visit) - 1:\n",
    "#                     rev_visit = x_masks[i_patient].any(dim=1)\n",
    "#                     rev_x[i_patient, rev_visit] = x[i_patient, rev_visit].flip(0)\n",
    "#                     rev_x_masks[i_patient, rev_visit] = x_masks[i_patient, rev_visit].flip(0)\n",
    "  \n",
    "#     for i_patient, patient in enumerate(targets):   \n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             for k_code, code in enumerate(visit):\n",
    "#                 y[i_patient, j_visit, k_code] = code\n",
    "#                 y_masks[i_patient, j_visit, k_code] = 1\n",
    "    \n",
    "    return x, x_masks, rev_x, rev_x_masks, y, y_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8fa891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "val_split = int(len(dataset)*0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46334f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6561\n",
      "Length of test dataset: 1312\n",
      "Length of val dataset: 875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_split = int(len(dataset)*0.75)\n",
    "test_split = int(len(dataset)*0.15)\n",
    "\n",
    "lengths = [train_split, test_split, len(dataset) - (train_split + test_split)]\n",
    "train_dataset, test_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2351f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, test_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 100\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               collate_fn=collate_fn,\n",
    "                                               shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=False)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             collate_fn=collate_fn,\n",
    "                                             shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = load_data(train_dataset, test_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cb6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    x[~masks] = 0\n",
    "    return x.sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86967a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_sum(alpha, beta, rev_v, rev_masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the hidden states for true visits (not padding visits) and then\n",
    "        sum the them up.\n",
    "\n",
    "    Arguments:\n",
    "        alpha: the alpha attention weights of shape (batch_size, seq_length, 1)\n",
    "        beta: the beta attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "        rev_v: the visit embeddings in reversed time of shape (batch_size, # visits, embedding_dim)\n",
    "        rev_masks: the padding masks in reversed time of shape (# visits, batch_size, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        c: the context vector of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "    \"\"\"\n",
    "    rev_masks = rev_masks[:,:].any(dim=2)\n",
    "    rev_v_masked = rev_v.clone()\n",
    "    rev_v_masked[~rev_masks] = 0\n",
    "    weights = alpha * beta\n",
    "    a_sum = torch.sum(weights * rev_v_masked, dim=1)\n",
    "    return a_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab9eecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.a_att` for alpha-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.a_att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            g: the output tensor from RNN-alpha of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            alpha: the corresponding attention weights of shape (batch_size, seq_length, 1)\n",
    "        \"\"\"\n",
    "        m = nn.Softmax(dim=1)\n",
    "        return m(self.a_att(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55ec0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Define the linear layer `self.b_att` for beta-attention using `nn.Linear()`;\n",
    "        \n",
    "        Arguments:\n",
    "            hidden_dim: the hidden dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        self.b_att = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, h):\n",
    "        \"\"\"\n",
    "        TODO: Implement the beta attention.\n",
    "        \n",
    "        Arguments:\n",
    "            h: the output tensor from RNN-beta of shape (batch_size, seq_length, hidden_dim) \n",
    "        \n",
    "        Outputs:\n",
    "            beta: the corresponding attention weights of shape (batch_size, seq_length, hidden_dim)\n",
    "            \n",
    "        HINT: consider `torch.tanh`\n",
    "        \"\"\"\n",
    "        return torch.tanh(self.b_att(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a4e3332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineRETAIN(\n",
       "  (embedding): Embedding(4903, 128)\n",
       "  (rnn_a): GRU(128, 128, batch_first=True)\n",
       "  (rnn_b): GRU(128, 128, batch_first=True)\n",
       "  (att_a): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (att_b): AlphaAttention(\n",
       "    (a_att): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=184, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaselineRETAIN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes, num_categories):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_codes, embedding_dim=128)\n",
    "        self.rnn_a = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.rnn_b = nn.GRU(128, hidden_size=128, batch_first=True)\n",
    "        self.att_a = AlphaAttention(128)\n",
    "        self.att_b = AlphaAttention(128)\n",
    "        self.fc = nn.Linear(128, num_categories)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "       \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        g, _ = self.rnn_a(rev_x)\n",
    "        h, _ = self.rnn_b(rev_x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "#         true_h_n = get_last_visit(output, masks)\n",
    "        alpha = self.att_a(g)\n",
    "        beta = self.att_b(h)\n",
    "        c = attention_sum(alpha, beta, rev_x, rev_masks)\n",
    "        logits = self.fc(c)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.softmax(logits)\n",
    "        #probs = self.sigmoid(logits)\n",
    "        \n",
    "#         probs = probs.reshape(probs.shape[0]*probs.shape[1], probs.shape[2])\n",
    "#         y_masks = y_masks.reshape(y_masks.shape[0]*y_masks.shape[1], y_masks.shape[2])\n",
    "#         probs = probs[y_masks.any(dim=1)]\n",
    "        return logits\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "baseline_retain = BaselineRETAIN(num_codes = len(codes), num_categories=len(sub_categories))\n",
    "baseline_retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d7eccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adadelta(baseline_retain.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afba2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    #base_cpu, base_ram = print_cpu_usage()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "#             import pdb; pdb.set_trace()\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks)\n",
    "#             import pdb; pdb.set_trace()\n",
    "#             y[~y_masks] = criterion.ignore_index\n",
    "#             last_y = y_masks.any(dim=2).sum(dim=1) - 1\n",
    "#             indices = last_y.unsqueeze(-1)\n",
    "#             indices = indices.repeat(1, y.shape[2])\n",
    "#             indices = indices.unsqueeze(1)\n",
    "#             y_filt = torch.gather(y, 1, indices)\n",
    "\n",
    "#             n_visits = y_masks.any(dim=2).sum(dim=1)\n",
    "#             for i_patient, j_visit in enumerate(n_visits):\n",
    "#                 for visit in range(j_visit - 1):\n",
    "#                     mask = y_masks[i_patient, visit+1]\n",
    "\n",
    "#                     yh = y_hat[i_patient, visit]\n",
    "#                     y_tmp = indices_to_multihot(\n",
    "#                         y[i_patient, visit+1], mask, yh)\n",
    "            \n",
    "            # stack into visits\n",
    "#             yh = y_hat.reshape(y_hat.shape[0] * y_hat.shape[1], y_hat.shape[2])\n",
    "#             y_masks = y_masks.reshape(y_masks.shape[0] * y_masks.shape[1], y_masks.shape[2])\n",
    "#             y = y.reshape(y.shape[0] * y.shape[1], y.shape[2])\n",
    "        \n",
    "                    \n",
    "            \n",
    "            y_mh = indices_to_multihot(y, y_masks, y_hat)\n",
    "            loss = criterion(y_hat, y_mh)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print_cpu_usage()\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        for k in range(5, 31, 5):\n",
    "            precision_k, accuracy_k = eval_model(model, test_loader, k=k)\n",
    "            print(f'Epoch: {epoch+1} \\t Validation precision@k{k}: {precision_k:.2f}, accuracy@k{k}: {accuracy_k:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34dfac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, k=15, n=-1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    all_precision = []\n",
    "    all_accuracy = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, x_masks, rev_x, rev_x_masks, y, y_masks in test_loader:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            n_eval = y.shape[0] - 1 if n == -1 else n\n",
    "            y_hat = model(x, x_masks, rev_x, rev_x_masks)\n",
    "            y_hat = F.softmax(y_hat, dim=-1)\n",
    "#             num_labels = y_hat.shape[1]\n",
    "#             num_categories = torch.count_nonzero(y, dim=1)\n",
    "#             nz_rows, nz_cols = torch.nonzero(y, as_tuple=True)\n",
    "            y_multihot = indices_to_multihot(y, y_masks, y_hat)\n",
    "            k_correct = 0\n",
    "#             predictions = 0\n",
    "            total_precision = 0\n",
    "            total_accuracy = 0\n",
    "            for i in range(n_eval):\n",
    "                visit_correct = 0\n",
    "#                 y_true = nz_cols[nz_rows == i]\n",
    "                y_true = y[i, y_masks[i]]\n",
    "                _, y_pred = torch.topk(y_hat[i], k)\n",
    "#                 for v in y_pred:\n",
    "#                     if v in y_true:\n",
    "#                         visit_correct += 1\n",
    "                for v in y_true:\n",
    "                    if v in y_pred:\n",
    "                        visit_correct += 1\n",
    "#                 predictions += len(y_true)\n",
    "                visit_precision = visit_correct / min(k, len(y_true))\n",
    "                visit_accuracy = visit_correct / len(y_true)\n",
    "                #print(f'visit {i}: precision: {visit_precision:0.2f} accuracy: {visit_accuracy:0.2f}')\n",
    "                k_correct += visit_correct\n",
    "                total_precision += visit_precision\n",
    "                total_accuracy += visit_accuracy\n",
    "            #import pdb; pdb.set_trace()\n",
    "#             precision_k = precision / k\n",
    "#             accuracy_k = k_correct / predictions\n",
    "            precision_k = total_precision / n_eval\n",
    "            accuracy_k = total_accuracy / n_eval\n",
    "            all_precision.append(precision_k)\n",
    "            all_accuracy.append(accuracy_k)\n",
    "            \n",
    "#             y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_hat = (y_hat > 0.5).int()\n",
    "#             y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "#             y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "#     import pdb; pdb.set_trace()\n",
    "    total_precision_k = np.mean(all_precision)\n",
    "    total_accuracy_k = np.mean(all_accuracy)\n",
    "    return total_precision_k, total_accuracy_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "680009f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_multihot(indices, masks, y_hat):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    #indices = indices[masks.any(dim=1)]\n",
    "    multihot = torch.zeros_like(y_hat, dtype=torch.float)\n",
    "    for idx, row in enumerate(indices):\n",
    "        y_idx = row[masks[idx]].unique()\n",
    "        multihot[idx] = F.one_hot(y_idx, y_hat.shape[1]).sum(0).float()\n",
    "    return multihot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a058ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cpu_usage():\n",
    "    load = psutil.getloadavg()[2]\n",
    "    cpu_usage = (load/os.cpu_count()) * 100\n",
    "    ram = psutil.virtual_memory()[2]\n",
    "    print(f\"CPU: {cpu_usage:0.2f}\")\n",
    "    print(f\"RAM %: {ram}\")\n",
    "    return cpu_usage, ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1e778f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 23.71\n",
      "RAM %: 61.9\n",
      "Epoch: 1 \t Training Loss: 41.331719\n",
      "Epoch: 1 \t Validation precision@k5: 0.66, accuracy@k5: 0.35\n",
      "Epoch: 1 \t Validation precision@k10: 0.62, accuracy@k10: 0.52\n",
      "Epoch: 1 \t Validation precision@k15: 0.65, accuracy@k15: 0.63\n",
      "Epoch: 1 \t Validation precision@k20: 0.72, accuracy@k20: 0.72\n",
      "Epoch: 1 \t Validation precision@k25: 0.77, accuracy@k25: 0.77\n",
      "Epoch: 1 \t Validation precision@k30: 0.82, accuracy@k30: 0.82\n",
      "CPU: 25.21\n",
      "RAM %: 62.0\n",
      "Epoch: 2 \t Training Loss: 40.443196\n",
      "Epoch: 2 \t Validation precision@k5: 0.68, accuracy@k5: 0.36\n",
      "Epoch: 2 \t Validation precision@k10: 0.63, accuracy@k10: 0.54\n",
      "Epoch: 2 \t Validation precision@k15: 0.67, accuracy@k15: 0.65\n",
      "Epoch: 2 \t Validation precision@k20: 0.73, accuracy@k20: 0.73\n",
      "Epoch: 2 \t Validation precision@k25: 0.79, accuracy@k25: 0.79\n",
      "Epoch: 2 \t Validation precision@k30: 0.83, accuracy@k30: 0.83\n",
      "CPU: 25.27\n",
      "RAM %: 61.9\n",
      "Epoch: 3 \t Training Loss: 40.007895\n",
      "Epoch: 3 \t Validation precision@k5: 0.69, accuracy@k5: 0.37\n",
      "Epoch: 3 \t Validation precision@k10: 0.64, accuracy@k10: 0.55\n",
      "Epoch: 3 \t Validation precision@k15: 0.68, accuracy@k15: 0.66\n",
      "Epoch: 3 \t Validation precision@k20: 0.74, accuracy@k20: 0.74\n",
      "Epoch: 3 \t Validation precision@k25: 0.80, accuracy@k25: 0.80\n",
      "Epoch: 3 \t Validation precision@k30: 0.84, accuracy@k30: 0.84\n",
      "CPU: 25.61\n",
      "RAM %: 62.0\n",
      "Epoch: 4 \t Training Loss: 39.662485\n",
      "Epoch: 4 \t Validation precision@k5: 0.69, accuracy@k5: 0.37\n",
      "Epoch: 4 \t Validation precision@k10: 0.65, accuracy@k10: 0.55\n",
      "Epoch: 4 \t Validation precision@k15: 0.69, accuracy@k15: 0.67\n",
      "Epoch: 4 \t Validation precision@k20: 0.75, accuracy@k20: 0.74\n",
      "Epoch: 4 \t Validation precision@k25: 0.80, accuracy@k25: 0.80\n",
      "Epoch: 4 \t Validation precision@k30: 0.85, accuracy@k30: 0.85\n",
      "CPU: 26.00\n",
      "RAM %: 62.0\n",
      "Epoch: 5 \t Training Loss: 39.399582\n",
      "Epoch: 5 \t Validation precision@k5: 0.70, accuracy@k5: 0.37\n",
      "Epoch: 5 \t Validation precision@k10: 0.66, accuracy@k10: 0.56\n",
      "Epoch: 5 \t Validation precision@k15: 0.70, accuracy@k15: 0.67\n",
      "Epoch: 5 \t Validation precision@k20: 0.76, accuracy@k20: 0.75\n",
      "Epoch: 5 \t Validation precision@k25: 0.81, accuracy@k25: 0.81\n",
      "Epoch: 5 \t Validation precision@k30: 0.85, accuracy@k30: 0.85\n",
      "CPU: 25.84\n",
      "RAM %: 62.0\n",
      "Epoch: 6 \t Training Loss: 39.167986\n",
      "Epoch: 6 \t Validation precision@k5: 0.71, accuracy@k5: 0.38\n",
      "Epoch: 6 \t Validation precision@k10: 0.67, accuracy@k10: 0.57\n",
      "Epoch: 6 \t Validation precision@k15: 0.70, accuracy@k15: 0.68\n",
      "Epoch: 6 \t Validation precision@k20: 0.76, accuracy@k20: 0.76\n",
      "Epoch: 6 \t Validation precision@k25: 0.81, accuracy@k25: 0.81\n",
      "Epoch: 6 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 26.44\n",
      "RAM %: 62.1\n",
      "Epoch: 7 \t Training Loss: 38.941495\n",
      "Epoch: 7 \t Validation precision@k5: 0.72, accuracy@k5: 0.38\n",
      "Epoch: 7 \t Validation precision@k10: 0.67, accuracy@k10: 0.57\n",
      "Epoch: 7 \t Validation precision@k15: 0.71, accuracy@k15: 0.68\n",
      "Epoch: 7 \t Validation precision@k20: 0.77, accuracy@k20: 0.76\n",
      "Epoch: 7 \t Validation precision@k25: 0.82, accuracy@k25: 0.82\n",
      "Epoch: 7 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 26.44\n",
      "RAM %: 62.0\n",
      "Epoch: 8 \t Training Loss: 38.747355\n",
      "Epoch: 8 \t Validation precision@k5: 0.72, accuracy@k5: 0.39\n",
      "Epoch: 8 \t Validation precision@k10: 0.68, accuracy@k10: 0.58\n",
      "Epoch: 8 \t Validation precision@k15: 0.71, accuracy@k15: 0.69\n",
      "Epoch: 8 \t Validation precision@k20: 0.77, accuracy@k20: 0.77\n",
      "Epoch: 8 \t Validation precision@k25: 0.82, accuracy@k25: 0.82\n",
      "Epoch: 8 \t Validation precision@k30: 0.86, accuracy@k30: 0.86\n",
      "CPU: 26.58\n",
      "RAM %: 61.9\n",
      "Epoch: 9 \t Training Loss: 38.581050\n",
      "Epoch: 9 \t Validation precision@k5: 0.73, accuracy@k5: 0.39\n",
      "Epoch: 9 \t Validation precision@k10: 0.68, accuracy@k10: 0.58\n",
      "Epoch: 9 \t Validation precision@k15: 0.72, accuracy@k15: 0.70\n",
      "Epoch: 9 \t Validation precision@k20: 0.78, accuracy@k20: 0.77\n",
      "Epoch: 9 \t Validation precision@k25: 0.83, accuracy@k25: 0.83\n",
      "Epoch: 9 \t Validation precision@k30: 0.87, accuracy@k30: 0.87\n",
      "CPU: 26.89\n",
      "RAM %: 62.0\n",
      "Epoch: 10 \t Training Loss: 38.394242\n",
      "Epoch: 10 \t Validation precision@k5: 0.73, accuracy@k5: 0.39\n",
      "Epoch: 10 \t Validation precision@k10: 0.69, accuracy@k10: 0.59\n",
      "Epoch: 10 \t Validation precision@k15: 0.73, accuracy@k15: 0.70\n",
      "Epoch: 10 \t Validation precision@k20: 0.78, accuracy@k20: 0.78\n",
      "Epoch: 10 \t Validation precision@k25: 0.83, accuracy@k25: 0.83\n",
      "Epoch: 10 \t Validation precision@k30: 0.87, accuracy@k30: 0.87\n",
      "CPU: 27.64\n",
      "RAM %: 61.9\n",
      "Epoch: 11 \t Training Loss: 38.239956\n",
      "Epoch: 11 \t Validation precision@k5: 0.73, accuracy@k5: 0.39\n",
      "Epoch: 11 \t Validation precision@k10: 0.69, accuracy@k10: 0.59\n",
      "Epoch: 11 \t Validation precision@k15: 0.73, accuracy@k15: 0.71\n",
      "Epoch: 11 \t Validation precision@k20: 0.79, accuracy@k20: 0.78\n",
      "Epoch: 11 \t Validation precision@k25: 0.84, accuracy@k25: 0.84\n",
      "Epoch: 11 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 27.50\n",
      "RAM %: 62.1\n",
      "Epoch: 12 \t Training Loss: 38.081083\n",
      "Epoch: 12 \t Validation precision@k5: 0.74, accuracy@k5: 0.40\n",
      "Epoch: 12 \t Validation precision@k10: 0.70, accuracy@k10: 0.60\n",
      "Epoch: 12 \t Validation precision@k15: 0.74, accuracy@k15: 0.71\n",
      "Epoch: 12 \t Validation precision@k20: 0.79, accuracy@k20: 0.79\n",
      "Epoch: 12 \t Validation precision@k25: 0.84, accuracy@k25: 0.84\n",
      "Epoch: 12 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 28.13\n",
      "RAM %: 62.1\n",
      "Epoch: 13 \t Training Loss: 37.937356\n",
      "Epoch: 13 \t Validation precision@k5: 0.75, accuracy@k5: 0.40\n",
      "Epoch: 13 \t Validation precision@k10: 0.71, accuracy@k10: 0.60\n",
      "Epoch: 13 \t Validation precision@k15: 0.74, accuracy@k15: 0.72\n",
      "Epoch: 13 \t Validation precision@k20: 0.80, accuracy@k20: 0.79\n",
      "Epoch: 13 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 13 \t Validation precision@k30: 0.88, accuracy@k30: 0.88\n",
      "CPU: 28.04\n",
      "RAM %: 62.2\n",
      "Epoch: 14 \t Training Loss: 37.784688\n",
      "Epoch: 14 \t Validation precision@k5: 0.75, accuracy@k5: 0.40\n",
      "Epoch: 14 \t Validation precision@k10: 0.71, accuracy@k10: 0.61\n",
      "Epoch: 14 \t Validation precision@k15: 0.75, accuracy@k15: 0.72\n",
      "Epoch: 14 \t Validation precision@k20: 0.80, accuracy@k20: 0.80\n",
      "Epoch: 14 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 14 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 28.33\n",
      "RAM %: 62.1\n",
      "Epoch: 15 \t Training Loss: 37.634567\n",
      "Epoch: 15 \t Validation precision@k5: 0.76, accuracy@k5: 0.41\n",
      "Epoch: 15 \t Validation precision@k10: 0.72, accuracy@k10: 0.61\n",
      "Epoch: 15 \t Validation precision@k15: 0.75, accuracy@k15: 0.73\n",
      "Epoch: 15 \t Validation precision@k20: 0.81, accuracy@k20: 0.80\n",
      "Epoch: 15 \t Validation precision@k25: 0.85, accuracy@k25: 0.85\n",
      "Epoch: 15 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 28.92\n",
      "RAM %: 62.1\n",
      "Epoch: 16 \t Training Loss: 37.488641\n",
      "Epoch: 16 \t Validation precision@k5: 0.76, accuracy@k5: 0.41\n",
      "Epoch: 16 \t Validation precision@k10: 0.72, accuracy@k10: 0.62\n",
      "Epoch: 16 \t Validation precision@k15: 0.76, accuracy@k15: 0.73\n",
      "Epoch: 16 \t Validation precision@k20: 0.81, accuracy@k20: 0.81\n",
      "Epoch: 16 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 16 \t Validation precision@k30: 0.89, accuracy@k30: 0.89\n",
      "CPU: 29.37\n",
      "RAM %: 62.2\n",
      "Epoch: 17 \t Training Loss: 37.361418\n",
      "Epoch: 17 \t Validation precision@k5: 0.77, accuracy@k5: 0.41\n",
      "Epoch: 17 \t Validation precision@k10: 0.73, accuracy@k10: 0.62\n",
      "Epoch: 17 \t Validation precision@k15: 0.76, accuracy@k15: 0.74\n",
      "Epoch: 17 \t Validation precision@k20: 0.82, accuracy@k20: 0.81\n",
      "Epoch: 17 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 17 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 29.86\n",
      "RAM %: 62.1\n",
      "Epoch: 18 \t Training Loss: 37.248671\n",
      "Epoch: 18 \t Validation precision@k5: 0.77, accuracy@k5: 0.41\n",
      "Epoch: 18 \t Validation precision@k10: 0.73, accuracy@k10: 0.63\n",
      "Epoch: 18 \t Validation precision@k15: 0.77, accuracy@k15: 0.74\n",
      "Epoch: 18 \t Validation precision@k20: 0.82, accuracy@k20: 0.82\n",
      "Epoch: 18 \t Validation precision@k25: 0.86, accuracy@k25: 0.86\n",
      "Epoch: 18 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n",
      "CPU: 29.87\n",
      "RAM %: 62.3\n",
      "Epoch: 19 \t Training Loss: 37.084072\n",
      "Epoch: 19 \t Validation precision@k5: 0.77, accuracy@k5: 0.42\n",
      "Epoch: 19 \t Validation precision@k10: 0.74, accuracy@k10: 0.63\n",
      "Epoch: 19 \t Validation precision@k15: 0.77, accuracy@k15: 0.75\n",
      "Epoch: 19 \t Validation precision@k20: 0.82, accuracy@k20: 0.82\n",
      "Epoch: 19 \t Validation precision@k25: 0.87, accuracy@k25: 0.87\n",
      "Epoch: 19 \t Validation precision@k30: 0.90, accuracy@k30: 0.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 30.57\n",
      "RAM %: 62.3\n",
      "Epoch: 20 \t Training Loss: 36.955495\n",
      "Epoch: 20 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 20 \t Validation precision@k10: 0.74, accuracy@k10: 0.64\n",
      "Epoch: 20 \t Validation precision@k15: 0.78, accuracy@k15: 0.75\n",
      "Epoch: 20 \t Validation precision@k20: 0.83, accuracy@k20: 0.82\n",
      "Epoch: 20 \t Validation precision@k25: 0.87, accuracy@k25: 0.87\n",
      "Epoch: 20 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 31.44\n",
      "RAM %: 62.2\n",
      "Epoch: 21 \t Training Loss: 36.842083\n",
      "Epoch: 21 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 21 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 21 \t Validation precision@k15: 0.78, accuracy@k15: 0.76\n",
      "Epoch: 21 \t Validation precision@k20: 0.83, accuracy@k20: 0.83\n",
      "Epoch: 21 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 21 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 31.89\n",
      "RAM %: 62.3\n",
      "Epoch: 22 \t Training Loss: 36.710299\n",
      "Epoch: 22 \t Validation precision@k5: 0.78, accuracy@k5: 0.42\n",
      "Epoch: 22 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 22 \t Validation precision@k15: 0.78, accuracy@k15: 0.76\n",
      "Epoch: 22 \t Validation precision@k20: 0.84, accuracy@k20: 0.83\n",
      "Epoch: 22 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 22 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 32.03\n",
      "RAM %: 62.3\n",
      "Epoch: 23 \t Training Loss: 36.568494\n",
      "Epoch: 23 \t Validation precision@k5: 0.79, accuracy@k5: 0.42\n",
      "Epoch: 23 \t Validation precision@k10: 0.75, accuracy@k10: 0.64\n",
      "Epoch: 23 \t Validation precision@k15: 0.79, accuracy@k15: 0.76\n",
      "Epoch: 23 \t Validation precision@k20: 0.84, accuracy@k20: 0.84\n",
      "Epoch: 23 \t Validation precision@k25: 0.88, accuracy@k25: 0.88\n",
      "Epoch: 23 \t Validation precision@k30: 0.91, accuracy@k30: 0.91\n",
      "CPU: 32.67\n",
      "RAM %: 62.3\n",
      "Epoch: 24 \t Training Loss: 36.467277\n",
      "Epoch: 24 \t Validation precision@k5: 0.79, accuracy@k5: 0.43\n",
      "Epoch: 24 \t Validation precision@k10: 0.76, accuracy@k10: 0.65\n",
      "Epoch: 24 \t Validation precision@k15: 0.79, accuracy@k15: 0.77\n",
      "Epoch: 24 \t Validation precision@k20: 0.84, accuracy@k20: 0.84\n",
      "Epoch: 24 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 24 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 33.13\n",
      "RAM %: 62.3\n",
      "Epoch: 25 \t Training Loss: 36.341828\n",
      "Epoch: 25 \t Validation precision@k5: 0.79, accuracy@k5: 0.43\n",
      "Epoch: 25 \t Validation precision@k10: 0.76, accuracy@k10: 0.65\n",
      "Epoch: 25 \t Validation precision@k15: 0.80, accuracy@k15: 0.77\n",
      "Epoch: 25 \t Validation precision@k20: 0.85, accuracy@k20: 0.84\n",
      "Epoch: 25 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 25 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 33.00\n",
      "RAM %: 62.3\n",
      "Epoch: 26 \t Training Loss: 36.213176\n",
      "Epoch: 26 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 26 \t Validation precision@k10: 0.77, accuracy@k10: 0.65\n",
      "Epoch: 26 \t Validation precision@k15: 0.80, accuracy@k15: 0.78\n",
      "Epoch: 26 \t Validation precision@k20: 0.85, accuracy@k20: 0.85\n",
      "Epoch: 26 \t Validation precision@k25: 0.89, accuracy@k25: 0.89\n",
      "Epoch: 26 \t Validation precision@k30: 0.92, accuracy@k30: 0.92\n",
      "CPU: 32.97\n",
      "RAM %: 62.3\n",
      "Epoch: 27 \t Training Loss: 36.102917\n",
      "Epoch: 27 \t Validation precision@k5: 0.80, accuracy@k5: 0.43\n",
      "Epoch: 27 \t Validation precision@k10: 0.77, accuracy@k10: 0.66\n",
      "Epoch: 27 \t Validation precision@k15: 0.80, accuracy@k15: 0.78\n",
      "Epoch: 27 \t Validation precision@k20: 0.86, accuracy@k20: 0.85\n",
      "Epoch: 27 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 27 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 32.85\n",
      "RAM %: 62.3\n",
      "Epoch: 28 \t Training Loss: 35.985447\n",
      "Epoch: 28 \t Validation precision@k5: 0.80, accuracy@k5: 0.44\n",
      "Epoch: 28 \t Validation precision@k10: 0.77, accuracy@k10: 0.66\n",
      "Epoch: 28 \t Validation precision@k15: 0.81, accuracy@k15: 0.78\n",
      "Epoch: 28 \t Validation precision@k20: 0.86, accuracy@k20: 0.86\n",
      "Epoch: 28 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 28 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 32.51\n",
      "RAM %: 62.3\n",
      "Epoch: 29 \t Training Loss: 35.874526\n",
      "Epoch: 29 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 29 \t Validation precision@k10: 0.78, accuracy@k10: 0.66\n",
      "Epoch: 29 \t Validation precision@k15: 0.81, accuracy@k15: 0.79\n",
      "Epoch: 29 \t Validation precision@k20: 0.86, accuracy@k20: 0.86\n",
      "Epoch: 29 \t Validation precision@k25: 0.90, accuracy@k25: 0.90\n",
      "Epoch: 29 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 32.44\n",
      "RAM %: 62.3\n",
      "Epoch: 30 \t Training Loss: 35.765627\n",
      "Epoch: 30 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 30 \t Validation precision@k10: 0.78, accuracy@k10: 0.67\n",
      "Epoch: 30 \t Validation precision@k15: 0.82, accuracy@k15: 0.79\n",
      "Epoch: 30 \t Validation precision@k20: 0.87, accuracy@k20: 0.86\n",
      "Epoch: 30 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 30 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 32.37\n",
      "RAM %: 62.3\n",
      "Epoch: 31 \t Training Loss: 35.652507\n",
      "Epoch: 31 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 31 \t Validation precision@k10: 0.78, accuracy@k10: 0.67\n",
      "Epoch: 31 \t Validation precision@k15: 0.82, accuracy@k15: 0.79\n",
      "Epoch: 31 \t Validation precision@k20: 0.87, accuracy@k20: 0.87\n",
      "Epoch: 31 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 31 \t Validation precision@k30: 0.93, accuracy@k30: 0.93\n",
      "CPU: 32.13\n",
      "RAM %: 62.3\n",
      "Epoch: 32 \t Training Loss: 35.547842\n",
      "Epoch: 32 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 32 \t Validation precision@k10: 0.79, accuracy@k10: 0.67\n",
      "Epoch: 32 \t Validation precision@k15: 0.82, accuracy@k15: 0.80\n",
      "Epoch: 32 \t Validation precision@k20: 0.87, accuracy@k20: 0.87\n",
      "Epoch: 32 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 32 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 32.34\n",
      "RAM %: 62.3\n",
      "Epoch: 33 \t Training Loss: 35.444071\n",
      "Epoch: 33 \t Validation precision@k5: 0.81, accuracy@k5: 0.44\n",
      "Epoch: 33 \t Validation precision@k10: 0.79, accuracy@k10: 0.67\n",
      "Epoch: 33 \t Validation precision@k15: 0.83, accuracy@k15: 0.80\n",
      "Epoch: 33 \t Validation precision@k20: 0.88, accuracy@k20: 0.87\n",
      "Epoch: 33 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 33 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 32.36\n",
      "RAM %: 62.3\n",
      "Epoch: 34 \t Training Loss: 35.345652\n",
      "Epoch: 34 \t Validation precision@k5: 0.82, accuracy@k5: 0.44\n",
      "Epoch: 34 \t Validation precision@k10: 0.79, accuracy@k10: 0.68\n",
      "Epoch: 34 \t Validation precision@k15: 0.83, accuracy@k15: 0.80\n",
      "Epoch: 34 \t Validation precision@k20: 0.88, accuracy@k20: 0.87\n",
      "Epoch: 34 \t Validation precision@k25: 0.91, accuracy@k25: 0.91\n",
      "Epoch: 34 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 32.49\n",
      "RAM %: 62.3\n",
      "Epoch: 35 \t Training Loss: 35.229255\n",
      "Epoch: 35 \t Validation precision@k5: 0.82, accuracy@k5: 0.44\n",
      "Epoch: 35 \t Validation precision@k10: 0.80, accuracy@k10: 0.68\n",
      "Epoch: 35 \t Validation precision@k15: 0.83, accuracy@k15: 0.81\n",
      "Epoch: 35 \t Validation precision@k20: 0.88, accuracy@k20: 0.88\n",
      "Epoch: 35 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 35 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 32.56\n",
      "RAM %: 62.3\n",
      "Epoch: 36 \t Training Loss: 35.128752\n",
      "Epoch: 36 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 36 \t Validation precision@k10: 0.80, accuracy@k10: 0.68\n",
      "Epoch: 36 \t Validation precision@k15: 0.84, accuracy@k15: 0.81\n",
      "Epoch: 36 \t Validation precision@k20: 0.88, accuracy@k20: 0.88\n",
      "Epoch: 36 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 36 \t Validation precision@k30: 0.94, accuracy@k30: 0.94\n",
      "CPU: 32.67\n",
      "RAM %: 62.3\n",
      "Epoch: 37 \t Training Loss: 35.026180\n",
      "Epoch: 37 \t Validation precision@k5: 0.82, accuracy@k5: 0.45\n",
      "Epoch: 37 \t Validation precision@k10: 0.80, accuracy@k10: 0.69\n",
      "Epoch: 37 \t Validation precision@k15: 0.84, accuracy@k15: 0.81\n",
      "Epoch: 37 \t Validation precision@k20: 0.89, accuracy@k20: 0.88\n",
      "Epoch: 37 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n",
      "Epoch: 37 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 33.11\n",
      "RAM %: 62.3\n",
      "Epoch: 38 \t Training Loss: 34.933541\n",
      "Epoch: 38 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 38 \t Validation precision@k10: 0.80, accuracy@k10: 0.69\n",
      "Epoch: 38 \t Validation precision@k15: 0.84, accuracy@k15: 0.82\n",
      "Epoch: 38 \t Validation precision@k20: 0.89, accuracy@k20: 0.88\n",
      "Epoch: 38 \t Validation precision@k25: 0.92, accuracy@k25: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 32.84\n",
      "RAM %: 62.3\n",
      "Epoch: 39 \t Training Loss: 34.852547\n",
      "Epoch: 39 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 39 \t Validation precision@k10: 0.81, accuracy@k10: 0.69\n",
      "Epoch: 39 \t Validation precision@k15: 0.84, accuracy@k15: 0.82\n",
      "Epoch: 39 \t Validation precision@k20: 0.89, accuracy@k20: 0.89\n",
      "Epoch: 39 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 39 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 32.14\n",
      "RAM %: 62.3\n",
      "Epoch: 40 \t Training Loss: 34.751591\n",
      "Epoch: 40 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 40 \t Validation precision@k10: 0.81, accuracy@k10: 0.69\n",
      "Epoch: 40 \t Validation precision@k15: 0.85, accuracy@k15: 0.82\n",
      "Epoch: 40 \t Validation precision@k20: 0.89, accuracy@k20: 0.89\n",
      "Epoch: 40 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 40 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 32.09\n",
      "RAM %: 62.3\n",
      "Epoch: 41 \t Training Loss: 34.661379\n",
      "Epoch: 41 \t Validation precision@k5: 0.83, accuracy@k5: 0.45\n",
      "Epoch: 41 \t Validation precision@k10: 0.81, accuracy@k10: 0.70\n",
      "Epoch: 41 \t Validation precision@k15: 0.85, accuracy@k15: 0.82\n",
      "Epoch: 41 \t Validation precision@k20: 0.90, accuracy@k20: 0.89\n",
      "Epoch: 41 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 41 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 31.79\n",
      "RAM %: 62.3\n",
      "Epoch: 42 \t Training Loss: 34.572548\n",
      "Epoch: 42 \t Validation precision@k5: 0.84, accuracy@k5: 0.45\n",
      "Epoch: 42 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 42 \t Validation precision@k15: 0.85, accuracy@k15: 0.83\n",
      "Epoch: 42 \t Validation precision@k20: 0.90, accuracy@k20: 0.89\n",
      "Epoch: 42 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 42 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 38.73\n",
      "RAM %: 58.3\n",
      "Epoch: 43 \t Training Loss: 34.479646\n",
      "Epoch: 43 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 43 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 43 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 43 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 43 \t Validation precision@k25: 0.93, accuracy@k25: 0.93\n",
      "Epoch: 43 \t Validation precision@k30: 0.95, accuracy@k30: 0.95\n",
      "CPU: 39.97\n",
      "RAM %: 58.5\n",
      "Epoch: 44 \t Training Loss: 34.416130\n",
      "Epoch: 44 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 44 \t Validation precision@k10: 0.82, accuracy@k10: 0.70\n",
      "Epoch: 44 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 44 \t Validation precision@k20: 0.90, accuracy@k20: 0.90\n",
      "Epoch: 44 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 44 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 39.54\n",
      "RAM %: 58.9\n",
      "Epoch: 45 \t Training Loss: 34.336534\n",
      "Epoch: 45 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 45 \t Validation precision@k10: 0.82, accuracy@k10: 0.71\n",
      "Epoch: 45 \t Validation precision@k15: 0.86, accuracy@k15: 0.83\n",
      "Epoch: 45 \t Validation precision@k20: 0.91, accuracy@k20: 0.90\n",
      "Epoch: 45 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 45 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 40.06\n",
      "RAM %: 59.2\n",
      "Epoch: 46 \t Training Loss: 34.253386\n",
      "Epoch: 46 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 46 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 46 \t Validation precision@k15: 0.86, accuracy@k15: 0.84\n",
      "Epoch: 46 \t Validation precision@k20: 0.91, accuracy@k20: 0.90\n",
      "Epoch: 46 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 46 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 41.84\n",
      "RAM %: 40.7\n",
      "Epoch: 47 \t Training Loss: 34.167721\n",
      "Epoch: 47 \t Validation precision@k5: 0.84, accuracy@k5: 0.46\n",
      "Epoch: 47 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 47 \t Validation precision@k15: 0.86, accuracy@k15: 0.84\n",
      "Epoch: 47 \t Validation precision@k20: 0.91, accuracy@k20: 0.90\n",
      "Epoch: 47 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 47 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 42.53\n",
      "RAM %: 41.3\n",
      "Epoch: 48 \t Training Loss: 34.095300\n",
      "Epoch: 48 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 48 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 48 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 48 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 48 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 48 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 42.50\n",
      "RAM %: 41.5\n",
      "Epoch: 49 \t Training Loss: 34.018705\n",
      "Epoch: 49 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 49 \t Validation precision@k10: 0.83, accuracy@k10: 0.71\n",
      "Epoch: 49 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 49 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 49 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 49 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 42.47\n",
      "RAM %: 41.8\n",
      "Epoch: 50 \t Training Loss: 33.952267\n",
      "Epoch: 50 \t Validation precision@k5: 0.85, accuracy@k5: 0.46\n",
      "Epoch: 50 \t Validation precision@k10: 0.83, accuracy@k10: 0.72\n",
      "Epoch: 50 \t Validation precision@k15: 0.87, accuracy@k15: 0.84\n",
      "Epoch: 50 \t Validation precision@k20: 0.91, accuracy@k20: 0.91\n",
      "Epoch: 50 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 50 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 42.30\n",
      "RAM %: 41.9\n",
      "Epoch: 51 \t Training Loss: 33.885132\n",
      "Epoch: 51 \t Validation precision@k5: 0.85, accuracy@k5: 0.47\n",
      "Epoch: 51 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 51 \t Validation precision@k15: 0.87, accuracy@k15: 0.85\n",
      "Epoch: 51 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 51 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 51 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 42.09\n",
      "RAM %: 42.3\n",
      "Epoch: 52 \t Training Loss: 33.816777\n",
      "Epoch: 52 \t Validation precision@k5: 0.85, accuracy@k5: 0.47\n",
      "Epoch: 52 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 52 \t Validation precision@k15: 0.87, accuracy@k15: 0.85\n",
      "Epoch: 52 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 52 \t Validation precision@k25: 0.94, accuracy@k25: 0.94\n",
      "Epoch: 52 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 53.39\n",
      "RAM %: 42.5\n",
      "Epoch: 53 \t Training Loss: 33.747703\n",
      "Epoch: 53 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 53 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 53 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 53 \t Validation precision@k20: 0.92, accuracy@k20: 0.91\n",
      "Epoch: 53 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 53 \t Validation precision@k30: 0.96, accuracy@k30: 0.96\n",
      "CPU: 52.46\n",
      "RAM %: 42.7\n",
      "Epoch: 54 \t Training Loss: 33.699374\n",
      "Epoch: 54 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 54 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 54 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 54 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 54 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 54 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 51.70\n",
      "RAM %: 42.8\n",
      "Epoch: 55 \t Training Loss: 33.629988\n",
      "Epoch: 55 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 55 \t Validation precision@k10: 0.84, accuracy@k10: 0.72\n",
      "Epoch: 55 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 55 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 55 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 55 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 50.61\n",
      "RAM %: 42.6\n",
      "Epoch: 56 \t Training Loss: 33.584561\n",
      "Epoch: 56 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 56 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 56 \t Validation precision@k15: 0.88, accuracy@k15: 0.85\n",
      "Epoch: 56 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 56 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 56 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 49.68\n",
      "RAM %: 42.9\n",
      "Epoch: 57 \t Training Loss: 33.516142\n",
      "Epoch: 57 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 57 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 57 \t Validation precision@k15: 0.88, accuracy@k15: 0.86\n",
      "Epoch: 57 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 57 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 48.93\n",
      "RAM %: 43.1\n",
      "Epoch: 58 \t Training Loss: 33.455336\n",
      "Epoch: 58 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 58 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 58 \t Validation precision@k15: 0.88, accuracy@k15: 0.86\n",
      "Epoch: 58 \t Validation precision@k20: 0.92, accuracy@k20: 0.92\n",
      "Epoch: 58 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 58 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 48.35\n",
      "RAM %: 43.3\n",
      "Epoch: 59 \t Training Loss: 33.402290\n",
      "Epoch: 59 \t Validation precision@k5: 0.86, accuracy@k5: 0.47\n",
      "Epoch: 59 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 59 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 59 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 59 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 59 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 47.65\n",
      "RAM %: 43.3\n",
      "Epoch: 60 \t Training Loss: 33.352799\n",
      "Epoch: 60 \t Validation precision@k5: 0.87, accuracy@k5: 0.47\n",
      "Epoch: 60 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 60 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 60 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 60 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 60 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 47.34\n",
      "RAM %: 43.4\n",
      "Epoch: 61 \t Training Loss: 33.317788\n",
      "Epoch: 61 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 61 \t Validation precision@k10: 0.85, accuracy@k10: 0.73\n",
      "Epoch: 61 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 61 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 61 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 61 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 46.99\n",
      "RAM %: 43.4\n",
      "Epoch: 62 \t Training Loss: 33.255290\n",
      "Epoch: 62 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 62 \t Validation precision@k10: 0.86, accuracy@k10: 0.73\n",
      "Epoch: 62 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 62 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 62 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 62 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 46.52\n",
      "RAM %: 43.6\n",
      "Epoch: 63 \t Training Loss: 33.199761\n",
      "Epoch: 63 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 63 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 63 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 63 \t Validation precision@k20: 0.93, accuracy@k20: 0.92\n",
      "Epoch: 63 \t Validation precision@k25: 0.95, accuracy@k25: 0.95\n",
      "Epoch: 63 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 45.85\n",
      "RAM %: 43.5\n",
      "Epoch: 64 \t Training Loss: 33.161687\n",
      "Epoch: 64 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 64 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 64 \t Validation precision@k15: 0.89, accuracy@k15: 0.86\n",
      "Epoch: 64 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 64 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 64 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 45.72\n",
      "RAM %: 43.6\n",
      "Epoch: 65 \t Training Loss: 33.138016\n",
      "Epoch: 65 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 65 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 65 \t Validation precision@k15: 0.89, accuracy@k15: 0.87\n",
      "Epoch: 65 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 65 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 65 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 45.33\n",
      "RAM %: 43.7\n",
      "Epoch: 66 \t Training Loss: 33.069656\n",
      "Epoch: 66 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 66 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 66 \t Validation precision@k15: 0.89, accuracy@k15: 0.87\n",
      "Epoch: 66 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 66 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 66 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 44.50\n",
      "RAM %: 43.8\n",
      "Epoch: 67 \t Training Loss: 33.036123\n",
      "Epoch: 67 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 67 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 67 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 67 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 67 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 67 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 43.70\n",
      "RAM %: 43.8\n",
      "Epoch: 68 \t Training Loss: 32.994683\n",
      "Epoch: 68 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 68 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 68 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 68 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 68 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 68 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 43.23\n",
      "RAM %: 44.0\n",
      "Epoch: 69 \t Training Loss: 32.948467\n",
      "Epoch: 69 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 69 \t Validation precision@k10: 0.86, accuracy@k10: 0.74\n",
      "Epoch: 69 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 69 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 69 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 69 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 42.98\n",
      "RAM %: 44.0\n",
      "Epoch: 70 \t Training Loss: 32.928148\n",
      "Epoch: 70 \t Validation precision@k5: 0.87, accuracy@k5: 0.48\n",
      "Epoch: 70 \t Validation precision@k10: 0.87, accuracy@k10: 0.74\n",
      "Epoch: 70 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 70 \t Validation precision@k20: 0.93, accuracy@k20: 0.93\n",
      "Epoch: 70 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 70 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 42.97\n",
      "RAM %: 44.2\n",
      "Epoch: 71 \t Training Loss: 32.880859\n",
      "Epoch: 71 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 71 \t Validation precision@k10: 0.87, accuracy@k10: 0.74\n",
      "Epoch: 71 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 71 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 71 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 71 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 42.78\n",
      "RAM %: 44.2\n",
      "Epoch: 72 \t Training Loss: 32.851397\n",
      "Epoch: 72 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 72 \t Validation precision@k10: 0.87, accuracy@k10: 0.74\n",
      "Epoch: 72 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 72 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 72 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 72 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 42.49\n",
      "RAM %: 44.3\n",
      "Epoch: 73 \t Training Loss: 32.829041\n",
      "Epoch: 73 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 73 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 73 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 73 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 73 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 73 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 41.75\n",
      "RAM %: 44.3\n",
      "Epoch: 74 \t Training Loss: 32.778726\n",
      "Epoch: 74 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 74 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 74 \t Validation precision@k15: 0.90, accuracy@k15: 0.87\n",
      "Epoch: 74 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 74 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 74 \t Validation precision@k30: 0.97, accuracy@k30: 0.97\n",
      "CPU: 41.60\n",
      "RAM %: 44.3\n",
      "Epoch: 75 \t Training Loss: 32.756586\n",
      "Epoch: 75 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 75 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 75 \t Validation precision@k15: 0.90, accuracy@k15: 0.88\n",
      "Epoch: 75 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 75 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 75 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 41.15\n",
      "RAM %: 44.4\n",
      "Epoch: 76 \t Training Loss: 32.720941\n",
      "Epoch: 76 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 76 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 76 \t Validation precision@k15: 0.90, accuracy@k15: 0.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 \t Validation precision@k20: 0.94, accuracy@k20: 0.93\n",
      "Epoch: 76 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 76 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.59\n",
      "RAM %: 44.4\n",
      "Epoch: 77 \t Training Loss: 32.684025\n",
      "Epoch: 77 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 77 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 77 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 77 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 77 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 77 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.36\n",
      "RAM %: 44.4\n",
      "Epoch: 78 \t Training Loss: 32.661852\n",
      "Epoch: 78 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 78 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 78 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 78 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 78 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 78 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.70\n",
      "RAM %: 45.6\n",
      "Epoch: 79 \t Training Loss: 32.628531\n",
      "Epoch: 79 \t Validation precision@k5: 0.88, accuracy@k5: 0.48\n",
      "Epoch: 79 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 79 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 79 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 79 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 79 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.78\n",
      "RAM %: 45.7\n",
      "Epoch: 80 \t Training Loss: 32.620967\n",
      "Epoch: 80 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 80 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 80 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 80 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 80 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 80 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.74\n",
      "RAM %: 45.9\n",
      "Epoch: 81 \t Training Loss: 32.578361\n",
      "Epoch: 81 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 81 \t Validation precision@k10: 0.87, accuracy@k10: 0.75\n",
      "Epoch: 81 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 81 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 81 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 81 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.44\n",
      "RAM %: 45.9\n",
      "Epoch: 82 \t Training Loss: 32.551365\n",
      "Epoch: 82 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 82 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 82 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 82 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 82 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 82 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.43\n",
      "RAM %: 46.0\n",
      "Epoch: 83 \t Training Loss: 32.530858\n",
      "Epoch: 83 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 83 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 83 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 83 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 83 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 83 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.66\n",
      "RAM %: 45.7\n",
      "Epoch: 84 \t Training Loss: 32.521568\n",
      "Epoch: 84 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 84 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 84 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 84 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 84 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 84 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 40.41\n",
      "RAM %: 45.7\n",
      "Epoch: 85 \t Training Loss: 32.481524\n",
      "Epoch: 85 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 85 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 85 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 85 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 85 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 85 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.66\n",
      "RAM %: 45.8\n",
      "Epoch: 86 \t Training Loss: 32.471541\n",
      "Epoch: 86 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 86 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 86 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 86 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 86 \t Validation precision@k25: 0.96, accuracy@k25: 0.96\n",
      "Epoch: 86 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.39\n",
      "RAM %: 45.8\n",
      "Epoch: 87 \t Training Loss: 32.444169\n",
      "Epoch: 87 \t Validation precision@k5: 0.88, accuracy@k5: 0.49\n",
      "Epoch: 87 \t Validation precision@k10: 0.88, accuracy@k10: 0.75\n",
      "Epoch: 87 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 87 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 87 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 87 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.25\n",
      "RAM %: 45.8\n",
      "Epoch: 88 \t Training Loss: 32.418810\n",
      "Epoch: 88 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 88 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 88 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 88 \t Validation precision@k20: 0.94, accuracy@k20: 0.94\n",
      "Epoch: 88 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 88 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.40\n",
      "RAM %: 45.9\n",
      "Epoch: 89 \t Training Loss: 32.403774\n",
      "Epoch: 89 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 89 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 89 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 89 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 89 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 89 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 38.92\n",
      "RAM %: 45.9\n",
      "Epoch: 90 \t Training Loss: 32.382014\n",
      "Epoch: 90 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 90 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 90 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 90 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 90 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 90 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 38.86\n",
      "RAM %: 45.9\n",
      "Epoch: 91 \t Training Loss: 32.368130\n",
      "Epoch: 91 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 91 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 91 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 91 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 91 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 91 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.30\n",
      "RAM %: 46.0\n",
      "Epoch: 92 \t Training Loss: 32.342992\n",
      "Epoch: 92 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 92 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 92 \t Validation precision@k15: 0.91, accuracy@k15: 0.88\n",
      "Epoch: 92 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 92 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 92 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 38.89\n",
      "RAM %: 46.0\n",
      "Epoch: 93 \t Training Loss: 32.326921\n",
      "Epoch: 93 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 93 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 93 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 93 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 93 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 93 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.58\n",
      "RAM %: 46.1\n",
      "Epoch: 94 \t Training Loss: 32.308502\n",
      "Epoch: 94 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 94 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 94 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 94 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 94 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 94 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 39.33\n",
      "RAM %: 46.3\n",
      "Epoch: 95 \t Training Loss: 32.299424\n",
      "Epoch: 95 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 95 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 \t Validation precision@k15: 0.91, accuracy@k15: 0.89\n",
      "Epoch: 95 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 95 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 95 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 38.67\n",
      "RAM %: 46.2\n",
      "Epoch: 96 \t Training Loss: 32.281187\n",
      "Epoch: 96 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 96 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 96 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 96 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 96 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 96 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 38.07\n",
      "RAM %: 46.2\n",
      "Epoch: 97 \t Training Loss: 32.257602\n",
      "Epoch: 97 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 97 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 97 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 97 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 97 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 97 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 37.57\n",
      "RAM %: 46.2\n",
      "Epoch: 98 \t Training Loss: 32.248519\n",
      "Epoch: 98 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 98 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 98 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 98 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 98 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 98 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 37.35\n",
      "RAM %: 46.3\n",
      "Epoch: 99 \t Training Loss: 32.233126\n",
      "Epoch: 99 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 99 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 99 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 99 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 99 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 99 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU: 36.87\n",
      "RAM %: 46.2\n",
      "Epoch: 100 \t Training Loss: 32.226201\n",
      "Epoch: 100 \t Validation precision@k5: 0.89, accuracy@k5: 0.49\n",
      "Epoch: 100 \t Validation precision@k10: 0.88, accuracy@k10: 0.76\n",
      "Epoch: 100 \t Validation precision@k15: 0.92, accuracy@k15: 0.89\n",
      "Epoch: 100 \t Validation precision@k20: 0.95, accuracy@k20: 0.94\n",
      "Epoch: 100 \t Validation precision@k25: 0.97, accuracy@k25: 0.97\n",
      "Epoch: 100 \t Validation precision@k30: 0.98, accuracy@k30: 0.98\n",
      "CPU times: user 1h 26min 25s, sys: 11min 57s, total: 1h 38min 22s\n",
      "Wall time: 1h 18min 32s\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "%time train(baseline_retain, train_loader, test_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a549f77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation precision@k5: 0.6374, accuracy@k5: 0.3509\n",
      "Validation precision@k10: 0.6264, accuracy@k10: 0.5398\n",
      "Validation precision@k15: 0.6770, accuracy@k15: 0.6548\n",
      "Validation precision@k20: 0.7367, accuracy@k20: 0.7327\n",
      "Validation precision@k25: 0.7848, accuracy@k25: 0.7844\n",
      "Validation precision@k30: 0.8267, accuracy@k30: 0.8267\n"
     ]
    }
   ],
   "source": [
    "for k in range(5, 31, 5):\n",
    "    precision_k, accuracy_k = eval_model(baseline_retain, val_loader, k=k)\n",
    "    print(f'Validation precision@k{k}: {precision_k:.4f}, accuracy@k{k}: {accuracy_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5f9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acba7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2232eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b7486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97abc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23348197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f5e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4h",
   "language": "python",
   "name": "dl4h"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
